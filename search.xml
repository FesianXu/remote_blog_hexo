<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[在TensorFlow中自定义梯度的两种方法]]></title>
    <url>%2F2018%2F10%2F21%2Ftensorflow%2Ftf_gradient_define%2F</url>
    <content type="text"><![CDATA[前言在深度学习中，有时候我们需要对某些节点的梯度进行一些定制，特别是该节点操作不可导（比如阶梯除法如$10 // 3 = 3$），如果实在需要对这个节点进行操作，而且希望其可以反向传播，那么就需要对其进行自定义反向传播时的梯度。在有些场景，如[2]中介绍到的梯度反转(gradient inverse)中，就必须在某层节点对反向传播的梯度进行反转，也就是需要更改正常的梯度传播过程，如下图的$-\lambda \dfrac{\partial L_d}{\partial \theta_f}$所示。 在tensorflow中有若干可以实现定制梯度的方法，这里介绍两种。 1. 重写梯度法重写梯度法指的是通过tensorflow自带的机制，将某个节点的梯度重写(override)，这种方法的适用性最广。我们这里举个例子[3]. 符号函数的前向传播采用的是阶跃函数$y = \rm{sign}(x)$，如下图所示，我们知道阶跃函数不是连续可导的，因此我们在反向传播时，将其替代为一个可以连续求导的函数$y = \rm{Htanh(x)}$，于是梯度就是大于1和小于-1时为0，在-1和1之间时是1。 使用重写梯度的方法如下，主要是涉及到tf.RegisterGradient()和tf.get_default_graph().gradient_override_map()，前者注册新的梯度，后者重写图中具有名字name=&#39;Sign&#39;的操作节点的梯度，用在新注册的QuantizeGrad替代。 12345678910111213141516171819#使用修饰器，建立梯度反向传播函数。其中op.input包含输入值、输出值，grad包含上层传来的梯度@tf.RegisterGradient("QuantizeGrad")def sign_grad(op, grad): input = op.inputs[0] # 取出当前的输入 cond = (input&gt;=-1)&amp;(input&lt;=1) # 大于1或者小于-1的值的位置 zeros = tf.zeros_like(grad) # 定义出0矩阵用于掩膜 return tf.where(cond, grad, zeros) # 将大于1或者小于-1的上一层的梯度置为0 #使用with上下文管理器覆盖原始的sign梯度函数def binary(input): x = input with tf.get_default_graph().gradient_override_map(&#123;"Sign":'QuantizeGrad'&#125;): #重写梯度 x = tf.sign(x) return x #使用x = binary(x) 其中的def sign_grad(op, grad):是注册新的梯度的套路，其中的op是当前操作的输入值/张量等，而grad指的是从反向而言的上一层的梯度。 通常来说，在tensorflow中自定义梯度，函数tf.identity()是很重要的，其API手册如下：1234tf.identity( input, name=None) 其会返回一个形状和内容都和输入完全一样的输出，但是你可以自定义其反向传播时的梯度，因此在梯度反转等操作中特别有用。这里再举个反向梯度[2]的例子，也就是梯度为$-\lambda \dfrac{\partial L_d}{\partial \theta_f}$而不是$\lambda \dfrac{\partial L_d}{\partial \theta_f}$。1234567891011121314151617import tensorflow as tfx1 = tf.Variable(1)x2 = tf.Variable(3)x3 = tf.Variable(6)@tf.RegisterGradient('CustomGrad')def CustomGrad(op, grad):# tf.Print(grad) return -grad g = tf.get_default_graph()oo = x1+x2with g.gradient_override_map(&#123;"Identity": "CustomGrad"&#125;): output = tf.identity(oo)grad_1 = tf.gradients(output, oo)with tf.Session() as sess: sess.run(tf.global_variables_initializer()) print(sess.run(grad_1)) 因为-grad，所以这里的梯度输出是[-1]而不是[1]。有一个我们需要注意的是，在自定义函数def CustomGrad()中，返回的值得是一个张量，而不能返回一个参数，比如return 0，这样会报错，如：1AttributeError: 'int' object has no attribute 'name' 显然，这是因为tensorflow的内部操作需要取返回值的名字而int类型没有名字。 PS:def CustomGrad()这个函数签名是随便你取的。 2. stop_gradient法对于自定义梯度，还有一种比较简洁的操作，就是利用tf.stop_gradient()函数，我们看下例子[1]：12t = g(x)y = t + tf.stop_gradient(f(x) - t) 这里，我们本来的前向传递函数是f(x)，但是想要在反向时传递的函数是g(x)，因为在前向过程中，tf.stop_gradient()不起作用，因此+t和-t抵消掉了，只剩下f(x)前向传递；而在反向过程中，因为tf.stop_gradient()的作用，使得f(x)-t的梯度变为了0，从而只剩下g(x)在反向传递。我们看下完整的例子：12345678910111213141516171819import tensorflow as tfx1 = tf.Variable(1)x2 = tf.Variable(3)x3 = tf.Variable(6)f = x1+x2*x3t = -fy1 = t + tf.stop_gradient(f-t)y2 = fgrad_1 = tf.gradients(y1, x1)grad_2 = tf.gradients(y2, x1)with tf.Session(config=config) as sess: sess.run(tf.global_variables_initializer()) print(sess.run(grad_1)) print(sess.run(grad_2)) 第一个输出为[-1]，第二个输出为[1]，显然也实现了梯度的反转。 Reference[1]. How Can I Define Only the Gradient for a Tensorflow Subgraph?[2]. Ganin Y, Ustinova E, Ajakan H, et al. Domain-adversarial training of neural networks[J]. Journal of Machine Learning Research, 2017, 17(1):2096-2030.[3]. tensorflow 实现自定义梯度反向传播[4]. Custom Gradients in TensorFlow]]></content>
      <categories>
        <category>TensorFlow Basic API</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SVM沉思录6——支持向量机中的核技巧那些事儿]]></title>
    <url>%2F2018%2F10%2F21%2Fsvm%2Fsvm6%2F</url>
    <content type="text"><![CDATA[前言 我们在前文[1-5]中介绍了线性支持向量机的原理和推导，涉及到了软和硬的线性支持向量机，还有相关的广义拉格朗日乘数法和KKT条件等。然而，光靠着前面介绍的这些内容，只能够对近似于线性可分的数据进行分割，而不能对非线性的数据进行处理，这里我们简单介绍下支持向量机中使用的核技巧，使用了核技巧的支持向量机就具备了分割非线性数据的能力。本篇可能是我们这个系列的最后一篇了，如果有机会我们在SMO中再会吧。 如有谬误，请联系指正。转载请注明出处。 联系方式：e-mail: FesianXu@163.comQQ: 973926198github: https://github.com/FesianXu 重回SVM我们在前文[1-5]中就线性SVM做了比较系统的介绍和推导，我们这里做个简单的小回顾。支持向量机(Support Vector Machine,SVM)，是一种基于最大间隔原则进行推导出来的线性分类器，如果引入松弛项，则可以处理近似线性可分的一些数据，其最终的对偶问题的数学表达形式为(1.1)，之所以用对偶形式求解是因为可以轻松地引入所谓的核技巧，我们后面将会看到这个便利性。 \min_{\alpha} \frac{1}{2}\sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_jy_iy_j(x_i \cdot x_j)- \sum_{i=1}^N\alpha_i \\ s.t. \ \sum_{i=1}^N\alpha_iy_i=0 \\ \alpha_i \geq0,i=1,\cdots,N \tag{1.1}其最终的分类超平面如(1.2): \theta(x) = \rm{sign}(\sum_{i=1}^N \alpha^*_iy_i(x_i \cdot x)+b^*) \tag{1.2}从KKT条件[3]中我们知道，除了支持向量SV会影响到决策面之外，其他所有的样本都是不会对决策面产生影响的，因此只有支持向量对应的$\alpha_i^* &gt; 0$，其他所有的$\alpha_j^*$都是等于0的。也就是说，我们的支持向量机只需要记住某些决定性的样本就可以了。实际上，这种需要“记住样本”的方法，正是一类核方法(kernel method)。这个我们后面可能会独立一些文章进行讨论，这里我们记住，因为SVM只需要记忆很少的一部分样本信息，因此被称之为稀疏核方法(Sparse Kernel Method)[6]。 更进一步观察SVM我们这里更进一步对SVM的对偶优化任务和决策面，也即是式子(1.1)(1.2)进行观察，我们会发现，有一个项是相同作用的，那就是$(x_i \cdot x_j)$和$(x_i \cdot x)$，这两项都是在度量两个样本之间的距离。我们会发现，因为点积操作 x_i \cdot x_j = ||x_i|| \cdot ||x_j|| \cdot \cos(\theta) \tag{2.1}在两个向量模长相同的情况下，可以知道这个点积的结果越大，两个样本之间的相似度越高，因此可以看作是一种样本之间的度量(metric)。这个我们可以理解，SVM作为一种稀疏核方法的之前就是一个核方法，是需要纪录训练样本的原始信息的。 但是，我们注意到，我们是在原始的样本特征空间进行对比这个相似度的，这个很关键，因为在原始的样本特征空间里面，样本不一定是线性可分的，如果在这个空间里面，线性SVM将没法达到很好的效果。 开始我们的非线性之路那么，我们在回顾了之前的一些东西之后，我们便可以开始我们的非线性之路了，抓好扶手吧，我们要起飞了。 高维映射对于非线性的数据，如下图所示，显然我们没法通过一个线性平面对其进行分割。当然，那仅仅是在二维的情况下我们没法对齐进行线性分割，谁说我们不能在更高的维度进行“维度打击”呢？！我们不妨把整个数据上升一个维度，投射到三维空间，我们将红色数据“拉高”，而绿色数据“留在原地”，那么我们就有了：发现没有，在二维线性不可分的数据，在三维空间就变得线性可分了。这个时候我们可以纪录下在三维情况下的决策面，然后在做个逆操作，将其投射到原先的二维空间中，那么我们就有了:看来这种维度打击还真是有效！ $\nabla$我们其实还可以再举个更为简单的例子。$\nabla$假如我们现在有一些数据，满足$x_1^2+x_2^2=1$，是的，我们不难发现这其实就是个以原点为圆心半径为1的圆，其参数为$x_1$和$x_2$，但是显然的，这个是个非线性的关系，如果要转换成一个线性的关系要怎么操作呢？简单，用$x_3 = x_1^2$和$x_4 = x_2^2$，我们有变形等价式$x_3+x_4=1$，于是我们便有了关于$x_3$和$x_4$的线性关系式，其关键就是映射$\phi(x)=x^2$。 别小看这个例子哦，这个是我们核技巧的一个关键的直观想法哦。没晕吧？让我们继续吧。 基函数其实我们刚才举得例子中的$\phi(x) = x^2$就是一个基函数(basic function)，其作用很直接，就是将一个属于特征空间$\mathcal{M}$的样本$\mathbf{x} \in \mathcal{M}$映射到新的特征空间$\mathcal{N}$，使得有$\phi(\mathbf{x}) \in \mathcal{N}$。如果诸位看官熟悉深度学习，那么我们就会发现，其实深度学习中的激活函数无非也就是起着这种作用，将浅层的特征空间映射到深层的特征空间，使得其尽可能地容易区分。可以说，激活函数就是一种基函数。 那么我们能不能把这种映射应用到，我们刚才的第二节提到的度量测试中的原始特征空间中的样本呢？答案自然是可以的，这样，我们就会有： (\phi(\mathbf{x}_i) \cdot \phi(\mathbf{x_j})) \tag{3.1}通常为了后续讨论，我们会将式子(3.1)表示为(3.2): \mathcal{k}(\mathbf{x}_i, \mathbf{x}_j) = (\phi(\mathbf{x}_i) \cdot \phi(\mathbf{x_j})) = \phi(\mathbf{x}_i)^T\phi(\mathbf{x}_j) \tag{3.2}好的，这样我们就将原始特征空间的样本映射到新的特征空间了，这个特征空间一般来说是更高维的线性可分的空间。我们将这里的$\mathcal{k}(\cdot, \cdot)$称之为核函数(kernels)，哦噢，我们的核函数正式出场了哦。 在给定了核函数的情况下，我们的对偶优化问题和决策面变成了： \min_{\alpha} \frac{1}{2}\sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_jy_iy_j \mathcal{k}(x_i \cdot x_j)- \sum_{i=1}^N\alpha_i \\ s.t. \ \sum_{i=1}^N\alpha_iy_i=0 \\ \alpha_i \geq0,i=1,\cdots,N \tag{3.3 对偶问题} \theta(x) = \rm{sign}(\sum_{i=1}^N \alpha^*_iy_i \mathcal{k}(x_i \cdot x)+b^*) \tag{3.4 决策面}但是，实际上我们是人工很难找到这个合适的映射$\phi(\cdot)$的，特别是在数据复杂，而不是像例子那样的时候，那么我们该怎么办呢？我们能不能直接给定一个核函数$\mathcal{k}(\cdot, \cdot)$，然后就不用理会具体的基函数了呢？这样就可以隐式地在特征空间进行特征学习，而不需要显式地指定特征空间和基函数$\phi(\cdot)$[9]。答案是可以的！ 我们给定一个Mercer定理[10]： 如果函数$\mathcal{k}(\cdot, \cdot)$是$\mathbb{R}^n \times \mathbb{R}^n \rightarrow \mathbb{R}$上的映射（也就是从两个n维向量映射到实数域，既是进行样本度量计算）。那么如果$\mathcal{k}(\cdot, \cdot)$是一个有效核函数（也称为Mercer核函数），那么当且仅当对于训练样例$[x^{(1)}, x^{(2)}, \cdots, x^{(m)}]$，其相应的核函数矩阵是对称半正定(positive semidefinite)的，并且有$\mathcal{k}(x,y) = \mathcal{k}(y,x)$。 嗯，定理很长，人生很短，这个定理说人话就是，如果这个核函数$\mathcal{k}(\cdot, \cdot)$是一个对称半正定的，并且其是个对称函数（度量的基本条件），那么这个核函数就肯定对应了某个样本与样本之间的度量，其关系正如(3.2)所示，因此隐式地定义出了样本的映射函数$\phi(\cdot)$，因此是个有效的核函数。 诶，但是对称半正定不是矩阵才能判断吗？这里的核函数是个函数耶？嗯…也不尽然，休息下，我们下一节继续吧。 无限维向量与希尔伯特空间先暂时忘记之前的东西吧，清清脑袋，轻装上阵。我们在以前学习过得向量和矩阵都是有限维度的，那么是否存在无限维的向量和矩阵呢？其实，函数正是可以看成无限维的向量，想法其实很简单，假如有一个数值函数$f:x \rightarrow y$，假设其定义域是整个实数，如果对应每一个输入，都输出一个输出值，我们可以把所有输出值排列起来，也就形成了一个无限维的向量，表达为${y}^{\infty}_i$。 而核函数$\mathcal{k}(\mathbf{x}_i, \mathbf{x}_j)$作为一个双变量函数，就可以看成一个行列都是无限维的矩阵了。这样我们就可以定义其正定性了： \int\int f(\mathbf{x})\mathcal{k}(\mathbf{x}, \mathbf{y})f(\mathbf{y}) \rm{d} \mathbf{x} \rm{d} \mathbf{y} \geq 0 \tag{3.5}既然是个矩阵，那么我们就可以对其进行特征分解对吧，只不过因为是无限维，我们需要使用积分，表达式类似于矩阵的特征值分解： \int \mathcal{k}(\mathbf{x}, \mathbf{y}) \Phi(\mathbf{x}) \rm{d} \mathbf{x} = \lambda \Phi(\mathbf{y}) \tag{3.6}这里的特征就不是特征向量了，而是特征函数（看成无限维向量也可以的）。对于不同的特征值$\lambda_1$和$\lambda_2$，和对应的特征函数$\Phi_1(\mathbf{x})$和$\Phi_2(\mathbf{x})$，有： \begin{aligned} \int \mathcal{k}(\mathbf{x}, \mathbf{y}) \Phi_1(\mathbf{x}) \Phi_2(\mathbf{x}) \rm{d} \mathbf{x} &= \int \mathcal{k}(\mathbf{x}, \mathbf{y}) \Phi_2(\mathbf{x}) \Phi_1(\mathbf{x}) \rm{d} \mathbf{x} \\ \rightarrow \int \lambda_1 \Phi_1(\mathbf{x}) \Phi_2(\mathbf{x}) \rm{d} \mathbf{x} &= \int \lambda_2 \Phi_2(\mathbf{x}) \Phi_1(\mathbf{x}) \rm{d} \mathbf{x} \end{aligned} \tag{3.7}因为特征值不为0，因此由(3.7)我们有: < \Phi_1, \Phi_2 > = \int \Phi_1(\mathbf{x}) \Phi_2(\mathbf{x}) \rm{d} \mathbf{x} = 0 \tag{3.8}也就是任意两个特征函数之间是正交(Orthogonal)的，一个核函数对应着无限个特征值${\lambda_i}_{i=1}^{\infty}$和无限个特征函数${\Phi_i}_{i=1}^{\infty}$，这个正是原先函数空间的一组正交基。 回想到我们以前学习到的矩阵分解，我们知道我们的矩阵$A$可以表示为： A = Q\Lambda Q^T \tag{3.9}其中$Q$是$A$的特征向量组成的正交矩阵，$\Lambda$是对角矩阵。特征值$\Lambda_{i,i}$对应的特征向量是矩阵$Q$的第$i$列。我们看到在有限维空间中可以将矩阵表示为特征向量和特征值的组合表达。同样的，在无限维空间中，也可以定义这种分解，因此可以将核函数$\mathcal{k}(\cdot,\cdot)$表示为: \mathcal{k}(\mathbf{x}, \mathbf{y}) = \sum_{i=0}^{\infty} \lambda_i \Phi_i(\mathbf{x}) \Phi_i(\mathbf{y}) \tag{3.10}重新整理下，将${\sqrt{\lambda_i}\Phi_i}_{i=1}^{\infty}$作为一组正交基，构建出一个空间$\mathcal{H}$。不难发现，这个空间是无限维的，如果再深入探讨，还会发现他是完备的内积空间，因此被称之为希尔伯特空间(Hilbert space)[13]。别被名字给唬住了，其实就是将欧几里德空间的性质延伸到了无限维而已。 回到我们的希尔伯特空间，我们会发现，这个空间中的任意一个函数（向量）都可以由正交基进行线性表出： f = \sum_{i=1}^{\infty} f_i \sqrt{\lambda_i} \Phi_i \tag{3.11}所以$f$可以表示为空间$\mathcal{H}$中的一个无限维向量： f = (f_1, f_2, \cdots,)^T_{\mathcal{H}} \tag{3.12}再生性(Reproduce)前面3.3讨论了很多关于函数在希尔伯特空间上的表出形式，我们这里在仔细观察下核函数。我们发现，其实核函数可以拆分为： \mathcal{k}(\mathbf{x}, \mathbf{y}) = \sum_{i=0}^{\infty}\lambda_i\Phi_i(\mathbf{x})\Phi_i(\mathbf{y}) = < \mathcal{k}(\mathbf{x},\cdot), \mathcal{k}(\mathbf{y}, \cdot) >_{\mathcal{H}} \tag{3.13}其中: \mathcal{k}(\mathbf{x}, \cdot) = (\sqrt{\lambda_1}\Phi_1(\mathbf{x}),\sqrt{\lambda_2}\Phi_2(\mathbf{x}),\cdots)^T_{\mathcal{H}} \\ \mathcal{k}(\mathbf{y}, \cdot) = (\sqrt{\lambda_1}\Phi_1(\mathbf{y}),\sqrt{\lambda_2}\Phi_2(\mathbf{y}),\cdots)^T_{\mathcal{H}} \tag{3.14}发现没有，(3.13)将核函数表示为了两个函数的内积，是不是很想我们的式子(3.2)了呢。我们把这种可以用核函数来再生出两个函数的内积的这种性质称之为再生性(reproduce)，对应的希尔伯特空间称之为再生核希尔伯特空间(Reproducing Kernel Hilbert Space,RKHS)，有点吓人的名词，但是如果你能理解刚才的分解，这个其实还是蛮直接的。 我们更进一步吧，如果定义一个映射$\phi(\cdot)$: \phi(\mathbf{x}) = (\sqrt{\lambda_1}\Phi_1(\mathbf{x}),\sqrt{\lambda_2}\Phi_2(\mathbf{x}),\cdots)^T \tag{3.15}当然这是个无限维的向量。这个映射将样本点$\mathbf{x} \in \mathbb{R}^n$投射到无限维的特征空间$\mathcal{H}$中，我们有： < \phi(\mathbf{x}), \phi(\mathbf{y}) > = \mathcal{k}(\mathbf{x}, \mathbf{y}) = \phi(\mathbf{x})^T\phi(\mathbf{y}) \tag{3.16}因此，我们解决了3.2中提出的问题，我们根本就不需要知道具体的映射函数$\phi$是什么形式的，特征空间在哪里（我们甚至可以投射到无限维特征空间，比如我们接下来要讲到的高斯核函数），只要是一个对称半正定的核函数$K$，那么就必然存在映射$\phi$和特征空间$\mathcal{H}$，使得式子(3.16)成立。 这就是所谓的核技巧(Kernel trick)[12]。 PS: 为了理解为什么是从原始的有限维的特征空间映射到无限维的希尔伯特空间，我们从式子(3.15)其实不难发现，$\mathbf{x} \in \mathbb{R}^n$，$\sqrt{\lambda_i}\Phi_i(\mathbf{x}) \in \mathbb{R}$，而我们的$i \rightarrow \infty$，因此可以看成映射成了无限维的特征。 高斯核函数的无限维映射性质有效的核函数，也就是对称半正定的核函数有很多，而且有一定的性质可以扩展组合这些核函数[6]，这一块内容比较多，我们以后独立一篇文章继续讨论。这里我们主要看下使用最多的核函数，高斯核函数，也经常称之为径向基函数。 高斯核函数的数学表达形式如下所示： \mathcal{k}(\mathbf{x}, \mathbf{y}) = \exp(-||\mathbf{x}-\mathbf{y}||^2/2\sigma^2) \tag{3.17}我们现在对(3.17)进行变形(这里为了方便假设$\mathbf{x},\mathbf{y}$是一维的)： \begin{aligned} \exp(-||x-y||^2/2\sigma^2) &= \exp(-\lambda||x-y||^2) \\ &= \exp(-\lambda x^2+2\lambda xy-\lambda y^2) \\ &= \exp(-\lambda x^2) \exp(-\lambda y^2) \exp(2\lambda xy) \end{aligned} \tag{3.18}利用泰勒展开[14]对式子(3.18)中的$\exp(2\lambda xy)$进行展开，有: \begin{aligned} \exp(2\lambda xy) &= \sum_{i=1}^{\infty} \dfrac{(2\lambda xy)^i}{i!} \\ &= \sum_{i=1}^{\infty} \sqrt{\dfrac{2^i \lambda}{i!}}x \cdot \sqrt{\dfrac{2^i \lambda}{i!}}y \end{aligned} \tag{3.19}现在结合(3.18)和(3.19)，我们有： \begin{aligned} \exp(-||x-y||^2&/2\sigma^2) \\ &= \sum_{i=1}^{\infty} \sqrt{\dfrac{2^i \lambda}{i!}} \exp{(-\lambda x^2)}x \cdot \sqrt{\dfrac{2^i \lambda}{i!}} \exp{(-\lambda y^2)}y \end{aligned} \tag{3.20}用序列$\mathbf{x} = {\sqrt{\dfrac{2^1 \lambda}{1!}} \exp{(-\lambda x^2)}x, \sqrt{\dfrac{2^2 \lambda}{2!}} \exp{(-\lambda x^2)}x, \cdots}$, $\mathbf{y}={\sqrt{\dfrac{2^1 \lambda}{1!}} \exp{(-\lambda y^2)}y, \sqrt{\dfrac{2^2 \lambda}{2!}} \exp{(-\lambda y^2)}y,\cdots}$这两个都是无限维向量，也即是一个映射函数$\phi(\cdot)$。于是式子(3.20)可以改写为: \exp(-||x-y||^2/2\sigma^2) = \mathbf{x}^T \mathbf{y} = \phi(\mathbf{x})^T \phi(\mathbf{y}) \tag{3.21}看，我们常用的高斯核函数正是一个无限维映射的核函数。 总结我们前面对再生核希尔伯特空间进行了简单的介绍，同时了解了无限维映射的核函数，高斯核函数，事实上，我们原始的SVM对偶问题推导中的$(x_i \cdot x_j)$也可以看成一种核函数，只不过这是个线性核函数而已，映射到了原始的特征空间，有： \mathcal{k}(\mathbf{x}, \mathbf{y}) = \mathbf{x}^T \mathbf{y} \tag{4.1}在后续的文章中，我们将会介绍更多的核函数，如多项式核函数对数sigmoid核函数等，同时在后续的文章中，我们也将继续探讨关于基函数的一些应用。 Reference[1]. 《SVM笔记系列之一》什么是支持向量机SVM[2]. 《SVM笔记系列之二》SVM的拉格朗日函数表示以及其对偶问题[3]. 《SVM笔记系列之三》拉格朗日乘数法和KKT条件的直观解释[4]. 《SVM笔记系列之四》最优化问题的对偶问题[5]. 《SVM笔记系列之五》软间隔线性支持向量机[6]. Bishop C M. Pattern recognition and machine learning (information science and statistics) springer-verlag new york[J]. Inc. Secaucus, NJ, USA, 2006.[7]. Zhang T. An introduction to support vector machines and other kernel-based learning methods[J]. AI Magazine, 2001, 22(2): 103.[8]. Everything You Wanted to Know about the Kernel Trick[9]. 李航. 统计学习方法[J]. 2012.[10]. 核函数（Kernels） [11]. 机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用[12]. A Story of Basis and Kernel – Part II: Reproducing Kernel Hilbert Space[13]. Hilbert space[14]. 函数的泰勒(Taylor)展开式]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>SVM</tag>
        <tag>Kernel Trick</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SVM沉思录5——软间隔线性支持向量机]]></title>
    <url>%2F2018%2F10%2F21%2Fsvm%2Fsvm5%2F</url>
    <content type="text"><![CDATA[前言在以前的文章中，我们介绍了支持向量机的基本表达式，那是基于硬间隔线性支持向量机的，即是假设数据是完全线性可分的，在数据是近似线性可分的时候，我们不能继续使用硬间隔SVM了，而是需要采用软间隔SVM，在这里我们简单介绍下软间隔线性支持向量机。本人无专业的数学学习背景，只能在直观的角度上解释这个问题，如果有数学专业的朋友，还望不吝赐教。如有误，请联系指正。转载请注明出处。联系方式：e-mail: FesianXu@163.comQQ: 973926198github: https://github.com/FesianXu有关代码开源: click 软间隔最大化在文章《SVM的拉格朗日函数表示以及其对偶问题》和《SVM支持向量机的目的和起源》中，我们推导了SVM的基本公式，那时的基本假设之一就是数据是完全线性可分的，即是总是存在一个超平面$W^TX+b$可以将数据完美的分开，但是正如我们在《SVM的拉格朗日函数表示以及其对偶问题》中最后结尾所说的： 但是，在现实生活中的数据往往是或本身就是非线性可分但是近似线性可分的，或是线性可分但是具有噪声的，以上两种情况都会导致在现实应用中，硬间隔线性支持向量机变得不再实用 因此，我们引入了软间隔线性支持向量机这个概念，硬间隔和软间隔的区别如下图所示： 我们的解决方案很简单，就是在软间隔SVM中，我们的分类超平面既要能够尽可能地将数据类别分对，又要使得支持向量到超平面的间隔尽可能地大。具体来说，因为线性不可分意味着某些样本点不能满足函数间隔大于等于1的条件，即是$\exists i, 1-y_i(W^Tx_i+b) &gt; 0$。解决方案就是通过对每一个样本点$(x_i, y_i)$引入一个松弛变量$\xi_i \geq 0$，对于那些不满足约束条件的样本点，使得函数间隔加上松弛变量之后大于等于1，于是我们的约束条件就变为了： y_i(W^T x_i+b)+\xi_i \geq 1 \\ = y_i(W^T x_i+b) \geq 1-\xi_i \tag{1.1}图像表示如： 超平面两侧对称的虚线为支持向量，支持向量到超平面的间隔为1。在硬间隔SVM中本应该是在虚线内侧没有任何的样本点的，而在软间隔SVM中，因为不是完全的线性可分，所以虚线内侧存在有样本点，通过向每一个在虚线内侧的样本点添加松弛变量$\xi_i$，将这些样本点搬移到支持向量虚线上。而本身就是在虚线外的样本点的松弛变量则可以设为0。于是，给每一个松弛变量赋予一个代价$\xi_i$，我们的目标函数就变成了： f(W, \xi) = \frac{1}{2} \Vert W \Vert ^2+C \sum_{i=1}^N\xi_i \\ i = 1,2, \cdots,N \tag{1.2}其中$C &gt; 0$称为惩罚参数，C值大的时候对误分类的惩罚增大，C值小的时候对误分类的惩罚减小，$(1.2)$有两层含义：使得$\frac{1}{2} \Vert W \Vert^2$尽量小即是间隔尽可能大，同时使得误分类的数量尽量小，C是调和两者的系数，是一个超参数。于是我们的软间隔SVM的问题可以描述为： \min_{W,b,\xi} \frac{1}{2} \Vert W \Vert^2+C \sum_{i=1}^N\xi_i \\ s.t. y_i(W^T x_i+b) \geq 1-\xi_i \\ \xi_i \geq 0 \\ i = 1,2, \cdots, N \tag{1.3}表述为标准形式： \min_{W,b,\xi} \frac{1}{2} \Vert W \Vert ^2+C \sum_{i=1}^N\xi_i \\ s.t. 1-\xi_i-y_i(W^T x_i+b) \leq 0 \\ -\xi_i \leq 0 \\ i = 1,2, \cdots, N \tag{1.4}软间隔SVM的拉格朗日函数表述和对偶问题我们采用《SVM的拉格朗日函数表示以及其对偶问题》中介绍过的相似的方法，将$(1.4)$得到其对偶问题。过程如下：将$(1.4)$转换为其拉格朗日函数形式： L(W, b, \xi, \alpha, \beta) = \frac{1}{2} \Vert W \Vert^2+C \sum_{i=1}^N\xi_i+\sum_{i=1}^N \alpha_i(1-\xi_i-y_i(W^T x_i+b))- \sum_{i=1}^N \beta_i \xi_i \\ = \frac{1}{2} \Vert W \Vert^2+C \sum_{i=1}^N\xi_i+ \sum_{i=1}^N \alpha_i - \sum_{i=1}^N \alpha_i \xi_i - \sum_{i=1}^N \alpha_i y_i(W^T x_i+b) - \sum_{i=1}^N \beta_i \xi_i \\ \tag{2.1}原问题可以表述为（具体移步《最优化问题的对偶问题》）: \min_{W,b,\xi} \max_{\alpha, \beta} L(W, b, \xi, \alpha, \beta) \tag{2.2}得到其对偶问题为： \max_{\alpha, \beta} \min_{W, b, \xi} L(W, b, \xi, \alpha, \beta) \tag{2.3}我们先求对偶问题$\theta_D(\alpha, \beta) = \min_{W, b, \xi} L(W, b, \xi, \alpha, \beta)$，根据KKT条件(具体移步《拉格朗日乘数法和KKT条件的直观解释》)，我们有： \nabla_{W} L(W, b, \xi, \alpha, \beta) = W-\sum_{i=1}^N\alpha_i y_i x_i = 0 \tag{2.4} \nabla_{b} L(W, b, \xi, \alpha, \beta) = \sum_{i=1}^N \alpha_i y_i = 0 \tag{2.5} \nabla_{\xi_i} L(W, b, \xi, \alpha, \beta) = C-\alpha_i-\beta_i = 0 \tag{2.6} \alpha_i \geq 0, \beta_i \geq 0 \tag{2.7}整理得到： W = \sum_{i=1}^N\alpha_i y_i x_i \\ \sum_{i=1}^N \alpha_i y_i = 0 \\ C = \alpha_i+\beta_i \tag{2.8}将$(2.8)$代入$(2.1)$，有： \begin{aligned} L(W, b, \xi, \alpha, \beta) &= \frac{1}{2} \sum_{i=1}^N\alpha_i y_i x_i \sum_{j=1}^N\alpha_j y_j x_j \\ &+(\alpha_i+\beta_i)\sum_{i=1}^N \xi_i+\sum_{i=1}^N \alpha_i - \sum_{i=1}^N \alpha_i \xi_i - \sum_{i=1}^N \beta_i \xi_i - \\ &\sum_{i=1}^N \alpha_iy_i(\sum_{j=1}^N\alpha_j y_j x_j \cdot x_i +b) \\ &= -\frac{1}{2} \sum_{i=1}^N\sum_{j=1}^N \alpha_i \alpha_j y_i y_j (x_i \cdot x_j)+\sum_{i=1}^N \alpha_i \end{aligned} \tag{2.9}所以问题变为： \max_{\alpha, \beta} \theta_D(\alpha, \beta) = \max_{\alpha} -\frac{1}{2} \sum_{i=1}^N\sum_{j=1}^N \alpha_i \alpha_j y_i y_j (x_i \cdot x_j)+\sum_{i=1}^N \alpha_i \tag{2.10}表述为最小化问题： \min_{\alpha} \frac{1}{2} \sum_{i=1}^N\sum_{j=1}^N \alpha_i \alpha_j y_i y_j (x_i \cdot x_j)-\sum_{i=1}^N \alpha_i \\ s.t. \sum_{i=1}^N \alpha_i y_i = 0 \\ \alpha_i \geq 0 \\ \beta_i \geq 0 \\ C = \alpha_i+\beta_i \tag{2.11}通过将$\beta_i = C-\alpha_i$，$(2.11)$可以化为： \min_{\alpha} \frac{1}{2} \sum_{i=1}^N\sum_{j=1}^N \alpha_i \alpha_j y_i y_j (x_i \cdot x_j)-\sum_{i=1}^N \alpha_i \\ s.t. \sum_{i=1}^N \alpha_i y_i = 0 \\ 0 \leq \alpha_i \leq C \tag{2.12}对比文章《SVM的拉格朗日函数表示以及其对偶问题》中的硬间隔SVM的最终的表达式： \min_{\alpha} \frac{1}{2}\sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_jy_iy_j(x_i \cdot x_j)- \sum_{i=1}^N\alpha_i s.t. \ \sum_{i=1}^N\alpha_iy_i=0 \alpha_i \geq0,i=1,\cdots,N \tag{2.13}不难发现软间隔SVM只是在对拉格朗日乘子$\alpha_i$的约束上加上了一个上界$C$。我们以后都会利用$(2.12)$求解，接下来我们在SMO算法中，也将对式子$(2.12)$进行求解。 引用 《SVM的拉格朗日函数表示以及其对偶问题》 CSDN 《SVM支持向量机的目的和起源》 CSDN 《最优化问题的对偶问题》 CSDN 《拉格朗日乘数法和KKT条件的直观解释》 CSDN 《统计学习方法》 豆瓣]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SVM沉思录4——最优化问题的对偶问题]]></title>
    <url>%2F2018%2F10%2F21%2Fsvm%2Fsvm4%2F</url>
    <content type="text"><![CDATA[前言在SVM的推导中，在得到了原问题的拉格朗日函数表达之后，是一个最小最大问题，通常会将其转化为原问题的对偶问题即是最大最小问题进行求解，我们这里简单介绍下最优化问题的对偶问题。本人无专业的数学学习背景，只能在直观的角度上解释这个问题，如果有数学专业的朋友，还望不吝赐教。注意，本文应用多限于SVM，因此会比较狭隘。如有谬误，请联系指正。转载请注明出处。联系方式：e-mail: FesianXu@163.comQQ: 973926198github: https://github.com/FesianXu有关代码开源: click 最优化问题最优化问题研究的是当函数（目标函数）在给定了一系列的约束条件下的最大值或最小值的问题，一般来说，一个最优化问题具有以下形式： \min_{x \in R^n} f(x) \\ s.t. g_i(x) \leq 0 ,i=1,2,\cdots, N\\ h_j(x) = 0, j=1,2,\cdots, M \tag{1.1}最优化问题可以根据目标函数和约束条件的类型进行分类: 如果目标函数和约束条件都为变量的线性函数, 称该最优化问题为线性规划; 如果目标函数为变量的二次函数, 约束条件为变量的仿射函数, 称该最优化问题为二次规划; 如果目标函数或者约束条件为变量的非线性函数, 称该最优化问题为非线性规划. 对偶问题最优化问题存在对偶问题，所谓对偶问题，源于这个思想： 原始问题比较难以求解，通过构建其对偶问题，期望解决这个对偶问题得到其原问题的下界（在弱对偶情况下，对于最小化问题来说），或者得到原问题的解（强对偶情况下）。 在SVM中，因为其属于凸优化问题，因此是强对偶问题，可以通过构建对偶问题解决得到原问题的解。我们举一个线性规划中一个经典问题，描述如下： 某工厂有两种原料A、B，而且能用其生产两种产品： 生产第一种产品需要2个A和4个B，能够获利6； 生产第二种产品需要3个A和2个B，能够获利4；此时共有100个A和120个B，问该工厂最多获利多少？ 可以简单得到其问题的数学表达式为： \max_{x_1, x_2} 6x_1+4x_2 \\ s.t. 2x_1+3x_2 \leq 100 \\ 4x_1+2x_2 \leq 120 \tag{2.1}当然，得到这个式子的根据就是最大化其卖出去的产品的利润。但是，如果只问收益的话，明显地，还可以考虑卖出原材料A和B的手段，前提就是卖出原材料的盈利会比生产商品盈利高，假设产品A和产品B的单价为$w_1$和$w_2$，从这个角度看，只要最小化购买原材料的价格，我们就可以得出另一个数学表达式： \min_{w_1, w_2} {100w_1+120w_2} \\ s.t. 2w_1+4w_2 \geq 6 \\ 3w_1+2w_2 \geq 4 \tag{2.2}其实，我们可以发现这其实是极大极小问题和其对偶问题，极小极大问题。 一些定义原始问题我们要讨论原问题和对偶问题，就需要一些定义，我们给出原始问题的非拉格朗日函数表达形式如式子$(1.1)$所示，引进其广义拉格朗日函数（详见文章《拉格朗日乘数法和KKT条件的直观解释》）： L(x, \alpha, \beta)_{x \in R^n} = f(x)+\sum_{i=1}^N \alpha_i g_i(x)+\sum_{j=1}^M \beta_j h_j(x) \tag{3.1}其中$x=(x^{(1)}, x^{(2)}, \cdots, x^{(n)})^T \in R^n$，而$\alpha_i$和$\beta_j$是拉格朗日乘子，其中由KKT条件有$\alpha_i \geq 0$，考虑关于x的函数： \theta_P(x) = \max_{\alpha, \beta; \alpha_i \geq 0} L(x, \alpha, \beta) \tag{3.2}这里下标$P$用以表示这个是原始问题。联想到我们在文章《SVM的拉格朗日函数表示以及其对偶问题》中一些关于对偶问题的讨论，我们知道其实$(3.2)$中的$\theta_P(x)$其实就表示了$(1.1)$中的原问题的目标函数和其约束条件，这里再探讨一下：假设我们存在一个x，使得x违反原始问题的约束条件，从而有$g_i(x) &gt; 0$或者$h_j(x) \neq 0$，那么我们可以推论出： \theta_P(x) = \max_{\alpha, \beta; \alpha_i \geq 0} [f(x)+\sum_{i=1}^N \alpha_i g_i(x)+ \sum_{j=1}^M \beta_j h_j(x)] = + \infty \tag{3.3}为什么呢？因为若存在某个i使得$g_i(x) &gt; 0$， 那么就可以令$\alpha_i \rightarrow +\infty$使得$\theta_P(x$)取得无穷大这个“最大值”；同样的，若存在一个j使得$h_j(x) \neq 0$， 那么就总是可以使得$\beta_j$让$\beta_j h_j(x) \rightarrow +\infty$， 而其他各个$\alpha_i$和$\beta_j$均取为0（满足约束条件的拉格朗日乘子取为0）。这样，只有对于满足约束条件的i和j，才会有$\theta_P(x)=f(x)$成立。于是我们有这个分段表达式： \theta_P(x) = \begin{cases} f(x) & x满足原始问题约束 \\ + \infty & 其他 \end{cases} \tag{3.4}所以，如果是最小化问题，我们有极小极大问题$(3.5)$: \min_{x} \theta_P(x) = \min_{x} \max_{\alpha, \beta; \alpha_i \geq 0} L(x, \alpha, \beta) \tag{3.5}其与式子$(1.1)$是完全等价的，有着同样的解。这样一来，我们就把原始的最优化问题转换为了广义拉格朗日函数的极小极大问题，为了后续讨论方便，我们记： p^* = \min_{x} \theta_P(x) \tag{3.6}其中$p^*$为问题的解。 极小极大问题的对偶， 极大极小问题我们定义： \theta_{D} (\alpha, \beta) = \min_{x} L(x, \alpha, \beta) \tag{3.7}在考虑极大化$(3.7)$有： \max_{\alpha, \beta; \alpha_i \geq 0} \theta_D(\alpha, \beta)=\max_{\alpha, \beta; \alpha_i \geq 0} \min_{x} L(x, \alpha, \beta) \tag{3.8}式子$(3.8)$称为广义拉格朗日函数的极大极小问题，将其变成约束形式，为： \max_{\alpha, \beta} \theta_D(\alpha, \beta)=\max_{\alpha, \beta} \min_{x} L(x, \alpha, \beta) \\ s.t. \alpha_i \geq 0 \tag{3.9}式子$(3.9)$被称为原问题的对偶问题，定义其最优解为： d^* = \max_{\alpha, \beta} \theta_D(\alpha, \beta) \tag{3.10}实际上，通过这种方法我们可以将式子$(2.1)$转化为式子$(2.2)$，也就是将原问题转化为对偶问题，有兴趣的朋友可以自行尝试。 原始问题和对偶问题的关系正如前面所谈到的，原始问题的解和对偶问题的解存在一定的关系，对于任意的$\alpha, x, \beta$，我们有： \theta_D(\alpha, \beta)=\min_{x} L(x, \alpha, \beta) \leq L(x, \alpha, \beta) \leq \max_{\alpha, \beta; \alpha_i \geq 0} L(x, \alpha, \beta)=\theta_P(x) \tag{4.1}等价于： \theta_D(\alpha, \beta) \leq \theta_P(x) \tag{4.2}注意，式子$(4.2)$对于所有的$x, \alpha, \beta$都成立，因为原始问题和对偶问题均有最优解，所以有： \max_{\alpha, \beta; \alpha_i \geq 0} \theta_D(\alpha, \beta) \leq \min_{x} \theta_P(x) \tag{4.3}容易得到： d^* = \max_{\alpha, \beta; \alpha_i \geq 0} \min_{x} L(x, \alpha, \beta) \leq \min_{x} \max_{\alpha, \beta; \alpha_i \geq 0} L(x, \alpha, \beta) = p^* \tag{4.4}由此我们得到了在最小化问题中$d^* \leq p^*$的结论，这个称为弱对偶。弱对偶指出，解决最小化问题的对偶问题可以得到原问题的解的下界。 既然有弱对偶就会存在强对偶。强对偶指的是$d^*=p^*$的情况，在某些情况下，原始问题和对偶问题的解相同，这时可以用解决对偶问题来代替原始问题，下面以定理的方式给出强对偶成立的重要条件而不予以证明： 考虑原始问题$(1.1)$和对偶问题$(3.9)$，假设$f(x)$和$g_i(x)$都是凸函数， $h_j(x)$是仿射函数，并且不等式约束$g_i(x)$是严格可行的，既存在$x$，对所有$i$有$g_i(x) &lt; 0 $，则存在$x^*,\alpha^*,\beta^*$，使得$x^*$是原始问题的解，$\alpha^*,\beta^*$是对偶问题的解（满足这个条件的充分必要条件就是$x^*, \alpha^*, \beta^*$满足KKT条件1），并且：$p^* = d^* = L(x^*, \alpha^*, \beta^*)$ 引用 最优化问题学习笔记1-对偶理论 CSDN 《统计学习方法》 豆瓣 如何理解对偶问题？ feng liu的回答 《拉格朗日乘数法和KKT条件的直观解释》 CSDN SVM的拉格朗日函数表示以及其对偶问题 CSDN 1. 见《拉格朗日乘数法和KKT条件的直观解释》 &#8617;]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SVM沉思录3——拉格朗日乘数法和KKT条件的直观解释]]></title>
    <url>%2F2018%2F10%2F21%2Fsvm%2Fsvm3%2F</url>
    <content type="text"><![CDATA[前言在SVM的推导中，出现了核心的一个最优化问题，这里我们简单介绍下最优化问题，特别是带有约束的最优化问题，并且引入拉格朗日乘数法和广义拉格朗日乘数法，介绍并且直观解释了KKT条件，用于解决带约束的最优化问题。本人无专业的数学学习背景，只能在直观的角度上解释这个问题，如果有数学专业的朋友，还望不吝赐教。如有误，请联系指正。转载请注明出处。联系方式：e-mail: FesianXu@163.comQQ: 973926198github: https://github.com/FesianXu有关代码开源: click 最优化问题我们在高中，包括在高数中都会经常遇到求解一个函数的最小值，最大值之类的问题，这类问题就是属于最优化问题。比如给出下列一个不带有约束的最优化问题： \min_{x} 3x^2+4x+5, x \in R \tag{1.1}其中的$3x^2+4x+5$我们称为目标函数(objective function)。这样的问题，直接利用罗尔定理（Rolle’s theorem）求出其鞍点，又因为其为凸函数而且可行域是整个$R$，求出的鞍点便是最值点，这个是对于无约束最优化问题的解题套路。如果问题带有约束条件，那么就变得不一样了，如： \min_{x, y} 3xy^2 \\ s.t. 4x+5y = 10 \tag{1.2}因为此时的约束条件是仿射函数（affine function）1，所以可以利用换元法将$x$表示为$y$的函数，从而将目标函数变为无约束的形式，然后利用罗尔定理便可以求出最值点了。然而如果约束条件一般化为$g(x, y) = c$，那么$x$就不一定可以用其他变量表示出来了，这个时候就要利用拉格朗日乘数法(Lagrange multipliers )了。 拉格朗日乘数法(Lagrange multipliers)我们先一般化一个二元最优化问题为$(2.1)$形式： \min_{x, y} f(x, y) \\ s.t. g(x, y) = c \tag{2.1}将目标函数$f(x, y)$和等式约束条件$g(x, y)=c$画出来就如下图所示： 其中的$f(x, y)$虚线为等高线，而红线为$g(x, y)=c$这个约束函数曲线与$f(x,y)$的交点的连线在$x-y平面$的映射。其中，假设有$d_3 &gt; d_2 &gt; d_1$， $d_1$点为最小值点（最优值点）。从直观上可以发现，在$g(x,y)=c$与$f(x,y)$的非最优化交点$A$,$B$,$C$,$D$上，其$f(x,y)$和$g(x,y)$的法线方向并不是共线的，注意，这个相当关键，因为如果不是共线的，说明$g(x,y)=c$与$f(x,y)$的交点中，还存在可以取得更小值的点存在。对于A点来说，B点就是更为小的存在。因此，我们从直觉上推论出只有当$g(x,y)=c$与$f(x,y)$的法线共线时，才是最小值点的候选点（鞍点）。推论到多元变量的问题的时候，法线便用梯度表示。于是，我们有原问题取得最优值的必要条件： \nabla f(x,y) = \nabla \lambda (g(x, y)-c) \tag{2.2}$(2.2)$其中的$\lambda$表示两个梯度共线。可以简单的变形为 \nabla L(x, y, \lambda) = \nabla f(x,y) - \nabla \lambda (g(x, y)-c) = 0 \tag{2.3}让我们去掉梯度算子，得出 L(x, y, \lambda) = f(x, y) - \lambda(g(x, y) - c) \tag{2.4}这个时候$\lambda$取个负号也是不影响的，所以式子$(2.4)$通常写作： L(x, y, \lambda) = f(x, y) + \lambda(g(x, y) - c) \tag{2.5}看！我们得出了我们高数中经常见到的等式约束下的拉格朗日乘数函数的表示方法。 多约束的拉格朗日乘数法以上，我们讨论的都是单约束的拉格朗日乘数法，当存在多个等式约束时（其实不等式约束也是一样的），我们进行一些推广。先一般化一个二元多约束最小化问题： \min_{x, y} f(x, y) \\ s.t. g_i(x, y) = 0, i = 1,2, \cdots,N \tag{2.6}对于每个目标函数和约束配对，我们有: L_1(x ,y ,\lambda_1) = f(x,y)+\lambda_1 g_1(x,y) \\ \vdots \\ L_N(x, y, \lambda_N) = f(x,y)+\lambda_N g_N(x,y)将上式相加有： \sum_{i=1}^N L_i(x,y,\lambda_i)=N f(x, y)+\sum_{i=1}^N \lambda_ig_i(x,y) \tag{2.7}定义多约束的拉格朗日函数为： L(x,y,\lambda) = f(x,y)+\frac{1}{N} \sum_{i=1}^N \lambda_ig_i(x,y) \tag{2.8}因为$\lambda_i$是常数，表示共线的含义而已，所以乘上一个常数$\frac{1}{N}$也不会有任何影响，我们仍然用$\lambda_i$表示，因此式子$(2.8)$变成： L(x,y,\lambda) = f(x,y)+\sum_{i=1}^N \lambda_ig_i(x,y) \tag{2.9}这就是多约束拉格朗日乘数法的函数表达形式。 一个计算例子让我们举一个单约束的拉格朗日乘数法的计算例子，例子来源于引用3。给出一个最大化任务： \max_{x,y} xy^2 \\ s.t. g(x,y):x^2+y^2-3=0 \tag{2.10}图像如： 只有一个约束，使用一个乘子$\lambda$，有拉格朗日函数： L(x,y,\lambda)=xy^2+\lambda(x^2+y^2-3)按照条件求解候选点： \nabla_{x,y,\lambda} L(x,y,\lambda) = (\frac{\partial L}{\partial x}, \frac{\partial L}{\partial y}, \frac{\partial L}{\partial \lambda})=(2xy+2\lambda x, x^2+2 \lambda y, x^2+y^2-3)=0有 x(y+\lambda)=0 \tag{i} x^2+2 \lambda y = 0 \tag{ii} x^2+y^2=3 \tag{iii}根据式子$(i)(ii)(iii)$， 解得有： (\pm \sqrt{2}, 1, -1); (\pm \sqrt{2}, -1, 1); (0, \pm \sqrt{3}, 0)代入$f(x,y)$，得到：2， -2， 0，也就是我们需要求得的最大值，最小值。可以从图中看出，我们观察到其等高线与约束投影线的确是相切的。 广义拉格朗日乘数法(Generalized Lagrange multipliers)上面我们的拉格朗日乘数法解决了等式约束的最优化问题，但是在存在不等式约束的最优化问题（包括我们SVM中需要求解的最优化问题）上，普通的拉格朗日乘数法并不能解决，因此学者提出了广义拉格朗日乘数法（Generalized Lagrange multipliers）， 用于解决含有不等式约束的最优化问题。这一章，我们谈一谈广义拉格朗日乘数法。 首先，我们先一般化我们的问题，规定一个二元标准的带有不等式约束的最小化问题(当然可以推广到多元的问题)，如： \min_{x, y} f(x, y) \\ s.t. g_i(x, y) \leq 0, i = 1,2,\cdots,N \\ h_i(x, y) = 0, i = 1,2, \cdots, M \tag{3.1}类似于拉格朗日乘数法，参照式子$(2.9)$，我们使用$\alpha_i$和$\beta_i$作为等式约束和不等式约束的拉格朗日乘子，得出下式： L(x, y, \alpha, \beta) = f(x, y)+\sum_{i=1}^N \alpha_ig_i(x ,y)+\sum_{i=1}^M \beta_i h_i(x, y) \tag{3.2}KKT条件（Karush–Kuhn–Tucker conditions）指出，当满足以下几个条件的时候，其解是问题最优解的候选解(摘自wikipedia)。 Stationarity 对于最小化问题就是：$\nabla f(x,y)+\sum_{i=1}^N \alpha_i \nabla g_i(x ,y)+\sum_{i=1}^M \beta_i \nabla h_i(x, y) = 0 \tag{3.3}$ 对于最大化问题就是：$\nabla f(x,y)-(\sum_{i=1}^N \alpha_i \nabla g_i(x ,y)+\sum_{i=1}^M \beta_i \nabla h_i(x, y)) = 0 \tag{3.4}$ Primal feasibility $g_i(x,y) \leq0, i = 1,2,\cdots,N \tag{3.5}$ $h_i(x, y) = 0, i = 1,2,\cdots,M \tag{3.6}$ Dual feasibility $\alpha_i \geq 0, i = 1,2, \cdots, N \tag{3.7}$ Complementary slackness $\alpha_i g_i(x, y) = 0, i = 1,2,\cdots,N \tag{3.8}$ 其中的第一个条件和我们的拉格朗日乘数法的含义是相同的，就是梯度共线的意思；而第二个条件就是主要约束条件，自然是需要满足的；有趣的和值得注意的是第三个和第四个条件，接下来我们探讨下这两个条件，以及为什么不等式约束会多出这两个条件。 为了接下来的讨论方便，我们将N设为3，并且去掉等式约束，这样我们的最小化问题的广义拉格朗日函数就变成了： L(x, y, \alpha, \beta) = f(x, y)+\sum_{i=1}^3 \alpha_ig_i(x ,y) \tag{3.9}绘制出来的示意图如下所示： 其中$d_i &gt; d_j, when i &gt; j$，而蓝线为最优化寻路过程。 让我们仔细观察式子$(3.7)$和$(3.8)$，我们不难发现，因为$\alpha_i \geq 0$而$g_i(x, y) \leq 0$，并且需要满足$\alpha_i g_i(x, y) = 0$，所以$\alpha_i$和$g_i(x,y)$之中必有一个为0，那为什么会这样呢？ 我们从上面的示意图入手理解并且记好公式$(3.3)$。让我们假设初始化一个点A， 这个点A明显不处于最优点，也不在可行域内，可知$g_2(x,y)&gt;0$违背了$(3.5)$，为了满足约束$(3.8)$，有$\alpha_2=0$，导致$\alpha_2 \nabla g_2(x,y)=0$，而对于$i=1,3$，因为满足约束条件而且$g_1(x,y) \neq 0, g_3(x,y) \neq 0$，所以$\alpha_1 = 0, \alpha_3 = 0$。这样我们的式子$(3.3)$就只剩下$\nabla f(x,y)$，因此对着$\nabla f(x,y)$进行优化，也就是沿着$f(x,y)$梯度方向下降即可，不需考虑其他的条件（因为还完全处于可行域之外）。因此，A点一直走啊走，从A到B，从B到C，从C到D，这个时候因为D点满足$g_2(x,y)=0$，因此$\alpha_2 &gt; 0$，所以$\alpha_2\nabla g_2(x,y) \neq 0$，因此$(3.3)$就变成了$\nabla f(x,y)+\alpha_2\nabla g_2(x,y)$所以在优化下一个点E的时候，就会考虑到需要满足约束$g_2(x,y) \leq 0$的条件，朝着向$g_2(x,y)$减小，而且$f(x,y)$减小的方向优化。因此下一个优化点就变成了E点，而不是G点。因此没有约束的情况下其优化路径可能是$A \rightarrow B \rightarrow C \rightarrow D \rightarrow G \rightarrow H$，而添加了约束之后，其路径变成了$A \rightarrow B \rightarrow C \rightarrow D \rightarrow E \rightarrow F$。 这就是为什么KKT条件引入了条件3和条件4，就是为了在满足不等式约束的情况下对目标函数进行优化。让我们记住这个条件，因为这个条件中某些$\alpha_i=0$的特殊性质，将会在SVM中广泛使用，而且正是这个性质定义了支持向量(SV)。 引用 拉格朗日乘子法如何理解？ 知乎 《统计学习方法》 豆瓣 《【直观详解】拉格朗日乘法和KKT条件》 微信公众号 《解密SVM系列（一）：关于拉格朗日乘子法和KKT条件》 CSDN Karush–Kuhn–Tucker conditions wikipedia 1. 最高次数为1的多项式，形如 $f(x) = AX+B$，其中$X$是$m \times k$的仿射矩阵，其与线性函数的区别就是，线性函数是$f(x) = AX$没有偏置项$B$。 &#8617;]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SVM沉思录2——SVM的拉格朗日函数表示以及其对偶问题]]></title>
    <url>%2F2018%2F10%2F21%2Fsvm%2Fsvm2%2F</url>
    <content type="text"><![CDATA[前言支持向量机的对偶问题比原问题容易解决，在符合KKT条件的情况下，其对偶问题和原问题的解相同，这里我们结合李航博士的《统计学习方法》一书和林轩田老师的《机器学习技法》中的内容，介绍下SVM的对偶问题。本人无专业的数学学习背景，只能直观上理解一些问题，请数学专业的朋友不吝赐教。如有谬误，请联系指正。转载请注明出处。联系方式：e-mail: FesianXu@163.comQQ: 973926198github: https://github.com/FesianXu有关代码开源: click SVM的原问题的拉格朗日乘数表示 我们在上一篇博文《SVM笔记系列1，SVM起源与目的》中，谈到了SVM的原问题，这里摘抄如下： \min_{W,b} \frac{1}{2} \Vert W \Vert^2 \\ s.t. 1-y_i(W^Tx_i+b) \leq 0, \ i=1,\cdots,N \tag{1.1}其满足形式: \min_{W,b} f(x) \\ s.t. c_i(x) \leq0, i=1,\cdots,k \\ h_j(x) = 0, j=1,\cdots,l \tag{1.2}假设原问题为$\theta_P(x)$，并且其最优解为$p^*=\theta_P(x^*)$。这是一个有约束的最优化问题，我们利用广义拉格朗日乘子法(具体移步《拉格朗日乘数法和KKT条件的直观解释》)，将其转换为无约束的形式： L(W,b,\alpha) = \frac{1}{2} \Vert W \Vert ^2 + \sum_{i=1}^N \alpha_i (1-y_i(W^Tx_i+b)), \ \alpha_i \geq 0 \tag{1.3}变形为： L(W,b,\alpha) = \frac{1}{2}||W||^2 + \sum_{i=1}^N {\alpha_i}-\sum_{i=1}^N{\alpha_iy_i(W^Tx_i+b)} , \ \alpha_i \geq 0 \tag{1.4}我们将会得到原问题的另一个表述为： \begin{aligned} f(x) &= \max_{\alpha} L(W, b, \alpha) \\ &=\max_{\alpha} \frac{1}{2} \Vert W \Vert ^2 + \sum_{i=1}^N {\alpha_i}-\sum_{i=1}^N{\alpha_iy_i(W^Tx_i+b)}, \\ &\alpha_i \geq 0 \end{aligned} \tag{1.5} \begin{aligned} \theta_P(x) &= \min_{W,b}f(x) = \min_{W,b} \max_{\alpha} L(W, b, \alpha) \\ &= \min_{W,b} \max_{\alpha} \frac{1}{2}||W||^2 + \sum_{i=1}^N {\alpha_i}-\sum_{i=1}^N{\alpha_iy_i(W^Tx_i+b)},, \ \alpha_i \geq 0 \end{aligned} \tag{1.6}这里我觉得有必要解释下为什么$f(x)$可以表述为$\max_{\alpha} L(W, b, \alpha)$这种形式。假设我们有一个样本点$x_i$是不满足原问题的约束条件$1-y_i(W^Tx_i+b) \leq 0$的，也就是说$1-y_i(W^Tx_i+b) \gt 0$，那么在$\max_{\alpha}$这个环节就会使得$\alpha_i \rightarrow +\infty$从而使得$L(W,b,\alpha) \rightarrow +\infty$。如果$x_i$是满足约束条件的，那么为了求得最大值，因为$1-y_i(W^Tx_i+b) \leq 0$而且$\alpha_i \geq 0$，所以就会使得$\alpha_i = 0$。由此我们得知： \max_{\alpha}L(W,b,\alpha) = \begin{cases} \frac{1}{2} \Vert W \Vert^2 & 1-y_i(W^Tx_i+b) \leq 0 满足约束条件\\ +\infty & 1-y_i(W^Tx_i+b) \gt 0 不满足约束条件 \end{cases} \tag{1.7}因此在满足约束的情况下， \max_{\alpha}L(W,b,\alpha)=\frac{1}{2} \Vert W \Vert ^2不满足约束条件的样本点则因为无法对正无穷求最小值而自然抛弃。这个时候，我们试图去解$\max_{\alpha}L(W,b,\alpha)$中的$\max_{\alpha}$我们会发现因为$L(W,b,\alpha)=\frac{1}{2}||W||^2 + \sum_{i=1}^N {\alpha_i}-\sum_{i=1}^N{\alpha_iy_i(W^Tx_i+b)}$对于$\alpha$是线性的，非凸的1，因此无法通过梯度的方法求得其最大值点，其最大值点应该处于可行域边界上，因此我们需要得到SVM的对偶问题进行求解。至此，我们得到了原问题的最小最大表述： \begin{aligned} \theta_P(x) &= \min_{W,b} \max_{\alpha} L(W, b, \alpha) \\ &=\min_{W,b} \max_{\alpha} \frac{1}{2} \Vert W \Vert^2 + \sum_{i=1}^N {\alpha_i}-\sum_{i=1}^N{\alpha_iy_i(W^Tx_i+b)} \\ &\alpha_i \geq0,i=1,\cdots,N \end{aligned} \tag{1.8} SVM的对偶问题从上面的讨论中，我们得知了SVM的原问题的最小最大表达形式为： \begin{aligned} \theta_P(x) &= \min_{W,b} \max_{\alpha} L(W, b, \alpha) \\ &=\min_{W,b} \max_{\alpha} \frac{1}{2}||W||^2 + \sum_{i=1}^N {\alpha_i}-\sum_{i=1}^N{\alpha_iy_i(W^Tx_i+b)} \\ &\alpha_i \geq0,i=1,\cdots,N \end{aligned} \tag{2.1}设SVM的对偶问题为$\theta_D(\alpha)$，其最优解为$d^*=\theta_D(\alpha^*)$，可知道其为： \begin{aligned} g(x) &= \min_{W,b} L(W,b,\alpha) \\ &=\min_{W,b} \frac{1}{2} \Vert W \Vert^2 + \sum_{i=1}^N {\alpha_i}-\sum_{i=1}^N{\alpha_iy_i(W^Tx_i+b)} \end{aligned} \tag{2.2} \begin{aligned} \theta_D(\alpha) &= \max_{\alpha}g(x) = \max_{\alpha} \min_{W,b} L(W,b,\alpha)\\ &=\max_{\alpha} \min_{W,b} \frac{1}{2} \Vert W \Vert^2 + \sum_{i=1}^N {\alpha_i}-\sum_{i=1}^N{\alpha_iy_i(W^Tx_i+b)} \end{aligned} \tag{2.3}此时，我们得到了对偶问题的最大最小表述，同样的，我们试图去求解$\theta_D(\alpha)$中的$\min_{W,b}$，我们会发现由于$L(W,b,\alpha)=\frac{1}{2} \Vert W \Vert^2 + \sum_{i=1}^N {\alpha_i}-\sum_{i=1}^N{\alpha_iy_i(W^Tx_i+b)}$对于$W$来说是凸函数，因此可以通过梯度的方法求得其最小值点（即是其极小值点）。 求解$\min_{W,b} L(W,b,\alpha)$，因为$L(W,b,\alpha)$是凸函数，我们对采用求梯度的方法求解其最小值（也是KKT条件中的，$\nabla_WL(W,b,\alpha)=0$和$\nabla_b L(W,b,\alpha)=0$）： \frac{\partial{L}}{\partial{W}}=W-\sum_{i=1}^N\alpha_iy_ix_i=0, i=1,\cdots,N \tag{2.4} \frac{\partial{L}}{\partial{b}}=\sum_{i=1}^N\alpha_iy_i=0,i=1,\cdots,N \tag{2.5}得出： W=\sum_{i=1}^N\alpha_iy_ix_i, \sum_{i=1}^N\alpha_iy_i=0, \alpha_i \geq0,i=1,\cdots,N \tag{2.6}将其代入$g(x)$，注意到$\sum_{i=1}^N\alpha_iy_i=0$,得： \begin{aligned} g(x) &= \frac{1}{2} \sum_{i=1}^N \alpha_iy_ix_i \sum_{j=1}^N a_jy_jx_j+\sum_{i=1}^N\alpha_i -\sum_{i=1}^N\alpha_iy_i(\sum_{j=1}^N \alpha_jy_jx_j \cdot x_i+b) \\ &= -\frac{1}{2}\sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_jy_iy_j(x_i \cdot x_j)+ \sum_{i=1}^N\alpha_i \end{aligned}整理为: \max_{\alpha}g(x) = \max_{\alpha} -\frac{1}{2}\sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_jy_iy_j(x_i \cdot x_j)+ \sum_{i=1}^N\alpha_i \\ s.t. \ \sum_{i=1}^N\alpha_iy_i=0 \\ \alpha_i \geq0,i=1,\cdots,N \tag{2.7}等价为求最小问题: \min_{\alpha}g(x) = \min_{\alpha} \frac{1}{2}\sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_jy_iy_j(x_i \cdot x_j)- \sum_{i=1}^N\alpha_i \\ s.t. \ \sum_{i=1}^N\alpha_iy_i=0 \\ \alpha_i \geq0,i=1,\cdots,N \tag{2.8}根据Karush–Kuhn–Tucker(KKT)条件2,我们有： \nabla_WL(W^*,b^*,\alpha^*)=W^*-\sum_{i=1}^N\alpha_i^*y_ix_i=0 \Longrightarrow W^* = \sum_{i=1}^N\alpha_i^*y_ix_i \tag{2.9} \nabla_bL(W^*,b^*,\alpha^*) = -\sum_{i=1}^N \alpha^*_i y_i=0 \tag{2.10} \alpha^*_i(1-y_i(W^*x_i+b^*))=0 \tag{2.11} 1-y_i(W^*x_i+b^*) \leq 0 \tag{2.12} \alpha^*_i \geq 0 \tag{2.13}前两个式子我们已经在求极值的时候利用了，得知: W^* = \sum_{i=1}^N\alpha_i^*y_ix_i \tag{2.14}并且其中至少有一个$\alpha_j^* \gt 0$，对此$j$有，$y_j(W^*x_j+b^*)-1=0$代入刚才的$W^*$，我们有 b^*=y_j-\sum_{i=1}^N\alpha^*_iy_i(x_i \cdot x_j) \tag{2.15}所以决策超平面为： \sum_{i=1}^N \alpha^*_iy_i(x_i \cdot x)+b^*=0 \tag{2.16}分类超平面为： \theta(x)=sign(\sum_{i=1}^N \alpha^*_iy_i(x_i \cdot x)+b^*) \tag{2.17}其中，我们可以观察到超平面只是依赖于$\alpha_i^*&gt;0$的样本点$x_i$，而其他样本点对其没有影响，所以这些样本是对决策超平面起着决定性作用的，因此我们将$\alpha_i^*&gt;0$对应的样本点集合$x_i$称为支持向量。同时，我们可以这样理解当$\alpha^*_i &gt;0$时，我们有$1-y_i(W^*x_i+b)=0$，这个恰恰是表明了支持向量的函数间隔都是1，恰好和我们之前的设定一致。 至此，我们得到了硬间隔线性支持向量机的数学表述形式，所谓硬间隔线性支持向量机，就是满足我之前的假设 两类样本是线性可分的，总是存在一个超平面$W^Tx+b$可以将其完美分割开。 但是，在现实生活中的数据往往是或本身就是非线性可分但是近似线性可分的，或是线性可分但是具有噪声的，以上两种情况都会导致在现实应用中，硬间隔线性支持向量机变得不再实用，因此我们将会在后续讨论用以解决近似线性可分的软间隔线性支持向量机和基于kernel的支持向量机，后者可以解决非线性可分的问题。下图表示了硬间隔线性支持向量机和软间隔支持向量机之间的区别。 在下一篇中，我们紧接着现在的内容，介绍序列最小最优化算法（Sequential Minimal Optimization,SMO），用于求解$\theta_D(x)$，得到$\alpha^*_i$以便于得到超平面的$W^*$和$b$。我们将在其他文章中介绍软间隔线性支持向量机，广义拉格朗日乘数法，KKT条件和基于kernel的支持向量机。 这里我们要记住我们需要最优化的目的式子，我们以后将会反复提到这个式子。 \min_{\alpha} \frac{1}{2}\sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_jy_iy_j(x_i \cdot x_j)- \sum_{i=1}^N\alpha_i s.t. \ \sum_{i=1}^N\alpha_iy_i=0 \alpha_i \geq0,i=1,\cdots,N 1. 易证明。参考wikipedia的凸函数定义。 &#8617; 2. 事实上，如果$\theta_D(x)$的$L(W,b,\alpha)$满足KKT条件，那么在SVM这个问题中，$W^*$和$b^*$和$\alpha^*_i$同时是原问题和对偶问题的解的充分必要条件是满足KKT条件，具体见《统计学习方法》附录和《拉格朗日乘数法和KKT条件的直观解释》。 &#8617;]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SVM沉思录1——什么是支持向量机SVM]]></title>
    <url>%2F2018%2F10%2F21%2Fsvm%2Fsvm1%2F</url>
    <content type="text"><![CDATA[前言支持向量机是常用的，泛化性能佳的，而且可以应用核技巧的机器学习算法，在深度学习流行前是最被广泛使用的机器学习算法之一，就算是深度学习流行的现在，支持向量机也由于其高性能，较低的计算复杂度而被人们广泛应用。这里结合李航博士的《统计学习方法》一书的推导和林轩田老师在《机器学习技法》中的讲解，谈谈自己的认识。如有谬误，请联系指正。转载请注明出处。联系方式：e-mail: FesianXu@163.comQQ: 973926198github: https://github.com/FesianXu有关代码开源: click SVM的起源 支持向量机(Support Vector Machine, SVM)是一种被广泛使用的机器学习算法，自从被Vapnik等人提出来之后便被广泛使用和发展。传统的支持向量机一般是二类分类器，其基本出发点很简单，就是找到一个策略，能够让线性分类器的分类超平面能够最大程度的把两类的样本最好地分割开，这里我们讨论下什么叫做最好地分割开，和实现这个对整个分类器的意义。 最好地分割数据 在进行接下来的讨论之前，为了简化我们的讨论从而直面问题所在，我们进行以下假设： 1） 我们现在的两类数据是线性可分的， 也就是总是存在一个超平面$W^TX+b$可以将数据完美的分开。2） 我们的数据维度是二维的，也就是特征维只有两个，类标签用+1， -1表示，这样方便我们绘制图像。 我在前篇博文《机器学习系列之 感知器模型》中已经介绍到了感知器这一简单的线性分类器。感知器很原始，只能对线性可分的数据进行准确分割，而且由于其激活函数选用的是阶跃函数，因此不能通过梯度的方法进行参数更新，而是只能采用错误驱动的策略进行参数更新，这样我们的超平面其实是不确定的，因为其取决于具体随机到的是哪个样本点进行更新，这是一个不稳定的结果。而且，由于采用了这种参数更新策略，感知器的超平面即使是能够将线性数据完美地分割开，也经常会出现超平面非常接近某一个类的样本，而偏离另一个类的样本的这种情况，特别是在真实情况下的数据是叠加了噪声的情况下。 如下图所示，其中绿线是感知器的运行结果，因为其算法的不稳定性，所以每次的结果都可能不同，选中的这一次我们可以看出来虽然绿线将两类数据完美地分割开了，但是和蓝色样本很接近，如果新来的测试样本叠加一个噪声，这个超平面就很容易将它分类错误，而最佳分类面粉色线则对噪声有着更好地容忍。 样本噪声 刚才我们谈到了样本集上叠加的噪声，噪声广泛存在于真实数据集中，无法避免，因此我们的分类超平面要能够对噪声进行一定的容忍。一般我们假设噪声为高斯噪声，如下图所示： 其中红点为实际的采样到的样本位置$P_{sample}$，而蓝点是可能的样本的实际位置$P_{actual}$，因为噪声$N$的叠加才使得其偏离到了红点位置，其中蓝点的位置满足高斯分布。 P_{sample} = P_{actual}+N, N \sim N(\mu, \sigma^2) \tag{1.1}最佳分类超平面 也就是说我们根据$P_{sample}$点训练出来的感知器的分类器超平面很可能会出现可以完美地划分$P_{sample}$点，但是却不能正确地划分对新来的测试样本的现象。因为新来的样本很可能位于蓝色的样本点的位置，也就是表现出了严重的过拟合现象， 而我们的支持向量机的机制可以很好地减免这种现象，具有更好的泛化能力。我们用几张图来表述下导致这种过拟合的原因：Figure 1, 感知器分类超平面能将线性可分的样本完美分割，但是由于样本叠加了高斯噪声$N$，所以当测试样本的数据出现在超平面“穿过”的“绿圈”之内时，就可能会出现错分的情况，这就是过拟合的一种表现。Figure 2,假设我们的样本集都是独立同分布采样的，那么其叠加的高斯噪声$N$应该都是相同分布$N \sim N(\mu, \sigma^2)$的，因此这个绿圈的大小应该都是相同的，因此最佳的分类超平面应该是可以和距离它最接近的若干个样本的边界相切的。我们把最接近超平面的若干个样本点称为支持向量，支持向量和超平面的距离越远，相当于我们可以容忍的噪声的高斯分布的方差越小，泛化性能越好。注意，这里的高斯分布的方差是我们假设的，不一定是实际数据集叠加的高斯噪声分布的方差，但是假设的越大，总是能带来更好的泛化能力。 SVM提出 我们在上面谈到了最佳分类超平面应能够使得支持向量距离超平面的距离最大，这个就是支持向量机的基本机制的最优化的目标，我们需要解决这个问题就必须要先数学形式化我们这个目的，只有这样才能进行最优化和求解。 数学形式化表述 我们这里对SVM问题进行数学上的形式化表述，以便于求解，这里主要讨论SVM的原问题，实际上，SVM通常转化为对偶问题进行求解，我们将在下一篇文章里讨论SVM的对偶问题。 函数间隔和几何间隔 我们刚才的表述中，我们知道了SVM的关键就是：使得支持向量距离分类超平面的距离之和最小，这里涉及到了“距离”这个概念，因此我们就必须要定义这个“距离”。这个距离可以定义为函数间隔(functional margin)和几何间隔(geometric margin)。我们分别来观察下这两个间隔。 函数间隔 我们表征一个样本点$x_i$到达一个超平面$\theta(x)=W^Tx+b$的距离，直接可以表述为: \gamma_i = y_i\theta(x_i) = y_i(W^Tx_i+b), \ x_i \in R^n \tag{2.1} 其中$y_i$为正确的标签，为$+1$或$-1$，乘上$y_i$的目的是当$x_i$分类正确的时候$\gamma_i$为正，而当分类错误的时候，$\gamma_i$为负，负数的最大值不超过0，所以也就不存在最大间隔了。整个式子也很好理解，当$x_i$使得$\theta(x)=0$时，该样本点就处于超平面上，当$x_i$使得$\theta(x)$大于0时，该样本点处于超平面之外，该值越大离超平面就越远。 几何间隔 函数间隔可以在一定程度上表征距离，但是存在一个问题，就是在$W$和$b$同时增大一个相同的倍数$\alpha$时，变成$\theta(x)=\alpha W^Tx+\alpha b$时，因为当$\alpha \neq 0$时，其零点还是相同的，所以表示的还是相同的超平面。但是我们从函数间隔的定义中可以看出，如果两者都放大$\alpha$倍，那么其函数间隔也被放大了$\alpha$倍，这个就不符合我们的需求了，我们希望的是只要是相同的一个样本点和一个固定的超平面，那么它们之间的距离应该是一定的，这个也是符合我们直观的。因此我们将函数间隔标准化，定义了几何间隔： \hat{\gamma_i} = \frac{y_i(W^Tx_i+b)}{||W||_{L2}} \tag{2.2} $||W||_{L2}$是L2范式，由于这个标准化因子的作用，使得$\hat{\gamma_i}$的值不会随着放大因子$\alpha$的变化而变化了。很容易看出： \gamma_i = \hat{\gamma_i}||W||_{L2} \tag{2.3}最大化最小距离 定义了几何间隔和函数间隔之后，我们就需要最大化最小距离了，这个听起来挺绕口的，其实意思很简单，就是求得一组$W$和$b$的情况下的最小样本距离，然后在不同的$W$和$b$的情况下最大化这个最小样本距离，最后得出的结果就是能够使得支持向量到超平面的距离最大的超平面了。我们观察下公式可能会更直观一些： \gamma = \min_{N=1,\cdots,N} \gamma_i, \ i=1,\cdots,N \tag{2.4} \hat{\gamma} = \min_{N=1,\cdots,N} \hat{\gamma_i}, \ i=1,\cdots,N \tag{2.5}这个就是最小几何间隔距离，我们现在最大化它，有： \max_{W,b} \hat{\gamma} \tag{2.6}将两者写在一起，可以表述为： \max_{W,b} \hat{\gamma} \\ s.t. \ \frac{y_i(W^Tx_i+b)}{ \Vert W \Vert} \geq \hat{\gamma}, \ i=1,\cdots,N \tag{2.7}容易看出其中的$\frac{y_i(W^Tx_i+b)}{ \Vert W \Vert} \geq \hat{\gamma}, i=1,\cdots,N$ 其实是和约束条件$\hat{\gamma} = \min_{N=1,\cdots,N} \hat{\gamma_i}, i=1,\cdots,N$等价的。我们做一些恒等变换有: \max_{W,b} \frac{\gamma}{ \Vert W \Vert} \\ s.t. \ y_i(W^Tx_i+b) \geq \gamma, \ i=1,\cdots,N \tag{2.8} 这里我们要想一下：$\gamma$的具体取值会不会影响到最优化后的$W$和$b$的取值呢？答案是不会的，因为我们只要令所有支持向量，也就是距离超平面最近的若干个样本点到超平面的距离为单位量，比如为1即可，这个可以通过等比例调整$W$和$b$容易地做到，其他样本也会随着进行相应的缩放。这样对整个超平面的最优化点是没有任何影响的。所以我们现在将$\gamma$设为常数1。现在有： \max_{W,b} \frac{1}{ \Vert W \Vert} \\ s.t. \ y_i(W^Tx_i+b) \geq 1, \ i=1,\cdots,N \tag{2.9}此时最大化问题转化为最小化问题： \min_{W,b} \frac{1}{2} \Vert W \Vert^2 \\ s.t. 1-y_i(W^Tx_i+b) \leq 0, \ i=1,\cdots,N \tag{2.10}至此，我们得到了SVM的标准原问题表达。注意到这个式子里的$1-y_i(W^Tx_i+b) \leq 0,$，当存在$x_i$使得$1-y_i(W^Tx_i+b) = 0$时，这个$x_i$就被称之为支持向量。如下图的虚线上的红色样本和蓝色样本所示，虽然样本有很多个，但是有效的，决定超平面的样本，也就是支持向量一共就只有五个，其到决策面的距离被标准化为了1。 我们接下来将会讨论SVM原问题拉格朗日函数形式以及其对偶问题，以便于更好地解决这个最优化问题。]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习Debug沉思录]]></title>
    <url>%2F2018%2F10%2F21%2Fdl_debug_thinking%2Fdl_debug%2F</url>
    <content type="text"><![CDATA[前言接触深度学习也有一两年了，一直没有将一些实战经验整理一下形成文字。本文打算用来纪录一些在深度学习实践中的调试过程，纪录一些经验之谈。因为目前深度学习业界的理论基础尚且薄弱，很多工程实践中的问题没法用理论解释得很好，这里的只是实践中的一些经验之谈，以供参考以及排错。本文将持续更新。需要强调的是，本文的很多单纯只是经验，在尽可能列出参考文献的同时却并无严格理论验证，希望大家见谅。欢迎大家集思广益，共同维护这个经验集，为整个社区贡献微弱力量。 如有问题请指出，转载请标注出处，联系方式： e-mail: FesianXu@163.comQQ: 973926198github: https://github.com/FesianXu 在分类问题中，损失函数及其快速得下降为0.0000在分类问题中，我们一般采用的是交叉熵[1]作为损失函数，如式(1.1)所示 \begin{aligned} \mathcal{L}_{cls} &= - \sum_{i=1}^n y_i \log{\hat{y}_i} = - \mathbf{y}^T \log{\mathbf{\hat{y}}} \\ \mathbf{y} &\in \mathbb{R}^n, \mathbf{\hat{y}} \in \mathbb{R}^n \end{aligned} \tag{1.1}其中$\hat{y}$和$\mathbf{\hat{y}}$是预测结果，以概率分布的形式表达，如$[0.2,0.3,0.3,0.2]$等，一般是通过softmax层实现，$y$和$\mathbf{ {y} }$是样本真实标签，在单分类问题中，采用的是独热编码[2]，只有一个分量是为1的，如$[0.0,1.0,0.0,0.0]$。（公式第二行是向量化表达） 我们发现，交叉熵损失的下确界是0，但是永远都不可能达到0，因为要达到0，那么所有的预测向量分布就必须完全和真实标签一致，退化为独热编码。但是实际上在神经网络中，经过了softmax层之后，是不可能使得除了目标分量的其他所有分量为0的（这个这里只是抛出了结论，讨论需要比较长的篇幅。），因此永远不可能达到0的，正是因为如此，交叉熵损失可以一直优化，这也是其比MSE损失优的一个点之一。 既然注意到了不可能为0，我们就可以分析，这肯定是自己程序问题，我们将经过softmax之前的logit打印出，如： [1023,-201,1021,124]发现了没有，这些值都很大，而softmax函数为: P(x_i) = \dfrac{\exp(x_i)}{\sum_{i=1}^n \exp{(x_i)}} \tag{1.2}我们会发现，过大或者过小的指数项，比如1023，会涉及到计算$e^{1023}$，这个数值在TensorFlow或者大部分框架中是溢出的，显示为inf，因此就会把该分量拉成1，而其他变成了0。这种操作是会导致严重的过拟合的。因此，一般来说，logit值不能太大，否则将会出现数值计算问题。 那么如何解决？出现这种问题的情况很多时候是因为参数初始化导致的数值计算问题，比如都采用了方差过小的高斯分布进行初始化，那么就会把网络的输出的范围拉的特别大，导致以上的问题。因此在参数初始化中，确保每一层的初始化都是在一定范围内的，可以考虑采用Xavier初始化，Kaiming初始化等。（这个初始化的影响我们将会以后讨论，这是一个新的话题。） 在正则化的过程中对神经网络的偏置也进行了正则一般来说，我们常用的是二范数正则，也即是岭回归，如式子(2.1) \mathcal{L} = \gamma_{\mathbf{w}}(y,\hat{y})+\dfrac{1}{2}\mathbf{w}^T\mathbf{w} \tag{2.1}一般来说，我们只会对神经网络的权值进行正则操作，使得权值具有一定的稀疏性[3]，减少模型的容量以减少过拟合的风险。同时，我们注意到神经网络中每一层的权值的作用是调节每一层超平面的方向（因为$\mathbf{w}$就是其法向量），因此只要比例一致，不会影响超平面的形状的。但是，我们必须注意到，每一层中的偏置是调节每一层超平面的平移长度的，如果你对偏置进行了正则，那么我们的$b$可能就会变得很小，或者很稀疏，这样就导致你的每一层的超平面只能局限于很小的一个范围内，使得模型的容量大大减少，一般会导致欠拟合[7]的现象。 因此，一般我们不会对偏置进行正则的，注意了。 学习率太大导致不收敛不收敛是个范围很大的问题，有很多可能性，其中有一种是和网络结构无关的原因，就是学习率设置的太大了，如下图所示，太大的学习率将会导致严重的抖动，使得无法收敛，甚至在某些情况下可能使得损失变得越来越大直到无穷。这个时候请调整你的学习率，尝试是否可以收敛。当然，这里的“太大”目前没有理论可以衡量，不过我喜欢从$10^{-3} \sim 10^{-4}$的Adam优化器[4]开始进行尝试优化。 下图展示了过大过小的学习率对模型性能的影响曲线图： 别在softmax层前面的输入施加了激活函数softmax函数如式(4.1)所示： S(x_i)=\dfrac{\exp{(x_i)}}{\sum_{j=1}^n \exp(x_j)} \tag{4.1}假设我们的网络提取出来的最后的特征向量是$\tilde{\mathbf{y}} = f_{\theta}(\mathbf{x}), \tilde{\mathbf{y}}\in\mathbb{R}^m$，如果我们最后的分类的类别有$n$类，那么我们会用一个全连接层将其映射到对应维度的空间里面，如式(4.2)。 \mathbf{y}=g_{\mathbf{w}}(\tilde{\mathbf{y}}) \\ \mathbf{y} \in \mathbb{R}^n \tag{4.2}那么，这个全连接层虽然说可以看成是分类器，但是我们最好把它看成是上一层的“近线性可分特征”的一个维度转换（有点绕，意思是我们这里只是一个维度的转换，而不涉及到kernel），不管怎么说，这个时候，我们的输出是不能有激活函数的，如下式是不可以的： \mathbf{y}=\sigma(g_{\mathbf{w}}(\tilde{\mathbf{y}}) ) \\ \sigma(\cdot) 为激活函数\\ \mathbf{y} \in \mathbb{R}^n \tag{4.3}这时候的输出，具有和分类类别相同的维度，在很多框架中被称之为logits值，这个值一般是在实数范围内的，一般不会太大，参考笔记第一点的情况。 检查原数据输入的值范围原始数据输入可能千奇百怪，每个特征维的值范围可能有着数量级上的差别，这个时候如果我们不对数据进行预处理，将会大大增大设计网络的负担。一般来说我们希望输入的数据是中心对齐的，也即是0均值的[5]，可以加速网络收敛的速度。同时，我们希望不同维度上的数值范围是一致的，可以采用一些归一化[6]的手段进行处理（这个时候假设每个维度重要性是一样的，比如我们图片的三个通道等）。 别忘了对你的训练数据进行打乱经常，你的训练过程非常完美，能够很好地拟合训练数据，但是在测试过程中确实一塌糊涂，是的，你的模型这个时候过拟合[7]了。这个时候你会检查模型的有效性，不过在进行这一步之前，不妨先检查下你的数据加载器(Data Loader)是否是正常设计的。 一般来说，我们的训练数据在训练过程中，每一个epoch[8]中，都是需要进行打乱(shuffle)的，很多框架的数据加载器参数列表中都会有这项选项，比如pytorch的DataLoader类[9]。为什么需要打乱呢？那是因为如果不打乱我们的训练数据，我们的模型就有可能学习到训练数据的个体与个体之间特定的排列顺序，而这种排列顺序，在很多情况下是无用的，会导致过拟合的糟糕现象。因此，我们在训练过程中，在每一个epoch训练中都对训练集进行打乱，以确保模型不能“记忆”样本之间的特定排序。这其实也是正则的一种手段。 在训练中，大概如： epoch\_1 \rightarrow shuffle \rightarrow epoch\_2 \rightarrow shuffle \cdots 一个batch中，label不要全部相同这个情况有点类似与笔记的第六点，我们需要尽量给训练过程中人为引入不确定性，这是很多正则手段，包括dropout，stochastic depth等的思路，这样能够有效地减少过拟合的风险。因此，一个batch中，尽量确保你的样本是来自于各个类的（针对分类问题而言），这样你的模型会减少执着与某个类别的概率，减少过拟合风险，同时也会加快收敛速度。 少用vanilla SGD优化器在高维度情况下的优化，其优化平面会出现很多鞍点（既是梯度为0，但却不是极点），通常，鞍点会比局部极值更容易出现（直观感受就是，因为高维度情况下，一个点周围有很多维度，如果是极值点，那么就需要其他所有维度都是朝向同一个方向“弯曲”的，而这个要比鞍点的各个方向“弯曲”的情况可能要小），因此这个时候我们更担心陷于鞍点，而不是局部极小值点（当然局部极小值点也是一个大麻烦，不过鞍点更麻烦）。如果采用普通的SGD优化器，那么就会陷于任何一个梯度为0的点，也就是说，极有可能会陷于鞍点。如果要使用SGD方法，建议使用带有momentum的SGD方法，可以有效避免陷入鞍点的风险。 下图是某个函数的三维曲线图和等高线图，我们可以看到有若干个局部最优点和鞍点，这些点对于vanilla SGD来说是不容易处理的。 检查各层梯度，对梯度爆炸进行截断有些时候，你会发现在训练过程中，你的损失突然变得特别大，或者特别小，这个时候不妨检查下每一层的梯度（用tensorboard的distribution可以很好地检查），很可能是发生了梯度爆炸(gradient explosion)的情况，特别是在存在LSTM等时序的网络中，很容易出现这种情况。因此，这个时候我们会用梯度截断进行处理，操作很简单粗暴，就是设置一个阈值，把超过这个阈值的梯度全部拉到这个阈值，如下图所示： 在tensorflow中也提供了相应的API供梯度截断使用[10]，如：123456tf.clip_by_value( t, clip_value_min, # 指定截断最小值 clip_value_max, # 指定截断最大值 name=None) 具体使用见[11]，在应用梯度之前，对梯度截断进行处理。 检查你的样本label有些时候，你的训练过程可以很好地收敛，当使用MSE损失[12]的时候甚至可能达到0.0000的情况。但是，当你把模型拿到测试集中评估的时候，却发现性能极差，仿佛没有训练一样。这是过拟合吗？显然是的，但是这可能并不是你的模型的问题，请检查你的数据加载中训练集的样本标签是否正确对应。 这个问题很白痴，但是却真的很容易在数据加载过程中因为种种原因把label信息和对应样本给混掉。根据文献[13]中的实验，用MSE损失的情况下，就算是你的label完全随机的，和样本一点关系都没有，也可以通过基于SGD的优化算法达到0.0000损失的。因此，请务必确保你的样本label是正确的。 分类问题中的分类置信度问题在分类问题中我们一般都是采用的是交叉熵损失，如式子(1.1)所示，在一些实验中，如果我们绘制出训练损失和分类准确度的曲线图，我们可能会有下图这种情况[14]： 其中上图为分类损失，紫色为训练损失，蓝色为测试损失，下图为分类准确度，绿色为训练准确度，蓝色为测试准确度。我们不难发现一个比较有意思的现象，就是当测试损失开始到最低点，开始向上反弹的时候，其测试准确度却还是上升的，而不是下降。 这是为什么呢？为什么分类准确度不会顺着分类损失的增大而减少呢？ 这个涉及到了分类过程中对某个类的“置信程度”的多少，比如： [0.9,0.01,0.02,0.07]模型是对第一类相当确信的，但是在第二种情况： [0.5,0.4,0.05,0.05]这对第一类的置信程度就很低了，虽然按照贝叶斯决策，还是会选择第一类作为决策结果。因此这就是导致以上现象的原因，在那个拐点后面，这个模型对于分类的置信程度其实已经变得很差了，虽然对于准确度而言，其还能分类正确。 但是这其实正是过拟合的一种表现，模型已经对自己的分类结果不确信了。 少在太小的批次中使用BatchNorm层Batch Normalization[15]，中文译作批规范化，在深度学习中是一种加快收敛速度，提高性能的一个利器，其本质和我们对输入的原数据进行0均值单位方差规范化差不多，是以batch为单位，对中间层的输出进行规范化，可以缓和内部协方差偏移(Internal Covariate Shift)的现象。其基本公式很简单，如下： \begin{aligned} \tilde{x} &= \dfrac{x_i-\mu}{\sigma_i} \\ x_i^{\rm{norm}} &= \gamma_i \cdot \tilde{x} + \beta_i \end{aligned} \tag{12.1}不过这里并不打算对BN进行详细讲解，只是想告诉大家，因为BN操作在训练过程中是对每个batch进行处理的，从每个batch中求得均值和方差才能进行操作。如果你的batch特别小（比如是受限于硬件条件或者网络要求小batch），那么BN层的batch均值和方差可能就会不能很好符合整个训练集的统计特征，导致差的性能。实际上，实验[16]说明了这个关系，当batch小于16时，性能大幅度下降。因此，少在太小的batch中使用BN层，如果实在要使用，在发生性能问题时优先检查BN层。 数值计算问题，出现NanNan(Not An Number)是一个在数值计算中容易出现的问题，在深度学习中因为涉及到很多损失函数，有些损失函数的定义域并不是整个实数，比如常用的对数，因此一不小心就会出现Nan。在深度学习中，如果某一层出现了Nan，那么是具有传递性的，后面的层也会出现Nan，因此可以通过二分法对此进行排错。 一般来说，在深度学习中出现Nan是由于除0异常或者是因为损失函数中的（比如交叉熵，KL散度）对数操作中，输入小于或者等于0了，一般等于0的情况比较多，因此通常会： \log(x+\epsilon) \approx \log(x)这里的$\epsilon$是个很小的值，一般取$10^{-6}$即可，可以防止因为对数操作中输入0导致的Nan异常。 需要注意的是，有些时候因为参数初始化或者学习率太大也会导致数值计算溢出，这也是会出现Nan的，一般这样会出现在较前面的层里面。 BN层放置的位置问题BN层有两种常见的放置位置，如下图所示：第一个是放在激活函数之前：第二个是放在激活函数之后： 在原始BN的论文[15]中，Batch Norm(BN)层是位于激活层之前的，因为是对原始的，未经过激活的logit数据进行数据分布的重整。然而，不少实验证实似乎BN层放在激活层之后效果会更好，这个原因目前不明。 dropout层应用在卷积层中可能导致更差的性能dropout[19]是hinton大神与2012年提出的一种神经网络正则手段，其可以简单解释为在训练过程中，按一定概率让神经网络中的某些神经元输出为0，其原因可以有几个解释，一个是作为一种集成模型进行解释，另一个可以看成是在特征提取学习过程中给数据加入噪声，可以看成是一种数据增强的正则手段。 在原始论文中，dropout被应用于全连接层中，而没有应用在卷积层中，Hinton的解释是因为卷积层参数并不多，过拟合风险较小不适合采用dropout这种大杀器的正则手段。有人也认为因为卷积网络是局部感知的，用dropout正则对于其在后层中对于全局信息的获取可能具有负作用[20]。 不过在一些工作中，也有人将dropout层应用在卷积层中的[17-18]，其层次安排为:$CONV-&gt;RELU-&gt;DROP$，不过其丢弃率$p$都是选择的较小数如$0.1$，$0.2$等，个人觉得这里的作用大概是对中间数据进行加入噪声，以便于数据增强的正则手段。 个人建议是可以尝试在卷积层中使用少量的dropout，用较小的丢弃率，但是最后别忘了扔掉这些dropout再进行一些探索，也许可以具有更好的效果。 较小的batch size可以提供较好的泛化现代的深度学习优化器基本上都是基于SGD算法进行修改而成的，在每一次训练中都是以一个batch size为单位进行训练的，在这个过程中相当于在统计这个batch中样本的一些统计特性，因此batch size是会影响模型的超曲线形状的。 一般来说较大的batch size比如128，256会和整个训练集的统计性质更相近，从而使得具有较少的多样性，而较小的batch size 比如16，32，因为batch size较小，不同batch之间的差异性较大，这种差异性可以看成是正则手段，有机会提高模型的泛化性能。（不过有些文章似乎不同意这个观点，认为较大batch size有较好性能，个人建议是大batch size和小batch size都可以跑跑，有可能能提升性能。） Reference[1]. Janocha K, Czarnecki W M. On loss functions for deep neural networks in classification[J]. arXiv preprint arXiv:1702.05659, 2017.(Overview about loss function used in DNN)[2]. tf.one_hot()进行独热编码[3]. 曲线拟合问题与L2正则[4]. Kinga D, Adam J B. A method for stochastic optimization[C]//International Conference on Learning Representations (ICLR). 2015, 5.[5]. &lt;深度学习系列&gt;深度学习中激活函数的选择[6]. 机器学习之特征归一化（normalization）[7]. 机器学习模型的容量，过拟合与欠拟合[8]. 在机器学习中epoch, iteration, batch_size的区别[9]. Pytorch Dataloader doc[10]. tf.clip_by_value[11]. 梯度截断的tensorflow实现[12]. 均方误差(MSE)和均方根误差(RMSE)和平均绝对误差(MAE)[13]. Du S S, Zhai X, Poczos B, et al. Gradient Descent Provably Optimizes Over-parameterized Neural Networks[J]. arXiv preprint arXiv:1810.02054, 2018.[14]. The inconsistency between the loss curve and metric curve?[15]. Ioffe S, Szegedy C. Batch normalization: accelerating deep network training by reducing internal covariate shift[C]// International Conference on International Conference on Machine Learning. JMLR.org, 2015:448-456.[16]. Wu Y, He K. Group normalization[J]. arXiv preprint arXiv:1803.08494, 2018.[17]. Park S, Kwak N. Analysis on the dropout effect in convolutional neural networks[C]//Asian Conference on Computer Vision. Springer, Cham, 2016: 189-204.[18]. Where should I place dropout layers in a neural network?[19]. Hinton G E, Srivastava N, Krizhevsky A, et al. Improving neural networks by preventing co-adaptation of feature detectors[J]. arXiv preprint arXiv:1207.0580, 2012.[20]. Why do I never see dropout applied in convolutional layers?]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Debug Practice</tag>
      </tags>
  </entry>
</search>
