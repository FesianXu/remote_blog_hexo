<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[概率学派和贝叶斯学派的区别]]></title>
    <url>%2F2018%2F10%2F24%2Fbasic%2Fexp_error%2F</url>
    <content type="text"><![CDATA[前言]]></content>
      <categories>
        <category>Basic concept</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>概率学派</tag>
        <tag>贝叶斯学派</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tf.device()指定运行设备]]></title>
    <url>%2F2018%2F10%2F24%2Ftensorflow%2Ftf.device%2F</url>
    <content type="text"><![CDATA[在TensorFlow中，模型可以在本地的GPU和CPU中运行，用户可以指定模型运行的设备。通常，如果你的TensorFlow版本是GPU版本的，而且你的电脑上配置有符合条件的显卡，那么在不做任何配置的情况下，模型是默认运行在显卡下的。运行代码将会提示以下内容：如果需要切换成CPU运算，可以调用tf.device(device_name)函数，其中device_name格式如/cpu:0其中的0表示设备号，TF不区分CPU的设备号，设置为0即可。GPU区分设备号\gpu:0和\gpu:1表示两张不同的显卡。在一些情况下，我们即使是在GPU下跑模型，也会将部分Tensor储存在内存里，因为这个Tensor可能太大了，显存不够放，相比于显存，内存一般大多了，于是这个时候就常常人为指定为CPU设备。这种形式我们在一些代码中能见到。如：12with tf.device('/cpu:0'): build_CNN() # 此时，这个CNN的Tensor是储存在内存里的，而非显存里。 需要注意的是，这个方法会减少显存的负担，但是从内存把数据传输到显存中是非常慢的，这样做常常会减慢速度。]]></content>
      <categories>
        <category>TensorFlow Basic API</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[概率学派和贝叶斯学派的区别]]></title>
    <url>%2F2018%2F10%2F22%2Fbasic%2Ffreqs_bayesian%2F</url>
    <content type="text"><![CDATA[前言对于一个数学模型来说，最主要的莫过于根据观察到的数据进行模型的参数估计了，而概率学派和贝叶斯学派对于这个参数估计有着不同的做法，接下来我们讨论下。如有谬误，请联系指正。转载请注明出处。联系方式：e-mail: FesianXu@163.comQQ: 973926198github: https://github.com/FesianXucode: 概率派和贝叶斯派的区别对于一个问题，从概率派和贝叶斯派看起来是完全不一样的，其最主要的区别就是对于一个问题中模型参数的“信仰”： 对于频率派学者来说，一个模型中的参数是“固定”的，而数据是在分布中随机采样的。我们要重点理解这个固定，这里指的固定意思是 对于一个模型或者也可说一个分布中的参数，我们相信它是固定不变的，而我们观察（采样）到的数据是这个分布中的一个独立同分布样本。也就是说，我们相信这个分布的参数不管你怎么采样，根据参数对其的估计都应该是不会变的，They remain constant!如果根据数据估计出来的参数和真实模型不符合，只可能是引入了噪声而已。在这个观点中，模型参数才是上帝，数据为之服务。 对于贝叶斯派学者来说，我们观察到的数据才是“固定”的，而我们的模型的参数才是在一直变化的。我们不停地观察数据，估计出来的模型参数就可能一直的变化。不仅如此，我们对于这个模型的参数可能会有一个最初始的信仰，称之为先验假设，一旦设置后了之后，我们就可以听由观察到的数据指导模型参数更新了。在这种观点中，我们的模型参数不再是一个参数，而是一个分布了。一般来说，对于贝叶斯派，有公式： P\{\theta|D\} = \dfrac{P\{D|\theta\}P\{\theta\}}{P\{D\}} \tag{1.0} 其中$P\{\theta|D\}$称为后验概率，指的是由观察数据和先验假设推测出来的参数分布，而$P\{\theta\}$称之为先验分布，指的是对于参数的专家知识或者假设而引入的知识，可以指导参数$\theta$的学习，而$P\{D|\theta\}$称之为似然函数，指的就是由于观察数据导致的参数更新。 我们举个投硬币的例子也说明下这两者区别： Question：现在我们有一个硬币，假设朝向正面的几率为$p$，朝向反面的几率为$1-p$，这个$p$是未知的，现在为了估计$p$，投掷了14次，其中有10次朝向正面，问再投掷两次，都朝向正向的概率为多少。 在传统的概率派解答中，因为相信这个模型的参数是固定的，所以很容易知道$p=\dfrac{10}{14}=0.714$，因此在后面投掷两次的过程中，假设都是独立过程，那么 P\{HH|data\}=p^2=0.51 \tag{1.1} 而在贝叶斯派眼中，问题就没有那么简单了，我们相信参数$p$不是简单的一个参数，而应该是一个随机变量，服从一个分布，那么我们就需要用观察到了的数据$data$去估计这个参数$p$的分布，利用贝叶斯公式有： P\{p|data\} = \dfrac{P\{data|p\}P\{p\}}{P\{data\}} \tag{1.2}因为在已知观察中，$data$是固定的，所以$P\{data\}=constant$是一个常数，不妨忽略它，有： P\{p|data\} \propto P\{data|p\}P\{p\} \tag{1.3}有: P\{data|p\} = C_{14}^{10} p^{10}(1-p)^{4} \tag{1.4}参数$C_{14}^{10}$可以忽略，现在对于先验假设$P\{p\}$进行假设，一般来说，我们希望这个假设是一个共轭先验（conjugate prior）1。这里用Beta分布作为硬币参数的先验假设， Beta(p;a,b)=\dfrac{\Gamma(a+b)}{\Gamma(a) \cdot \Gamma(b)} \cdot p^{a-1}(1-p)^{b-1} \tag{1.5}其中伽马函数$\Gamma(\cdot)$定义为: \Gamma(x) = \int_{0}^{+\infty} t^{x-1}e^{-t} \rm dt \tag{1.6}Beta分布有两个控制参数a和b，不同的a和b其CDF的形状差别很大： 在这个先验假设下，我们有： P\{p\} = Beta(p;a,b) \tag{1.7}同样的，因为$\dfrac{\Gamma(a+b)}{\Gamma(a)}$是常数项，忽略所以有： \begin{align} P\{p|data\} &\propto p^{10}(1-p)^{4} \cdot p^{a-1} (1-p)^{b-1} \\ & \propto p^{10+a-1}(1-p)^{4+b-1} \end{align} \tag{1.8}为了让 \int_{0}^{+\infty} p\{p|data\} \rm dp = 1 \tag{1.9}需要拼凑系数，可知道系数为（这里不是特别懂） \dfrac{\Gamma((10+a)+(4+b))}{\Gamma(10+a) \cdot \Gamma(4+b)} = \dfrac{1}{B(10+a,4+b)} \tag{1.10}其中$B(x,y)$为Beta函数，$B(x,y) = \dfrac{\Gamma(x) \Gamma(y)}{\Gamma(x+y)}$ 于是最终有参数$p$的概率分布为: P\{p|data\} = Beta(p;a+10, b+4) \tag{1.11}如果我们对$p$毫无先验可言，那么可以令$a=b=0$，这个时候的计算结果就和频率学派的一模一样，但是如果我们自认为对这个硬币的参数$p$有所了解，但是又不是完全了解，比如说我们知道这个先验应该是一个均匀分布的（也就是正面和反面都应该是0.5的，这个应该是最朴素和直观的假设了。），而均匀分布是Beta分布的一个特例，我们可以令$a=b=1$，这个时候有： P\{p|data\} = Beta(p;11,5) \tag{1.12}图像如： 可以看到因为引入了这个朴素的假设，使得$p$变成了一个中心在$p=0.7$附近的钟形分布，这个时候就发现了和频率派的区别：我们的参数p是一个分布，而不只是一个数值而已。 有了$P\{p|data\}$，我们回归原问题，求: P\{HH|data\} = \int_{0}^{1} P\{HH|p\} P\{p|data\} \rm dp \tag{1.13}这里用积分的原因很简单，就是因为我们的p是一个分布，其值从0到1，因此需要用积分。这里进行两个假设： 投掷硬币每一次都是独立无关的。 在这接下来的两个投掷过程中我们不更新$P\{p|data\}$ 所以有： P\{HH|p\} = [P\{H|p\}]^2 = p^2 \tag{1.14}所以有: P\{HH|data\} = \int_{0}^{1} p^2 \cdot P\{p|data\} \rm dp \tag{1.15}所以有: \begin{align} P\{HH|data\} &= \dfrac{1}{B(10+a,4+b)} \int_{0}^{1} p^{(10+a-1)+2} (1-p)^{4+b-1} \\ &= \dfrac{B(10+a+2,4+b)}{B(10+a, 4+b)} \end{align} \tag{1.16}同样假设$a=b=1$则有$\dfrac{B(13,5)}{B(11, 5)}=0.485$，从这里就看出了频率学派和贝叶斯学派的区别。 总结频率学派和贝叶斯学派的方法优缺点概况： 频率学派是目前深度学习中最常使用的指导思想，但是要想其效果好，必须基于数据量巨大的情况下，否则很难估计出一个好的参数。（因为其不引入任何先验假设，只能从大数据中学习得到。） 贝叶斯学派的方法可以应用在数据量小的情况下，而且方便引入各种专家知识和先验知识，有些场景中表现更为优越。 实际上，频率学派和贝叶斯学派有着千丝万缕的关系，不可割裂看待，也没有孰优孰劣。 Reference Bishop 《Pattern Recognize and Machine Learning, PRML》 《Are you a Bayesian or a Frequentist? (Or Bayesian Statistics 101)》 《Bayesian and frequentist reasoning in plain English》 《先验概率、后验概率以及共轭先验》 1. 后验概率分布（正⽐于先验和似然函数的乘积）拥有与先验分布相同的函数形式。这个性质被叫做共轭性（Conjugacy）。共轭先验（conjugate prior）有着很重要的作⽤。它使得后验概率分布的函数形式与先验概率相同，因此使得贝叶斯分析得到了极⼤的简化 &#8617;]]></content>
      <categories>
        <category>Basic concept</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>概率学派</tag>
        <tag>贝叶斯学派</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tensorflow中的位操作]]></title>
    <url>%2F2018%2F10%2F22%2Ftensorflow%2Ftf_bits%2F</url>
    <content type="text"><![CDATA[前言版本号：1.10TensorFlow支持位操作，在模块tf.bitwise中包含了几个基本的位操作，分别是： bitwise_and() 位与操作 $c = a &amp; b$ bitwise_or() 位或操作 $c = a | b$ bitwise_xor() 位异或操作 $c = (~a &amp; b) | (a &amp; ~b)$ invert() 位反操作 $c = ~a$ left_shift() 位左移操作 $c = a &lt;&lt; b$ right_shift() 位右移操作 $ c = a &gt;&gt; b$ 使用方法很简单，以位与为例：12345tf.bitwise.bitwise_and( x, y, name=None) 使用例子：1234567a = tf.constant(8) # 0000,1000b = tf.constant(4) # 0000,0100bitor = tf.bitwise.bitwise_or(a,b)bitand = tf.bitwise.bitwise_and(a,b)with tf.Session(config=config) as sess: print(sess.run(bitor)) print(sess.run(bitand)) 输出第一个为12，第二个为0，其他操作类似于此。]]></content>
      <categories>
        <category>TensorFlow Basic API</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scp指令用于主机之间的文件相互拷贝]]></title>
    <url>%2F2018%2F10%2F22%2Flinux_cmd%2Fscp%2F</url>
    <content type="text"><![CDATA[前言scp，是secure copy的简称，是linux系统下基于ssh登陆进行安全的远程文件拷贝命令。其基本使用方法见《Linux scp命令》。这里纪录下平时在开发过程中遇到的用法，以为备忘。 应用背景：在我们实验室进行深度学习开发，实验室有一个公网ip，记为ipc，因为机房有若干服务器，因此在路由器上进行了端口映射，每个端口对应每个服务器，其中我们小组的服务器的端口号为A和B，如果现在需要从主机b的文件file传送到主机a中，可以在主机A中用下列命令，fetch文件b：1scp -P B user_name@ipc:/home/user_name/file a_dst_path 其中user_name@ipc:/home/user_name/file为主机B的用户名和文件地址，通过指定端口B来进行端口映射的选择，a_dst_path是本机A的保存地址。如果拷贝的是文件夹，则加上-r即可。]]></content>
      <categories>
        <category>常用linux指令</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[einsum的基础使用]]></title>
    <url>%2F2018%2F10%2F22%2Ftensorflow%2Ftf.einsum%2F</url>
    <content type="text"><![CDATA[前言einsum全称为Einstein summation convention，是一种求和的范式，在很多基于多维张量的张量运算库，如numpy,tensorflow,pytorch中都有所应用。einsum可以用一种很简单的，统一的方式去表示很多多维张量的运算。让我们以numpy中的einsum为例子，理解这种运算表达方式。 这里贴出numpy中的einsum的API：1numpy.einsum(subscripts, *operands, out=None, dtype=None, order='K', casting='safe', optimize=False) 其中关键的参数有subscripts用于指定计算模式,operands用于指定操作数，我们给个例子，如果现在我们有两个矩阵12345678910A = np.array([[1,2,3],[1,3,4],[2,3,4]])B = np.array([[9,2,4],[1,1,7],[5,2,4]])'''A -&gt; array([[1, 2, 3], [1, 3, 4], [2, 3, 4]])B -&gt; array([[9, 2, 4], [1, 1, 7], [5, 2, 4]])''' 如果我们现在想实现一个运算，如下公式所述: s(j) = \sum_{i=0}^{2} A[i,j]*B[i,j]我们利用einsum这种形式就能够很好的表达，如:1s = np.einsum('ij,ij-&gt;j',A,B) 其输出结果为1array([20, 13, 56]) 其中的subscripts参数就很好地描述了上述公式描述的运算过程，我们这里可以细究下这个参数。这个参数由三大部分构成，a,b-&gt;c其中a和b是描述的输入张量的索引，如上面的ij表示A和B张量的i行j列。c表示的是输出的索引，如上文中的j。当你指定了输出的索引之后，就可以把这个索引看成是固定的值了，因为他将会是作为一个自变量参数存在的，而可以把其他的索引变量（输入的索引变量）看成是循环变量。这个方式可以实现很多复杂的矩阵运算，如123a = np.arange(60.).reshape(3,4,5)b = np.arange(24.).reshape(4,3,2)np.einsum('ijk,jil-&gt;kl', a, b) Reference[1]. EINSUM IS ALL YOU NEED - EINSTEIN SUMMATION IN DEEP LEARNING]]></content>
      <categories>
        <category>TensorFlow Basic API</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>TensorFlow</tag>
        <tag>编程技巧</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tensorflow中的image预处理操作函数]]></title>
    <url>%2F2018%2F10%2F22%2Ftensorflow%2Ftf_image_prepocess_api%2F</url>
    <content type="text"><![CDATA[前言TensorFlow中有着一个image模块专门用于处理图片数据的预处理，里面定义了若干常见的图像预处理函数，让我们列举出来，介绍一下，API地为 tf.image. tf.image.adjust_brightness(images, delta) ：用于改变原图像的明亮度，也就是在原图像的基础上加上一个delta，于是我们有new_image = old_image+delta。 tf.image.adjust_contrast(images,contrast_factor)：改变原图片的对比度，公式为$(x - mean) * contrast\_factor + mean$, 其中的x为原输入，mean为每个通道的均值。 tf.image.adjust_jpeg_quality(images, jpeg_quality)：改变原jpeg图像的编码质量（不是很懂什么叫做编码质量，望指正），其中的jpeg_quality需在[0,100]这个区间内。 tf.image.adjust_gamma：伽马矫正，$\rm{Out} = \rm{Input}^{gamma}$ tf.image.adjust_hue(image,delta)：改变原图像的色彩(hue)，本函数将原RGB图像转换到HSV空间后，在hue通道上加上delta后，再转换回RGB空间。 tf.image.adjust_saturation(images,saturation_factor)：类似于改变色彩的函数，这个函数改变的是饱和度。 tf.image.central_crop(images,central_fraction)：对图像进行中心修剪，也就是按照一定比例保留中间的像素，去除中心之外的像素，注意输出图像和输入图像的尺寸是一样的，也就是说中心之外的像素置位了而已。其中central_fraction在[0,1]区间，表示的是中心保留的比例。 tf.image.convert_image_dtype(image,dtype,saturate=False)：改变图片的数据类型，其中dtype是目标数据类型，注意到这个转换不是单纯的数据类型转换，他会将浮点类型的图片归一化到[0,1]之间，整型的数据类型归一化到[0,MAX]之间，这里的MAX指的是不同整型数据类型能够表达的最大正数。注意到在浮点型转换到整型的过程中可能会出现上溢或者下溢的问题，如果指定了saturate=True将可以避免这个问题，它会在数据类型转换之前截断输入的值，比如输入是1.02，那么就会截为1.0 tf.image.decode_bmp,tf.image.decode_gif,tf.image.decode_jpeg,tf.image.encode_png，这些都是图片解码函数，传入图片的地址，解码出图片。可以用更为同一的接口tf.image.decode_image代替，它会自动判断图片的格式并且解码。 编码和解码函数TensorFlow提供了一些操作用于编码和解码JPEG，PNG格式的图片。编码后的图像用一个标量字符串张量(Scalar String Tensor，我觉得就是一个图片的地址吧)表示，解码后的图像用一个3D的uint8类型的张量表示，尺寸为[height,width,channels]。（PNG也可以支持uint16的数据类型）。这些编解码操作在一个时刻只能应用于一张图片，他们的输入和输出都是可变长的，如果你需要固定尺寸的图片，那么就对输出结果进行裁剪或者resize吧。相关函数有： tf.image.decode_bmp tf.image.decode_gif tf.image.decode_jpeg tf.image.encode_jpeg tf.image.decode_png tf.image.encode_png tf.image.decode_image resize操作resize操作用于将输入图像重新缩放或者放大到固定的尺寸，通常是在数据类型tf.float32情况下应用的。有个简便的函数tf.image.resize_images同时支持了4D和3D的张量作为输入并且输出。4D的张量指的是包括了batch，3D张量就仅仅是一张图片。其他的resize操作仅仅支持4D张量作为输入，如： tf.image.resize_area tf.image.resize_bicubic （双立方插值） tf.image.resize_bilinear （双线性插值） tf.image.resize_nearest_neighbor （最近邻插值） 裁剪Crop有些场景中需要对原输入图像进行裁剪，可能是随机裁剪，也可是中心对齐裁剪，TF提供了一系列的裁剪函数： tf.image.resize_image_with_crop_or_pad tf.image.central_crop tf.image.pad_to_bounding_box tf.image.crop_to_bounding_box tf.image.extract_glimpse tf.image.crop_and_resize 翻转，旋转和转置操作有些时候采取这些操作有利于数据的增广，增大训练集。这些函数输入都是4D张量。 tf.image.flip_up_down （向下翻转） tf.image.random_flip_up_down（随机上下翻转） tf.image.flip_left_right（向左右翻转） tf.image.random_flip_left_right（随机左右翻转） tf.image.transpose_image（图像转置，调换width和height轴） tf.image.rot90（顺时针方向旋转90°） 颜色空间变换提供了RGB颜色空间到HSV颜色空间的转换函数，必须在float32的数据类型下进行，可以考虑用tf.image.convert_image_dtype对整型输入进行转换。 tf.image.rgb_to_grayscale（RGB到灰度图） tf.image.grayscale_to_rgb（灰度图到RGB，注意，并不是伪彩色，而是单纯复制了三个通道而已） tf.image.hsv_to_rgb（HSV到RGB） tf.image.rgb_to_hsv（RGB到HSV） tf.image.convert_image_dtype（转换图片的数据类型） 图片调整TF提供了一系列的函数用于调整图片的基本参数，如：明亮度，对比度，色彩，饱和度等。每个操作都需要在预先定义好的参数，或者随机的参数（从一个预定义的区间中随机取）中完成，随机调整有利于在训练集中提高泛化性能。 tf.image.adjust_brightness（调整明亮度） tf.image.random_brightness（随机明亮度） tf.image.adjust_contrast（调整对比度） tf.image.random_contrast（随机对比度） tf.image.adjust_hue（调整色彩） tf.image.random_hue（随机色彩） tf.image.adjust_gamma（gamma矫正） tf.image.adjust_saturation（调整饱和度） tf.image.random_saturation（随机饱和度） tf.image.per_image_standardization（图片标准化，零均值单位方差） bounding box相关 tf.image.draw_bounding_boxes （绘制bounding box） tf.image.non_max_suppression （非极大抑制） tf.image.sample_distorted_bounding_box 解噪 tf.image.total_variation （计算图片总方差）]]></content>
      <categories>
        <category>TensorFlow Basic API</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tensorflow编程实践：结构化你的模型]]></title>
    <url>%2F2018%2F10%2F22%2Ftensorflow%2Fstructure_your_model%2F</url>
    <content type="text"><![CDATA[前言这篇文章是翻译自《Structuring Your TensorFlow Models》，这篇文章主要描述了在TensorFlow中如何更好地结构化地定义你的网络模型，以便于更好地扩展和调试。我们将会发现，采用这种构建方法，可以将整个模型变得模块化。如果对这篇文章有着更好的建议，请联系我，谢谢。 在TensorFlow中定义你的模型很容易导致一个臃肿复杂，难以维护的代码，这个是一个很糟糕的事情，因为深度模型本身就难以调试和差错，因此在模型的搭建过程中应该尽可能的用模块化的方法去搭建模型和结构化模型。如何以一种高可读性和可复用性的手段结构化你的代码呢？如果你急于求成，可以直接参考working example gist。也可以参考这篇关于如何在TensorFlow中实现快速原型的文章fast prototyping，结构化模型的基本思想就在这里描述了。 定义计算图给每一个模型定义一个类是很科学而且高效的做法。那么，用什么作为这个类的接口呢？通常来说，你的模型会和一些输入数据和目标数据(target)的占位符(placeholder)存在关联，毕竟你需要以此来喂(feed)数据，并且提供一个接口给主程序训练(training)，评估(evaluation)和推理(inference)。也就是说，我们这个类，至少要提供这几种接口才是一个较为完整的深度网络模型类： 喂数据的接口， 如输入和目标的占位符等。 提供给主程序调用的训练，评估和推理接口 （可选）一个网络中间输出结果，这种用法类似与pytorch和keras的做法，把一些成熟的网络模块化后直接输出中间处理结果，以便于更好的模块化。 我们观察一下例子：12345678910111213141516171819202122232425class Model: def __init__(self, data, target): data_size = int(data.get_shape()[1]) target_size = int(target.get_shape()[1]) weight = tf.Variable(tf.truncated_normal([data_size, target_size])) bias = tf.Variable(tf.constant(0.1, shape=[target_size])) incoming = tf.matmul(data, weight) + bias self._prediction = tf.nn.softmax(incoming) cross_entropy = -tf.reduce_sum(target, tf.log(self._prediction)) self._optimize = tf.train.RMSPropOptimizer(0.03).minimize(cross_entropy) mistakes = tf.not_equal( tf.argmax(target, 1), tf.argmax(self._prediction, 1)) self._error = tf.reduce_mean(tf.cast(mistakes, tf.float32)) @property def prediction(self): return self._prediction @property def optimize(self): return self._optimize @property def error(self): return self._error 这是一个基本的关于如何在TF中定义模型的代码，然而，这个代码还是存在一些问题的。最明显的问题是，这整个模型的计算图都定义在了一个函数里面，也就是构造器。如果你的模型变得很复杂，这个构造器将变得异常臃肿，这样既不是可读性强的，也不是可复用性强的编程习惯。（译者：而且，这里还有一个问题，如果按照以上的代码去进行整个模型的图的构建，那么不管我们是不是需要用到整个模型的每个子模型，他都会给我一股脑地预先构建出来。这样其实不是一个很好的方案，因为很多时候，模型很大，有很多分支，而且训练阶段也有不止一个阶段，每个阶段可能用到不同的分支。因此并没有必要一股脑把所有分支给构建出来，用本文的思路可以实现很好的结构化模型。） 利用类属性(Properties)去结构化你的模型吧简单地将你的构建计算图的代码从构造器中分离出来不能解决任何问题，因为每一次这个函数被调用的时候，这个计算图都会添加新的节点。这并不是我们想要的，我们需要的是这个计算图只会在我们第一次调用某个模块的构建的时候在整个计算图中添加新的节点，在第二次或者更多次的时候，不需要其再次添加相同的节点，这个被称之为惰性加载(lazy-loading)。1234567891011121314151617181920212223242526272829303132333435class Model: def __init__(self, data, target): self.data = data self.target = target self._prediction = None self._optimize = None self._error = None @property def prediction(self): if not self._prediction: data_size = int(self.data.get_shape()[1]) target_size = int(self.target.get_shape()[1]) weight = tf.Variable(tf.truncated_normal([data_size, target_size])) bias = tf.Variable(tf.constant(0.1, shape=[target_size])) incoming = tf.matmul(self.data, weight) + bias self._prediction = tf.nn.softmax(incoming) return self._prediction @property def optimize(self): if not self._optimize: cross_entropy = -tf.reduce_sum(self.target, tf.log(self.prediction)) optimizer = tf.train.RMSPropOptimizer(0.03) self._optimize = optimizer.minimize(cross_entropy) return self._optimize @property def error(self): if not self._error: mistakes = tf.not_equal( tf.argmax(self.target, 1), tf.argmax(self.prediction, 1)) self._error = tf.reduce_mean(tf.cast(mistakes, tf.float32)) return self._error 这个比第一个例子好多了，你的代码现在可以在类中的方法中结构化，因此你只需要单独地关注某个部分就可以了。然而，这个代码为了实现这个惰性加载的逻辑，额外多出了很多判断的分子，这个仍然是有些臃肿的，我们利用python中自带的修饰器的性质，可以进行一些修改。 惰性类属性修饰器python是一种很灵活的语言，在下一个例子中，我将展示给你如何从上一个例子的代码中除去冗余的代码。我们将会使用一个表现得像是@property但是只会实际上调用这个函数一次的修饰器。如果你对如何定制修饰器不熟悉，也许你可以先参考这篇文章python修饰器教程。采用这种方法，可以有效地减少一些为了实现惰性加载的逻辑而额外多出的代码，如if not self._error:这一部分。我们观察一下我们需要的修饰器代码：123456789101112import functoolsdef lazy_property(function): attribute = '_cache_' + function.__name__ @property @functools.wraps(function) def decorator(self): if not hasattr(self, attribute): setattr(self, attribute, function(self)) return getattr(self, attribute) return decorator 采用这个修饰器，我们的例子可以被简化为下面的代码：1234567891011121314151617181920212223242526272829class Model: def __init__(self, data, target): self.data = data self.target = target self.prediction self.optimize self.error @lazy_property def prediction(self): data_size = int(self.data.get_shape()[1]) target_size = int(self.target.get_shape()[1]) weight = tf.Variable(tf.truncated_normal([data_size, target_size])) bias = tf.Variable(tf.constant(0.1, shape=[target_size])) incoming = tf.matmul(self.data, weight) + bias return tf.nn.softmax(incoming) @lazy_property def optimize(self): cross_entropy = -tf.reduce_sum(self.target, tf.log(self.prediction)) optimizer = tf.train.RMSPropOptimizer(0.03) return optimizer.minimize(cross_entropy) @lazy_property def error(self): mistakes = tf.not_equal( tf.argmax(self.target, 1), tf.argmax(self.prediction, 1)) return tf.reduce_mean(tf.cast(mistakes, tf.float32)) 注意到我们在关于图构建的函数中都是用了这个修饰器的。需要另外注意的是，当你运行tf.initialize_variables()以初始化变量的时候，务必留意你是否已经定义了这个计算图，否则是会报错的。 更进一步，用名字空间(Scopes)组织计算图我们现在有了一个更为简洁干净的方法去定义我们的模型，但是这个计算图仍然还是太过于拥挤了，如果你曾经用tensorboard可视化过整个计算图，你肯定明白我说的是什么意思，整个计算图将会包括很多小节点之间的互连。解决这个问题可以通过用一个“包裹”把这些互连的内容给打包起来，通过利用tf.name_scope()或者tf.variable_scope()你就可以实现这个功能，这两者的具体区别我们以后再谈，姑且看下这两个函数怎么使用。在计算图中，你可以指定某些节点被聚合在一起。我们而且，可以让我们的修饰器自动地实现这个功能，而不需要每个都人工手动完成。1234567891011121314import functoolsdef define_scope(function): attribute = '_cache_' + function.__name__ @property @functools.wraps(function) def decorator(self): if not hasattr(self, attribute): with tf.variable_scope(function.__name): setattr(self, attribute, function(self)) return getattr(self, attribute) return decorator 因为每个模块都是有功能性的特异性的，因此给每个模块一个新的名字。除此之外，这个模型和之前那个完全一样。我们甚至可以走得更远一点，我们可以让@define_scope修饰器可以传递参数给tf.variable_scope()，比如说给定义一个默认的初始化之类的。如果你对这方面的感兴趣，请移步blog_tensorflow_scope_decorator.py。]]></content>
      <categories>
        <category>TensorFlow Practice</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>TensorFlow</tag>
        <tag>结构化模型</tag>
        <tag>编程技巧</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[生成模型和判别模型的区别]]></title>
    <url>%2F2018%2F10%2F22%2Fbasic%2Fgenerative_discriminative_model%2F</url>
    <content type="text"><![CDATA[前言 机器学习中有两种大类的模型，分别是生成模型和判别模型，其分别代表了不同的预测思想，我们这里讨论一下两者的异同。如有谬误，请联系指正。转载请注明出处。 联系方式：e-mail: FesianXu@163.comQQ: 973926198github: https://github.com/FesianXu 机器学习的目标首先，我们先要讨论整个机器学习的目标，在应用中最常见到的分类问题中，我们需要根据现有样本$x_i \in R^n$预测出其标签$y_i \in \{ 0,1,\dots,m\}$，因此我们可以选择学习出条件概率$P(y_i|x_i), x_i \in R^n, y_i \in \{0,1,\dots.m\}$，如softmax分类器，logistic回归，亦是或者学习出样本特征$x_i$到标签$y_i$的直接映射（比起前者没有概率，而是直接的一个结果），如感知器Perceptron，SVM支持向量机。学习出了条件概率或者是其映射之后，我们就可以根据其样本特征$x_i$预测其标签了$y_i$。 生成模型 or 判别模型这里，我们直接给出两者的定义： 判别模型： 模型直接学习出条件概率$P(y_i|x_i)$，模型包括kNN，感知机，决策树，逻辑回归，最大熵模型，SVM，提升方法，条件随机场，神经网络，··· 生成模型： 模型学习出联合概率分布$P(x,y)$，然后根据贝叶斯公式，得出条件概率分布$P(y|x) = \frac{P(x,y)}{\sum_iP(x,y_i)}$，模型包括朴素贝叶斯法、隐马尔科夫模型、混合高斯模型、AODE、Latent Dirichlet allocation（unsup）、Restricted Boltzmann Machine，··· 于是，两者的区别就是是否需要学习出联合概率分布$P(x,y)$。我们这里举一个维基百科里面的例子： 假如我们现在有四个样本：$(x,y)=\{(0,0), (0,0), (1,0), (1,1)\}$ 在判别模型眼中：$P(Y|X)$ y=0 y=1 x=0 1 0 x=1 1/2 1/2 而在生成模型眼中：$P(X,Y)$ y=0 y=1 x=0 1/2 0 x=1 1/4 1/4 而在博客《机器学习之判别式模型和生成式模型》中，举了一个很好的例子描述这两者的区别，这里引用如下： 判别模型：要确定一个羊是山羊还是绵羊，用判别模型的方法是从历史数据中学习到模型，然后通过提取这只羊的特征来预测出这只羊是山羊的概率，是绵羊的概率。 生成模型：利用生成模型是根据山羊的特征首先学习出一个山羊的模型，然后根据绵羊的特征学习出一个绵羊的模型，然后从这只羊中提取特征，放到山羊模型中看概率是多少，在放到绵羊模型中看概率是多少，哪个大就是哪个。 Reference Generative Model Wikipedia 知乎， 机器学习“判定模型”和“生成模型‘有什么区别？ 机器学习之判别式模型和生成式模型]]></content>
      <categories>
        <category>Basic concept</category>
      </categories>
      <tags>
        <tag>生成模型</tag>
        <tag>判别模型</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《贝叶斯之旅》第四讲，曲线拟合问题与L2正则]]></title>
    <url>%2F2018%2F10%2F22%2Fbayesian%2Fcurve_fitting%2F</url>
    <content type="text"><![CDATA[前言我们在[1]中曾经谈到了在贝叶斯理论下指导的曲线拟合问题以及基于此的L2正则化解释，其实，对于L2正则化还可以从模型复杂度的角度进行解释，现在，我们针对非贝叶斯观点的曲线拟合问题和L2正则进行讨论。 如有谬误，请联系指正。转载请注明出处。 联系方式：e-mail: FesianXu@163.comQQ: 973926198github: https://github.com/FesianXu 曲线拟合回想[1]我们谈到的多项式曲线拟合问题，我们这里重新描述一遍：假设我们有一个训练集，其中有N个观察值，其自变量x写作$\mathbf{x} \equiv (x_1,\cdots,x_N)^T$，同时，对应的观察因变量值y写作$\mathbf{t} \equiv (t_1,\cdots,t_N)^T$。如下图是一组生成的数据，绿线为通过函数$f(x)$生成的，真实的生成曲线，蓝点是从绿线上取值后添加上噪声数据得到的。（这里的噪声可能来自于随机过程中的噪声，也可能是因为存在一些相关的变量没有被观察到）我们的目标，就是利用训练集来训练一个模型，对于一个新的$\hat{x}$输入，可以预测出其$\hat{t}$。这个过程中，将会隐式地学习到用来生成这个绿色曲线的$f(x)$。如果我们不考虑我们预测的不确定性，那么我们直接就可以采用最小化误差函数的方法进行模型参数值估计。我们假设模型为多项式模型，如下所示： y(x,\mathbf{w})=w_0+w_1x+w_2x^2+\cdots+w_Mx^M =\sum_{j=0}^M w_jx^j \tag{1.1}注意到，这个模型是关于$\mathbf{w}$的线性模型，但是并不是关于$x$的线性模型，像这种多项式，关于未知参数呈现线性的模型统称为线性模型(Linear Model)。为了让我们的模型尽可能的接近训练集的数据，我们引入一个所谓的误差函数(error function)去度量预测值和真实值之间的距离，一般我们可以采用平方和函数作为误差函数，从[1]中，我们将会发现，当数据噪声满足0均值高斯分布时，可以推出平方和损失函数。 E(\mathbf{w}) = \frac{1}{2}\sum_{n=1}^N \{y(x_n,\mathbf{w})-t_n\}^2 \tag{1.2}下图展示了计算预测值和真实值之间的距离示意图，绿色距离之和即为所求。 因为式子(1.2)是一个关于$\mathbf{w}$的二次函数，关于这些系数的导数将会是一个关于$\mathbf{w}$线性的，通过令其梯度的每一个分量的导数为0，我们可以知道其有一个唯一解$\mathbf{w}^*$，这个可以完全通过闭式解得到。当然也可以通过基于梯度下降的方法得到近似解[3]。 模型复杂度接下来的问题就在于如何选取超参数$M$。如下图所示，M太大，将会导致模型复杂度太大，使得模型容易过拟合[4]；然而，如果M太小，则模型的复杂度太低，拟合能力差，导致欠拟合。但是，我们需要注意的是，导致模型过拟合和欠拟合的，不仅仅和超参数的设置有关，而且很重要的一点是：和你训练集的好坏，规模也有重要的关系。如下图所示，左图是$N=15$个样本点，而右图是$N=100$个样本点，同样地采用了$M=9$的超参数设置，我们可以明显地看到，样本数量更多的一个，越符合真实的数据生成函数。不失一般地说，模型容量越大，模型复杂度越高，就需要更多的数据进行训练，以排除噪声的影响。 我们再次回到$M=0,1,6,9$的四种情况，我们分别观察它的$\mathbf{w}^*$系数，我们有： 不难发现，M越大，其参数$w_i^*$的幅度也越大，并且是正负交错的，这使得拟合曲线有着极大的震荡，能够在训练集上精确地拟合每一个训练数值，导致其泛化性能极差。在[1]中我们将会对$\mathbf{w}$进行一个先验假设，通过贝叶斯理论的方法减缓这种情况的发生。然而，现在我们可以同样完成这一件事情，通过添加一个惩罚项(penalty)即可，我们称之为正则项(regularization)。形式如： \tilde{E}(\mathbf{w}) = \frac{1}{2}\sum_{n=1}^N\{y(x_n,\mathbf{w})-t_n\}^2+\frac{\lambda}{2}\mathbf{w}^T\mathbf{w}其中的惩罚项(正则项)$\mathbf{w}^T\mathbf{w}=||\mathbf{w}||^2=w_0^2+\cdots+w_M^2$，然后$\lambda$调节其和平方和损失之间的重要性比例。这种正则称之为L2正则化，因为求模操作也被称之为L2范式的原因。通过引入这种正则操作，使得参数能够尽可能的小，而不会导致上面谈到的问题。这种二次正则子称为岭回归(ridge regression)，在神经网络相关文献中，也称之为权值衰减(weight decay)（注意和学习率指数衰减分开）。 参考我们在[4]中曾经讨论过的，我们一般有两种方式限制模型容量，通过设置超参数进而控制模型的假设空间太困难了，比如这里的$M$的选取就是一个困难的事。因此我们往往采取第二种做法，添加正则项对模型进行偏好排除，我们设置一个足够大的$M$，当然也不能太大，但是起码不用担心其不够容量对数据进行拟合即可，然后添加合适的正则项进行模型的偏好排除就可以较为容易地控制模型容量。这个方法也是在深度学习中经常使用的。 最后我们定量地观察下正则项前系数$\lambda$对参数$w_i^*$的影响，如下图所示,当$\lambda \rightarrow 1$的时候，可以观察到参数的确都缩小到了合适的范围。 Reference[1] 《贝叶斯曲线拟合以及对L2正则化的贝叶斯解释》[2] Bishop C M. Pattern recognition and machine learning (information science and statistics) springer-verlag new york[J]. Inc. Secaucus, NJ, USA, 2006.[3] 随机梯度下降法，批量梯度下降法和小批量梯度下降法以及代码实现[4] 机器学习模型的容量，过拟合与欠拟合]]></content>
      <categories>
        <category>Bayesian Theory</category>
      </categories>
      <tags>
        <tag>贝叶斯理论</tag>
        <tag>统计学习方法</tag>
        <tag>L2正则</tag>
        <tag>曲线拟合</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《贝叶斯之旅》第三讲，贝叶斯曲线拟合以及对L2正则化的贝叶斯解释]]></title>
    <url>%2F2018%2F10%2F22%2Fbayesian%2Fbayesian_curve_fitting%2F</url>
    <content type="text"><![CDATA[前言 在以前文章中，我们讨论过《概率学派和贝叶斯学派的区别》和《 &lt;机器学习系列&gt; 线性回归模型》，这里我们讨论下曲线拟合问题中的数据点的噪声问题，以及根据贝叶斯理论的L2正则化解释。 如有谬误，请联系指正。转载请注明出处。 联系方式：e-mail: FesianXu@163.comQQ: 973926198github: https://github.com/FesianXu 曲线拟合问题这里的曲线指的是多项式曲线（polynomial curve）1，如下图所示： 一般来说，概率学派按照最小化平方和误差函数，如下所示，来进行参数的学习的。 \mathcal{T}_{\theta} = \arg \min_{\theta} \mathcal{L}(\hat{y},y) \\ \hat{y}_j = \sum_{i=0}^N \theta_i x_{(i,j)}^{i} = y(x;\theta)\\ \mathcal{L}(\hat{y}, y) = \dfrac{1}{2}||\hat{y}-y||^2 \tag{1.1}$x_{(i,j)}$表示第$j$个样本的第$i$维数据值。更新策略采用梯度下降法[4]即可更新参数，达到收敛。 用概率角度看待曲线拟合，考虑下噪声吧~但是按照上面策略进行曲线拟合是没有考虑到数据的不确定性（uncertainty）的，这种不确定性体现在数据是添加了噪声的，而基于直接估计出一个点，然后直接拟合的方式没有考虑到这种噪声。为了描述这种不确定性，我们接下来以一种概率的角度去看待曲线拟合问题。 假设我们通过多项式模型预测出来的并不是一个单纯的数字，而是一个分布，一般来说我们将其假设为是一个均值为$t$（也就是预测目标值），方差为$\sigma^2$（$\beta=\dfrac{1}{\sigma^2}$，$\beta$称之为精确度precision），因此预测出来的分布如下式所示： p(t|x, \textbf{w}, \beta) = \mathcal{N} (t|y(x, \textbf{w}), \beta^{-1}) \tag{1.2}我们之所以假设为是高斯分布，是因为我们假设数据添加的噪声是高斯噪声，既是： \mathbf{x}_{\rm{observe}} = \mathbf{x}_{\rm{real}}+\mathcal{N}(\mu,\sigma^2) \tag{1.3 数据的噪声分解模型}图像看起就更加直观了： 可以看出，对于某一个预测，其为一个分布（蓝色线），其中预测的均值的预期就是观察值点A，可以看出，参数$\beta$决定了其置信范围$2\sigma$的大小。这个$2\sigma$的范围可以认为是认为假设的，噪声的主要范围。 如果采用频率学派中的观点，那么就会采用极大似然法进行参数估计。似然函数如下所示： p(\textbf{t}|\textbf{x},\textbf{w}, \beta) = \prod_{i=0}^N \mathcal{N} (t_n | y(x_n, \textbf{w}), \beta^{-1}) \tag{1.4}为了计算方便转化为对数似然后，有： \mathcal{L} = \ln p(\textbf{t}|\textbf{x},\textbf{w}, \beta) \\ = -\dfrac{\beta}{2} \sum_{n=1}^N \{y(x_n, \textbf{w})-t_n\}^2 + \dfrac{N}{2}\ln \beta - \dfrac{N}{2} \ln (2\pi) \tag{1.5}为了估计出$\mathbf{w}$，我们用$\mathcal{L}$对$\mathbf{w}$求偏导数，并且令其为0。我们可以发现(1.5)中的后两项和$\mathbf{w}$并没有关系，因此可以舍弃。同时，因为$\beta$的取值并不会影响$\mathbf{w}$的极值点，因此可以令其为$\beta=1$。最终，我们有： \mathcal{L} = -\dfrac{1}{2} \sum_{n=1}^N \{y(x_n, \textbf{w})-t_n\}^2 \\ \mathcal{T} = \max_{\mathbf{w}} \mathcal{L} = \min_{\mathbf{w}} \mathcal{-L} \tag{1.6}不难发现，其实(1.6)式子就是平方和损失，因此我们得出结论：$\nabla$平方和损失，是在假设数据噪声符合0均值高斯分布的情况下推导出的。$\nabla$ 当然，这里的精度$\beta$也可以用最大似然法估计，有： \frac{1}{\hat{\beta}} = \frac{1}{N} \sum_{n=1}^N \{y(x_n,\hat{\mathbf{w})}-t_n\}^2 \tag{1.7}其中的$\hat{\mathbf{w}}$是对权值的估计。 对参数引入先验假设，向着贝叶斯的更进一步注意到我们之前讨论的都是没有对参数$\mathbf{w}$进行任何假设的，也就是说其可以符合任何分布。这个很不贝叶斯，如果我们能对参数引入合理的先验假设，那么就能提高其泛化性能[5]。我们不妨假设$\mathbf{w}$符合高斯分布，其均值为0，方差为一个对角矩阵（既是假设每个参数之间独立，其中$\alpha$控制了每个参数的range），数学表达为： p(\mathbf{w}|\alpha) = \mathcal{N}(\mathbf{w}|\mathbf{0},\alpha^{-1}\mathbf{I}) \\ = (\frac{\alpha}{2\pi})^{(M+1)/2} \rm{exp}\{-\frac{\alpha}{2}\mathbf{w}^T\mathbf{w}\} \tag{2.1 对参数的先验假设}其中$M$为多项式次数。如同$\alpha$这样的，控制着整个模型的超空间形状的参数，称之为超参数(hyperparameters)。引入了这个先验假设后，我们模型的后验： p(\mathbf{w}|\mathbf{x},\mathbf{t},\alpha,\beta) \propto p(\mathbf{t}|\mathbf{x},\mathbf{w},\beta)p(\mathbf{w}|\alpha) \tag{2.2}我们现在可以在给定了训练集$\{\mathbf{x},\mathbf{t}\}$的情况下，通过找到一个最可能的$\mathbf{w}$来估计出$\mathbf{w}$。换句话说，我们可以最大化这个后验概率，这个技术称之为最大后验概率法(MAximum Posterior,MAP)。取(2.2)的负对数，我们有: \ln{p(\mathbf{w}|\mathbf{x},\mathbf{t},\alpha,\beta)} \propto \ln{p(\mathbf{t}|\mathbf{x},\mathbf{w},\beta)}+\ln{p(\mathbf{w}|\alpha)} \tag{2.3}结合(1.6)和(2.1)，舍弃掉和$\mathbf{w}$无关的项之后，我们有： \frac{\beta}{2}\sum_{n=1}^N \{y(x_n,\mathbf{w})-t_n\}^2+\frac{\alpha}{2}\mathbf{w}^T\mathbf{w} \\ \Rightarrow \frac{1}{2}\sum_{n=1}^N \{y(x_n,\mathbf{w})-t_n\}^2+\frac{\alpha}{\beta}\mathbf{w}^T\mathbf{w}令$\gamma=\dfrac{\alpha}{\beta}$，于是我们就有了在正则项中最常见到的L2正则项$\dfrac{\gamma}{2}\mathbf{w}^T\mathbf{w}$了。于是我们得到结论：$\nabla$在贝叶斯理论中，L2正则项是在参数$\mathbf{w}$符合0均值高斯分布的情况下推导出来的，其系数$\gamma$决定了正则的程度。$\nabla$ 最终一步，贝叶斯曲线拟合在上一步中，虽然我们根据最大后验法估计出了$\mathbf{w}$，但是对于曲线拟合来说，这并不是我们的最终目标，我们的最终目标是估计出目标值$\hat{\mathbf{t}}$出来。在完全的贝叶斯处理过程中，我们的估计出来的$\mathbf{w}$是一个分布，为了得到预测值$\hat{\mathbf{t}}$，我们要用概率的加法和乘法法则，对所有可能的$\mathbf{w}$进行积分，得到目标值。这个操作将在贝叶斯理论中一直沿用。具体到我们的曲线拟合的例子，当我们给定了训练集$\{\mathbf{x},\mathbf{t}\}$的时候，当输入一个新的输入$x$的时候，我们期望得到其预测值$t$。也就是说我们需要得出$p(t|x,\mathbf{x},\mathbf{t})$，由概率的基本和积定理有： p(t|x,\mathbf{x},\mathbf{t}) = \int p(t|x,\mathbf{w})p(\mathbf{w}|\mathbf{x},\mathbf{t}) \rm{d} \mathbf{w} \tag{3.1}因为采用了共轭先验[6]假设，因此我们的后验概率同样是一个高斯分布。也即是： p(t|x,\mathbf{x},\mathbf{t}) = \mathcal{N}(t|m(x),s^2(x)) \tag{3.2}这个时候，均值和方差可以给定为[1] page 31（暂时并不知道怎么算出来的） m(x) = \beta \phi(x)^T \mathbf{S} \sum_{n=1}^N \phi(x_n) t_n \tag{3.3} s^2(x) = \beta^{-1}+\phi(x)^T\mathbf{S}\phi(x) \tag{3.4} \mathbf{S}^{-1} = \alpha\mathbf{I}+\beta \sum_{n=1}^N \phi(x_n)\phi(x)^T \tag{3.5}其中的$\phi(x)=x^i,i=0,\cdots,M$。可以观察到，这个均值$m(x)$是取决于$x$的，在式子(3.4)中的第一项，代表了因为目标的噪声所带来的不确定性。而第二项，表示了因为$\mathbf{w}$的不确定所带来的不确定性，这个正是贝叶斯处理所带来的结果。下图的绿线表示了生成样本的基线，蓝色样本表示基线上添加高斯噪声的结果，红线是预测的均值，红区域是正负1个标准差的区域。 Reference[1] Bishop C M. Pattern recognition and machine learning (information science and statistics) springer-verlag new york[J]. Inc. Secaucus, NJ, USA, 2006.[2] 《概率学派和贝叶斯学派的区别》[3] 《 &lt;机器学习系列&gt; 线性回归模型》[4] 《随机梯度下降法，批量梯度下降法和小批量梯度下降法以及代码实现》[5] 《机器学习模型的容量，过拟合与欠拟合》[6] 《先验概率、后验概率以及共轭先验》 1. A curve obtained by fitting polynomials to each ordinate of an ordered sequence of points. 指的是用多项式函数$f(\textbf{X}; \theta)=\sum_{i=0}^N \theta_i x_i^{i}, \textbf{X} \in \mathbb{R}^N$。其中如果指数全部变为1而不是$i$，则退化为线性回归。 &#8617;]]></content>
      <categories>
        <category>Bayesian Theory</category>
      </categories>
      <tags>
        <tag>贝叶斯理论</tag>
        <tag>统计学习方法</tag>
        <tag>概率论</tag>
        <tag>贝叶斯曲线拟合</tag>
        <tag>正则</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用pytorch踩过的坑]]></title>
    <url>%2F2018%2F10%2F22%2Fpytorch%2Fpytorch_debug%2F</url>
    <content type="text"><![CDATA[前言 pytorch的交叉熵pytorch的交叉熵nn.CrossEntropyLoss在训练阶段，里面是内置了softmax操作的，因此只需要喂入原始的数据结果即可，不需要在之前再添加softmax层。这个和tensorflow的tf.softmax_cross_entropy_with_logits如出一辙.[1][2] Reference[1]. Why does CrossEntropyLoss include the softmax function?[2]. Do I need to use softmax before nn.CrossEntropyLoss()?]]></content>
      <categories>
        <category>Pytorch Basic API</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《贝叶斯之旅》第二讲，分类问题的两大过程，推理和决策]]></title>
    <url>%2F2018%2F10%2F21%2Fbayesian%2Fcls_stage%2F</url>
    <content type="text"><![CDATA[前言前面[1]我们介绍了贝叶斯决策的一些知识，介绍了基于最小化分类错误率和最小化分类损失的两种决策准则，接下来，我们简单讨论下分类问题中的二个步骤，推理和决策。 如有谬误，请联系指正。转载请注明出处。 联系方式：e-mail: FesianXu@163.comQQ: 973926198github: https://github.com/FesianXu 分类问题我们在之前的文章中已经介绍过分类问题了，简单的说就是给定一个样本$\bf{x} \in \mathbb{R}^n$，将其划分到有限的标签集$\bf{Y} \in \{0,1,\cdots,m\}$中。通常来说，我们可以将整个分类问题划分为两个独立的过程，分别是推理（inference）和决策(decision)阶段。在推理阶段，我们通过已有的训练集，学习到后验概率$p(\mathcal{C}_k|\bf{x})$，或者也可以通过学习联合概率分布$p(\mathcal{C}_k, \bf{x})$，然后也可以得到后验概率。而接下来，在决策阶段，就根据这个后验概率，对样本的类别进行判断决策。这个决策过程可以参考文章[1]的讨论。 注意到，很多时候，这两个过程可以合在一起，将问题简化为成：学习一个映射$f(\bf{x}) \in \mathbb{R}^m, \bf{x} \in \mathbb{R}^n$，直接将样本映射到类别标签。这个过程中，将不会涉及到任何的后验概率等，而是直接得出预测结果，这个函数因此称之为判别函数(Discriminant function)。[2] page 43 事实上，这些讨论过的方法都可以用来解决分类问题，并且在实际应用中都有所应用，我们按照复杂程度进行降序排列之后，有： 通过解决推理问题之后，我们可以给每一个类别估计出类条件概率$p(\mathbf{x}|\mathcal{C}_k)$，同时，先验概率$p(\mathcal{C}_k)$也很容易可以估计出来，然后通过贝叶斯公式我们可以得到后验概率： p(\mathcal{C}_k | \mathbf{x}) = \frac{p(\mathbf{x}|\mathcal{C}_k)p(\mathcal{C}_k)}{p(\mathbf{x})} \tag{1.1}我们有: p(\mathbf{x}) = \sum_{k} p(\mathbf{x}|\mathcal{C}_k)p(\mathcal{C}_k) \tag{1.2 对输入分布进行建模}等价地，我们可以对联合概率密度$p(\mathbf{x},\mathcal{C}_k)$进行建模，然后进行标准化后得到后验概率。像这种显式地或者隐式地对输入和输出进行概率分布建模的模型，称之为生成模型(generative models)，因为从这个联合分布中进行采样可以生成输入空间中的一些虚假生成数据(synthetic data)。 通过解决推理问题后，得到后验概率$p(\mathcal{C}_k|\mathbf{x})$，然后通过决策论进行类别判断。这种模型称之为判别模型(Discriminative model)。 寻找一个函数$f(\mathbf{x})$，称之为判别函数，直接将输入的$\mathbf{x}$映射到一个类别标签上，比如SVM分类器等。在这个情形下，并没有用到任何概率，也就是说我们对预测的结果其实是没有办法判断可靠程度的。 我们接下来分别讨论下这三种方法的优劣点。 孰优孰劣，判别模型和生成模型生成模型生成模型是对于数据量需求最高的，同时运算量也是最大的，因为其需要训练出包含$\mathbf{x}$和$\mathcal{C}_k$的联合分布，如果数据量不够，将会导致严重的过拟合现象[3]。对于很多应用下来说，$\mathbf{x}$是一个维度很高的特征向量，因此为了使得类条件概率得到一个较为合理的精度，就需要很多的数据量进行计算。但是，生成模型也有一些很好的性质，比如说可以从中进行采样生成出一些假数据，这个应用目前在很多image inpainting[4]，style transfer[5]任务中经常用到。而且，因为通过联合概率分布可以通过式子(1.2)计算出边缘概率分布$p(\mathbf{x})$。这个输入空间的边缘概率分布很有用，因为其可以判断输入的新数据是否是一个所谓的离群点(outlier)，离群点如下图所示。这个就是所谓的离群点检测(outlier detection)或者称之为异常检测(novelty detection)，这个在网络欺诈预测，银行欺诈预测，电子垃圾邮件检测中很有用。 判别模型在分类任务中，很多时候你只是做个分类而已，并不用进行离群点检测，也不需要生成虚假样本.这个时候，如果还用生成模型去进行后验概率的估计，就浪费了很多资源。我们观察下图，我们可以发现，类条件概率其实和后验概率并没有必然的影响。这个时候，你就需要采用判别模型。不仅如此，采用了判别模型还有一个好处就是，可以利用所谓的拒绝域(reject option)把一些过于边缘的判断拒绝掉。比如我们仅有10%的把握判断某人为癌症患者，那么我们就情愿不做这个判断，交给更为权威的人或者系统进行下一步的处理。如下图所示，绿色的水平线表示拒绝水平，只有后验概率高于这个水平线，才能认为是可靠的判断。我们将会看到，在基于判别函数的情况下，因为并没有概率的存在，因此并不能进行这种操作。 判别函数方法有比以上俩种方法更为简单，计算量更少的方法，那就是判别函数法。在这个情况下，因为是直接用训练数据拟合一个函数$f(\mathbf{x})$对样本进行分类，因此无法得到后验概率$p(\mathcal{C}_k|\mathbf{x})$。在这个方法中，只能最小化分类错误率，而没法给不同类型的分类错误进行区别[1]，采用最小化分类风险，这是个遗憾的地方。 Reference[1] 《贝叶斯之旅||第一讲，贝叶斯决策》 [2] Bishop C M. Pattern recognition and machine learning (information science and statistics) springer-verlag new york[J]. Inc. Secaucus, NJ, USA, 2006. [3] 《机器学习模型的容量，过拟合与欠拟合》 [4] 《基于深度学习的Image Inpainting (图像修复)论文推荐(持续更新)》 [5] 《Image Style Transfer》]]></content>
      <categories>
        <category>Bayesian Theory</category>
      </categories>
      <tags>
        <tag>贝叶斯理论</tag>
        <tag>统计学习方法</tag>
        <tag>概率论</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《贝叶斯之旅》第一讲，贝叶斯决策]]></title>
    <url>%2F2018%2F10%2F21%2Fbayesian%2Fbayesian_decision%2F</url>
    <content type="text"><![CDATA[前言在机器学习中，有两大门派，分别是频率学派和贝叶斯学派，在现在深度学习大行其道的时代下，数据量空前庞大，频率学派占据了比较大的优势，而贝叶斯学派似乎有点没落，然而，贝叶斯理论在机器学习中是有着很重要的地位的，它从理论上揭示了模型为什么可以工作，为什么会fail，在数据量必须小的一些任务中，通常也可以表现得比频率学派的好，让我们开始我们的贝叶斯之旅吧。这一讲，主要阐述的是在贝叶斯的观点中，我们如何根据现有的数据和假设，对未知的样本进行分类决策。 如有谬误，请联系指正。转载请注明出处。 联系方式：e-mail: FesianXu@163.comQQ: 973926198github: https://github.com/FesianXu 为什么要贝叶斯我们在以前的文章《概率派和贝叶斯派的区别》中，曾经讨论过频率学派和贝叶斯学派看待未知模型参数的一些观点，我们这里简单描述下就是： 频率学派相信我们的模型参数尽管未知，但是其是有一个真实的值$\theta$的，只要我们的样本足够多，我们就可以准确无偏地估计出这个真实的值出来；而贝叶斯学派相信我们的模型的未知参数是一个随机变量，而不是一个简简单单的值，因此是符合一个分布的。也就是说，基于我们现有的样本数据，我们对模型中的未知参数的估计都是估计出这些未知参数先验分布的一些参数而已，比如高斯分布的均值和协方差矩阵等等，在贝叶斯学派眼中，模型的参数本身就不是确定的，因此只能用随机变量表达。 我们从以上的区别中可以看出，在贝叶斯模型中，因为每个参数都是一个随机变量，也即是符合某个分布的，如果我们对数据的来源有一定的自信（比如我们的数据是关于电子科技大学的男女比例，我们就会知道这个比例将会大到爆炸，这个我们是很有自信的，因此可以作为先验概率引入的。），那么你将可以通过假设参数分布的形式，引入你对数据的先验知识（prior knowledge），我们称之为对参数的先验假设，表示为$p(\theta)$。我们以后将会发现，如果这个先验知识足够合理，将会使得模型即使是在小规模的数据上训练，都可以获得较为理想的效果，这点是频率学派模型较难做到的。 总结来说，也就是贝叶斯模型在小数据集上具有更好的泛化性能，至于什么叫泛化性能，参考以前文章《经验误差，泛化误差》。 利用贝叶斯理论进行分类在进行进一步讨论之前，我们对我们接下来需要用的的符号进行统一的规定表示和解释： 样本(sample)，$\mathbf{x} \in \mathbb{R}^n$，其中的$n$称之为样本的维度(dimension)。 状态(state)，第一类：$\omega = \omega_1$;第二类：$\omega = \omega_2$，在其他文献中，这个通常也称之为类别(class)，指的是某个样本配对的类别属性。 先验概率(prior)，$p(\omega_1)$，$p(\omega_2)$，指的是对某些类别的预先知道的知识，比如在预测某个病人是否是癌症病人的例子，在没有得到任何关于这个病人的信息之前，因为我们知道得癌症是一个较为低概率的事件，因此其先验概率$p(\omega=癌症)$是一个很小的值。先验概率表现了我们对于某个知识的“信仰”。 样本分布密度(sample distribution density)，$p(\mathbf{x})$。 类条件概率密度(class-conditional probablity density)，$p(\mathbf{x}|\omega_1)$,$p(\mathbf{x}|\omega_2)$，这个概率也经常被称之为似然概率(likelihood probablity)。 以上的术语将会在以后的文章中经常见到，我们届时再做更加深入的讨论。 让我们考虑一个情景： 给你$n$个样本作为已知的训练集，$\mathbf{X}=\{\mathbf{x}_1,\mathbf{x}_2,\cdots,\mathbf{x}_n\}$，其对应的标签为，$\mathbf{Y}=\{\mathbf{y}_1\}$，先给你一个新的样本$\mathbf{x}$，其需要预测其标签。 这个就是基本的分类问题的情景，为了简便，不妨将这里的标签看成是二分类标签$\mathbf{y}_i \in \{+1,-1\}$。我们可以将这个分类问题等价为求$p(\omega_1|\mathbf{x})$和$p(\omega_2|\mathbf{x})$的概率大小，一般来说，如果$p(\omega_1|\mathbf{x}) &gt; p(\omega_2|\mathbf{x})$，那么就可以将其判断为第一类了对吧！反之亦然。 因为有概率论中的贝叶斯公式，我们有： p(\omega_i|\mathbf{x}) = \frac{p(\omega_i,\mathbf{x})}{p(\mathbf{x})}=\frac{p(\omega_i)p(\mathbf{x}|\omega_i)}{\sum_{j=1}^M p(\omega_i)p(\mathbf{x}|\omega_i)} \tag{1.1 贝叶斯公式}因为$p(\mathbf{x})$在$p(\omega_1|\mathbf{x})$和$p(\omega_2|\mathbf{x})$都是一样的，因此在分类问题中，一般可以忽略这个项，我们有： p(\omega_i|\mathbf{x}) \propto p(\omega_i)p(\mathbf{x}|\omega_i) \tag{1.2 贝叶斯公式常用形式}其中，$p(\omega_i)$称之为先验概率；$p(\mathbf{x}|\omega_i)$称之为似然概率，或者称之为类条件概率；$p(\omega_i|\mathbf{x})$称之为后验概率(posterior)。其中，因为我们已经有了先前样本$\mathbf{X}$以及其对应的标签$\mathbf{Y}$，因此可以估计出先验概率和似然概率出来（一般情况下，需要对似然概率进行建模，我们后续再讨论）。 $\nabla$总而言之，我们通过人工的先验概率，和从已有数据中学习到的似然概率中，可以得到后验概率，而后验概率为我们的分类提供了很重要的依据。$\nabla$ 决策论，如何做出一个合理的选择机器学习整个过程可以分为两个阶段，一是推理(inference)阶段，二是决策(decision)阶段。推理阶段主要是从训练样本集中估计出$p(\mathbf{x}, \mathbf{t})$分布，决策阶段是根据这个联合概率分布，如何作出一个合理的决策，对样本进行分类。 决策论(Decision Theory)[1]指导我们如何根据在推理阶段得出的$p(\mathbf{x}, \mathbf{t})$分布进行合理的分类。一般来说，决策策略可分为最小错误分类率策略和最小期望损失策略，我们分别介绍下。 最小错误分类率最小分类错误率(minimizing the misclassification rate)策略的主要目的就是让分类错误率最小化，这个在大多数情况下是适用的。我们先对分类错误率这个概念进行定义，显然，考虑二分类情况，将类别1的物体分类到了2或者相反就是误分类了，用数学表达式表达就是： p(\rm{mistake}) = p(\mathbf{x} \in \mathcal{R}_1,\mathcal{C}_2)+ p(\mathbf{x} \in \mathcal{R}_2,\mathcal{C}_1) \\ = \int_{\mathcal{R}_1} p(\mathbf{x},\mathcal{C}_2) \rm{d}\mathbf{x}+ \int_{\mathcal{R}_2} p(\mathbf{x},\mathcal{C}_1) \rm{d}\mathbf{x}其中的$\mathcal{R}_k$称之为决策区域(decision regions)，如果输入向量在决策区域$k$下，那么该输入向量的所有样本都是被预测为了$k$类。$p(\mathbf{x} \in \mathcal{R}_i, \mathcal{C}_j)$表示将属于类别$j$的样本分类为了类别$i$。对于一个新样本$\mathbf{x}$，为了最小化$p(\rm{mistake})$，我们应该将其类别分到式子(2.1)中的被积函数中较小的一个，因为这样，较大的一项就会因为决策区域不适合而变为0了，因此只会剩下一项较小的。换句话说，就是如果$p(\mathbf{x},\mathcal{C}_1) &gt; p(\mathbf{x},\mathcal{C}_2)$，那么就将其预测为$\mathcal{C}_1$。 我们这里引用[1] page 40 给出的图示进行理解，如下图所示，其中$\hat{x}$表示决策边界，大于$\hat{x}$将会被预测为第二类，小于则会被预测为第一类，于是，我们的决策错误率就是红色区域，绿色区域和蓝色区域的面积了。我们可以清楚的发现，不管$\hat{x}$怎么移动，绿色和蓝色区域的和是一个常数，只有红色区域会在变化，因此直观上看，只有当$\hat{x} = x_0$的时候，也就是$p(x,\mathcal{C_1})=p(x,\mathcal{C_2})$的时候，才会有最小分类错误率。我们有： p(x,\mathcal{C_1})=p(x,\mathcal{C_2}) \\ \Rightarrow p(\mathcal{C}_1|x)p(x) = p(\mathcal{C}_2|x)p(x) \\ \Rightarrow p(\mathcal{C_1|x}) = p(\mathcal{C_2|x}) \tag{2.2 最优决策}也就是说，当$p(\mathcal{C_1|x}) &gt; p(\mathcal{C_2|x})$时，选择$\mathcal{C}_1$作为理论分类错误率最小的选择。我们可以发现，选择具有最大后验概率的类别作为预测结果能够达到最小分类错误率的效果，这个原则我们称之为最大后验概率原则，同时，我们留意，在参数估计中也有一个称之为最大后验概率估计(maximize a posterior probablity, MAP)的原则，请不要混淆。 当类别多于2类时，比如有$K$类时，计算正确率将会更加方便，我们有： p(\rm{correct}) = \sum_{k=1}^K p(\mathbf{x} \in \mathcal{R}_k, \mathcal{C}_k) \\ = \sum_{k=1}^K \int_{\mathcal{R}_k} p(\mathbf{x},\mathcal{C}_k) \rm{d} \mathbf{x} \tag{2.3 多类分类的正确率}同理的，同样是选择具有最大后验概率的类别作为预测结果，能够达到最小分类错误率。 注意到，这个原则有一些等价的表达形式，我们将会在这个系列的附录中进行补充。 最小期望损失按道理来说，最小分类错误已经可以在绝大多数任务中使用了，但是有一些任务，比如医生根据CT影像对病人进行癌症的诊断，在这些任务中，错报和漏报可有着不同的后果。如果只是错报，将没有疾病的人诊断为病人，顶多再去进行一次体检排查，但是如果将有癌症的患者漏报成没有疾病的人，那么就可能错失了最佳的治疗时机，因此这种情况下，这两种错误方式可有着不同的代价。 为了对这个代价进行数学描述，我们引入了一个损失矩阵(loss matrix)用来描述不同错误分类带来的不同代价： cancer normal cancer 0 1000 normal 1 0 这个矩阵很好的描述了我们刚才的需求，让我们用$L$表示，其中$L_{i,j}$表示其第$i$行,$j$列的元素。与最小化分类错误率不同的，我们定义一个代价函数： \mathbb{E}[L] = \sum_{k} \sum_{j} \int_{\mathcal{R}_j} L_{k,j}p(\mathbf{x}, \mathcal{C}_k) \rm{d} \mathbf{x} \tag{3.1 代价函数}我们的目标是最小化(3.1)。当然，如果你需要对一个样本$\mathbf{x}$作出决策，你也许需要将其分解为： R(\alpha_k|\mathbf{x}) = \sum_j L_{k,j}p(\mathcal{C}_k|\mathbf{x}) \tag{3.2 分类风险}这里的$R(\cdot)$表示Risk，表示分类为$k$类的风险，当然是越小越好。 因此总结来说，最小化风险的计算步骤为： 计算后验概率： p(\mathcal{C}_k|x) = \frac{p(x|\mathcal{C}_k) p(\mathcal{C}_k)}{\sum_{i=1}^c p(x|\mathcal{C}_i)p(\mathcal{C}_i)} \\ k = 1,2,\cdots,c 计算风险： R(\mathcal{R}_k|\mathbf{x}) = \sum_j L_{k,j}p(\mathcal{C}_k|\mathbf{x}) 决策： \alpha = \arg \min _{i = 1, \cdots, c} R(\alpha_i | x) 显然，当损失矩阵是一个单位矩阵的时候，最小分类错误率和最小分类风险等价。 Reference[1] Bishop C M. Pattern recognition and machine learning (information science and statistics) springer-verlag new york[J]. Inc. Secaucus, NJ, USA, 2006.[2] 张学工. 模式识别[J]. 2010.]]></content>
      <categories>
        <category>Bayesian Theory</category>
      </categories>
      <tags>
        <tag>贝叶斯理论</tag>
        <tag>统计学习方法</tag>
        <tag>概率论</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow高阶函数之 tf.foldl()和tf.foldr()]]></title>
    <url>%2F2018%2F10%2F21%2Ftensorflow%2Ftf.foldr()%2F</url>
    <content type="text"><![CDATA[前言在TensorFlow中有着若干高阶函数，如之前已经介绍过了的tf.map_fn()，见博文TensorFlow中的高阶函数：tf.map_fn()，此外，还有几个常用的高阶函数，分别是tf.foldl()，tf.foldr()，我们简要介绍下。 tf.foldl()类似于python中的reduce()函数，假设elems是一个大于等于一阶的张量或者列表，形状如[n,...]，那么该函数将会重复地调用fn与这个列表上，从左到右进行处理，这里讲得不清楚，我们看看官方的API手册和一些例子理解一下，地址：https://www.tensorflow.org/api_docs/python/tf/foldl123456789tf.foldl( fn, elems, initializer=None, parallel_iterations=10, back_prop=True, swap_memory=False, name=None) 其中fn是一个可调用函数，也可以用lambda表达式；elems是需要处理的列表；initializer是一个可选参数，可以作为fn的初始累加值（accumulated value）；至于parallel_iterations是并行数，其他的函数可以查询官网，就不累述了。最重要的参数无非是fn,elems和initializer，其中fn是一个具有两个输入参数的函数，需要返回一个值作为累计结果，elems是一个列表或者张量，用于被fn累计处理。形象的来说，就是1tf.foldl(fn,elems=[x1,x2,x3,x4]) = fn(fn(fn(x1,x2),x3),x4) 如果给定了initializer，那么初始的累计参数（也就是fn的第一个参数）就是他了，如果没有给定，也即是initializer=None那么elems中必须至少有一个值，第一个值将会被作为初始值。例子：12345import tensorflow as tfelems = [1, 2, 3, 4, 5, 6]sum = tf.foldl(lambda a, x: a + x, elems)with tf.Session() as sess: print(sess.run(sum)) 将会输出21，也即是(((((1+2)+3)+4)+5)+6)，如果给定了一个初始化值，就变为：12345import tensorflow as tfelems = [1, 2, 3, 4, 5, 6]sum = tf.foldl(lambda a, x: a + x, elems,initializer=10)with tf.Session() as sess: print(sess.run(sum)) 输出变为31。此处的累加是最简单的应用，还可以有更多复杂的应用，就看应用场景了。至于tf.foldr()和这个函数是基本上一样的，无非就是从右边开始计算到左边而已。]]></content>
      <categories>
        <category>TensorFlow Basic API</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tf.group()用于组合多个操作]]></title>
    <url>%2F2018%2F10%2F21%2Ftensorflow%2Ftf_group%2F</url>
    <content type="text"><![CDATA[前言tf.group()用于创造一个操作，可以将传入参数的所有操作进行分组。API手册如:1234tf.group( *inputs, **kwargs) ops = tf.group(tensor1, tensor2,...)其中*inputs是0个或者多个用于组合tensor，一旦ops完成了，那么传入的tensor1,tensor2,...等等都会完成了，经常用于组合一些训练节点，如在Cycle GAN中的多个训练节点，例子如：1234567generator_train_op = tf.train.AdamOptimizer(g_loss, ...)discriminator_train_op = tf.train.AdamOptimizer(d_loss,...)train_ops = tf.groups(generator_train_op ,discriminator_train_op)with tf.Session() as sess: sess.run(train_ops) # 一旦运行了train_ops,那么里面的generator_train_op和discriminator_train_op都将被调用 注意的是，tf.group()返回的是个操作，而不是值，如果你想下面一样用，返回的将不是值123456789a = tf.Variable([5])b = tf.Variable([6])c = a+bd = a*be = a/bops = tf.group(c,d,e)with tf.Session() as sess: sess.run(tf.global_variables_initializer()) ee = sess.run(ops) 返回的将不是c,d,e的运算结果，而是一个None，就是因为这个是一个操作，而不是一个张量。如果需要返回结果，请参考tf.tuple()]]></content>
      <categories>
        <category>TensorFlow Basic API</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tf.tuple()用于组合多个张量输入]]></title>
    <url>%2F2018%2F10%2F21%2Ftensorflow%2Ftf_tuple%2F</url>
    <content type="text"><![CDATA[前言tf.tuple()用于组合多个张量输入组成的列表[tensor1,tensor2,...]，然后返回一个计算过后的张量列表[cal_tensor1,cal_tensor2,...]，这点和tf.group()是不同的，API手册如：12345tf.tuple( tensors, name=None, control_inputs=None) ops = tf.tuple([tensor1,tensor2,...],control_inputs=c_ops)其中tensors是由多个tensor组成的列表，其中的control_inputs是添加额外的控制输入，添加的输入c_ops必须在整个ops完成之前得到执行，但是c_ops的输出是不会返回的。 API上描述，这个可以作为提供一种并行处理的机制，所有的输入的tensor可以并行计算，但是所有tensor的计算出来的值将会以tuple的形式返回，并且这个只能在并行计算完成之后得到。(This can be used as a “join” mechanism for parallel computations: all the argument tensors can be computed in parallel, but the values of any tensor returned by tuple are only available after all the parallel computations are done.) 使用例子：12345678910a = tf.Variable([5])b = tf.Variable([6])c = a+bd = a*be = a/bops = tf.tuple([c,d,e])with tf.Session() as sess: sess.run(tf.global_variables_initializer()) ee = sess.run(ops) print(ee) 输出1[array([11], dtype=int32), array([30], dtype=int32), array([0.83333333])] 可以和tf.group()用于组合多个操作的例子进行对比。]]></content>
      <categories>
        <category>TensorFlow Basic API</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pytorch中的L2和L1正则化，自定义优化器设置等操作]]></title>
    <url>%2F2018%2F10%2F21%2Fpytorch%2Fpytorch_reg%2F</url>
    <content type="text"><![CDATA[前言在pytorch中进行L2正则化，最直接的方式可以直接用优化器自带的weight_decay选项指定权值衰减率，相当于L2正则化中的$\lambda$，也就是： \mathcal{L}_{reg} = ||y-\hat{y}||^2+\lambda||W||^2 \tag{1}中的$\lambda$。但是有一个问题就是，这个指定的权值衰减是会对网络中的所有参数，包括权值$w$和偏置$b$同时进行的，很多时候如果对$b$进行L2正则化将会导致严重的欠拟合1，因此这个时候一般只需要对权值进行正则即可，当然，你可以获取模型中的所有权值，然后按照定义的方法显式地进行处理，得到一个正则损失之后在交给优化器优化，这是一个通用的方法。但是其实还有更为简单的方法，同样在优化器中提供了。 torch.optim中包含了很多现成的优化器，包括SGD，Adadelta，Adam，Adagrad，RMSprop等，使用它很简单，你需要传入一个可迭代的参数列表（里面必须都是Variable类型的）进行优化，然后你可以指定一些优化器的参数，如学习率，动量，权值衰减等。例子如：12optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum=0.9,weight_decay=1e-5)optimizer = optim.Adam([var1, var2], lr = 0.0001) 此外，优化器还支持一种称之为Per-parameter options的操作，就是对每一个参数进行特定的指定，以满足更为细致的要求。做法也很简单，与上面不同的，我们传入的待优化变量不是一个Variable而是一个可迭代的字典，字典中必须有params的key，用于指定待优化变量，而其他的key需要匹配优化器本身的参数设置。我们看一下例子：1234optim.SGD([ &#123;'params': model.base.parameters()&#125;, &#123;'params': model.classifier.parameters(), 'lr': 1e-3&#125; ], lr=1e-2, momentum=0.9) 其中，我们可以看到，传入的list中有两个字典，每一个都是一个独立的参数组，其中每一组中都有一个paramskey，用于指定需要训练的参数，如model.base.parameters()就是base网络中的所有参数，尔后，也可以在每一组内单独设置学习率，权值衰减等。如果不显式地在组内设定，那么就会继承优化器的全局参数，如lr=1e-2,momentum=0.9等，如果组内指定了，那么全局的将不会覆盖掉组内的参数设置。这样我们就可以灵活的给每一个子网络设定不同的学习率，权值衰减，momentum了，我们也可以给权值设定权值衰减，而不作用与偏置，如：123456789101112weight_p, bias_p = [],[]for name, p in model.named_parameters(): if 'bias' in name: bias_p += [p] else: weight_p += [p]# 这里的model中每个参数的名字都是系统自动命名的，只要是权值都是带有weight，偏置都带有bias，# 因此可以通过名字判断属性，这个和tensorflow不同，tensorflow是可以用户自己定义名字的，当然也会系统自己定义。optim.SGC([ &#123;'params': weight_p, 'weight_decay':1e-5&#125;, &#123;'params': bias_p, 'weight_decay':0&#125; ], lr=1e-2, momentum=0.9) Reference[1]. PyTorch Documentation -&gt; torch.optim 1. Goodfellow I, Bengio Y, Courville A, et al. Deep learning[M]. Cambridge: MIT press, 2016. &#8617;]]></content>
      <categories>
        <category>Pytorch Basic API</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在TensorFlow中自定义梯度的两种方法]]></title>
    <url>%2F2018%2F10%2F21%2Ftensorflow%2Ftf_gradient_define%2F</url>
    <content type="text"><![CDATA[前言在深度学习中，有时候我们需要对某些节点的梯度进行一些定制，特别是该节点操作不可导（比如阶梯除法如$10 // 3 = 3$），如果实在需要对这个节点进行操作，而且希望其可以反向传播，那么就需要对其进行自定义反向传播时的梯度。在有些场景，如[2]中介绍到的梯度反转(gradient inverse)中，就必须在某层节点对反向传播的梯度进行反转，也就是需要更改正常的梯度传播过程，如下图的$-\lambda \dfrac{\partial L_d}{\partial \theta_f}$所示。 在tensorflow中有若干可以实现定制梯度的方法，这里介绍两种。 1. 重写梯度法重写梯度法指的是通过tensorflow自带的机制，将某个节点的梯度重写(override)，这种方法的适用性最广。我们这里举个例子[3]. 符号函数的前向传播采用的是阶跃函数$y = \rm{sign}(x)$，如下图所示，我们知道阶跃函数不是连续可导的，因此我们在反向传播时，将其替代为一个可以连续求导的函数$y = \rm{Htanh(x)}$，于是梯度就是大于1和小于-1时为0，在-1和1之间时是1。 使用重写梯度的方法如下，主要是涉及到tf.RegisterGradient()和tf.get_default_graph().gradient_override_map()，前者注册新的梯度，后者重写图中具有名字name=&#39;Sign&#39;的操作节点的梯度，用在新注册的QuantizeGrad替代。 12345678910111213141516171819#使用修饰器，建立梯度反向传播函数。其中op.input包含输入值、输出值，grad包含上层传来的梯度@tf.RegisterGradient("QuantizeGrad")def sign_grad(op, grad): input = op.inputs[0] # 取出当前的输入 cond = (input&gt;=-1)&amp;(input&lt;=1) # 大于1或者小于-1的值的位置 zeros = tf.zeros_like(grad) # 定义出0矩阵用于掩膜 return tf.where(cond, grad, zeros) # 将大于1或者小于-1的上一层的梯度置为0 #使用with上下文管理器覆盖原始的sign梯度函数def binary(input): x = input with tf.get_default_graph().gradient_override_map(&#123;"Sign":'QuantizeGrad'&#125;): #重写梯度 x = tf.sign(x) return x #使用x = binary(x) 其中的def sign_grad(op, grad):是注册新的梯度的套路，其中的op是当前操作的输入值/张量等，而grad指的是从反向而言的上一层的梯度。 通常来说，在tensorflow中自定义梯度，函数tf.identity()是很重要的，其API手册如下：1234tf.identity( input, name=None) 其会返回一个形状和内容都和输入完全一样的输出，但是你可以自定义其反向传播时的梯度，因此在梯度反转等操作中特别有用。这里再举个反向梯度[2]的例子，也就是梯度为$-\lambda \dfrac{\partial L_d}{\partial \theta_f}$而不是$\lambda \dfrac{\partial L_d}{\partial \theta_f}$。1234567891011121314151617import tensorflow as tfx1 = tf.Variable(1)x2 = tf.Variable(3)x3 = tf.Variable(6)@tf.RegisterGradient('CustomGrad')def CustomGrad(op, grad):# tf.Print(grad) return -grad g = tf.get_default_graph()oo = x1+x2with g.gradient_override_map(&#123;"Identity": "CustomGrad"&#125;): output = tf.identity(oo)grad_1 = tf.gradients(output, oo)with tf.Session() as sess: sess.run(tf.global_variables_initializer()) print(sess.run(grad_1)) 因为-grad，所以这里的梯度输出是[-1]而不是[1]。有一个我们需要注意的是，在自定义函数def CustomGrad()中，返回的值得是一个张量，而不能返回一个参数，比如return 0，这样会报错，如：1AttributeError: 'int' object has no attribute 'name' 显然，这是因为tensorflow的内部操作需要取返回值的名字而int类型没有名字。 PS:def CustomGrad()这个函数签名是随便你取的。 2. stop_gradient法对于自定义梯度，还有一种比较简洁的操作，就是利用tf.stop_gradient()函数，我们看下例子[1]：12t = g(x)y = t + tf.stop_gradient(f(x) - t) 这里，我们本来的前向传递函数是f(x)，但是想要在反向时传递的函数是g(x)，因为在前向过程中，tf.stop_gradient()不起作用，因此+t和-t抵消掉了，只剩下f(x)前向传递；而在反向过程中，因为tf.stop_gradient()的作用，使得f(x)-t的梯度变为了0，从而只剩下g(x)在反向传递。我们看下完整的例子：12345678910111213141516171819import tensorflow as tfx1 = tf.Variable(1)x2 = tf.Variable(3)x3 = tf.Variable(6)f = x1+x2*x3t = -fy1 = t + tf.stop_gradient(f-t)y2 = fgrad_1 = tf.gradients(y1, x1)grad_2 = tf.gradients(y2, x1)with tf.Session(config=config) as sess: sess.run(tf.global_variables_initializer()) print(sess.run(grad_1)) print(sess.run(grad_2)) 第一个输出为[-1]，第二个输出为[1]，显然也实现了梯度的反转。 Reference[1]. How Can I Define Only the Gradient for a Tensorflow Subgraph?[2]. Ganin Y, Ustinova E, Ajakan H, et al. Domain-adversarial training of neural networks[J]. Journal of Machine Learning Research, 2017, 17(1):2096-2030.[3]. tensorflow 实现自定义梯度反向传播[4]. Custom Gradients in TensorFlow]]></content>
      <categories>
        <category>TensorFlow Basic API</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SVM沉思录6——支持向量机中的核技巧那些事儿]]></title>
    <url>%2F2018%2F10%2F21%2Fsvm%2Fsvm6%2F</url>
    <content type="text"><![CDATA[前言 我们在前文[1-5]中介绍了线性支持向量机的原理和推导，涉及到了软和硬的线性支持向量机，还有相关的广义拉格朗日乘数法和KKT条件等。然而，光靠着前面介绍的这些内容，只能够对近似于线性可分的数据进行分割，而不能对非线性的数据进行处理，这里我们简单介绍下支持向量机中使用的核技巧，使用了核技巧的支持向量机就具备了分割非线性数据的能力。本篇可能是我们这个系列的最后一篇了，如果有机会我们在SMO中再会吧。 如有谬误，请联系指正。转载请注明出处。 联系方式：e-mail: FesianXu@163.comQQ: 973926198github: https://github.com/FesianXu 重回SVM我们在前文[1-5]中就线性SVM做了比较系统的介绍和推导，我们这里做个简单的小回顾。支持向量机(Support Vector Machine,SVM)，是一种基于最大间隔原则进行推导出来的线性分类器，如果引入松弛项，则可以处理近似线性可分的一些数据，其最终的对偶问题的数学表达形式为(1.1)，之所以用对偶形式求解是因为可以轻松地引入所谓的核技巧，我们后面将会看到这个便利性。 \min_{\alpha} \frac{1}{2}\sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_jy_iy_j(x_i \cdot x_j)- \sum_{i=1}^N\alpha_i \\ s.t. \ \sum_{i=1}^N\alpha_iy_i=0 \\ \alpha_i \geq0,i=1,\cdots,N \tag{1.1}其最终的分类超平面如(1.2): \theta(x) = \rm{sign}(\sum_{i=1}^N \alpha^*_iy_i(x_i \cdot x)+b^*) \tag{1.2}从KKT条件[3]中我们知道，除了支持向量SV会影响到决策面之外，其他所有的样本都是不会对决策面产生影响的，因此只有支持向量对应的$\alpha_i^* &gt; 0$，其他所有的$\alpha_j^*$都是等于0的。也就是说，我们的支持向量机只需要记住某些决定性的样本就可以了。实际上，这种需要“记住样本”的方法，正是一类核方法(kernel method)。这个我们后面可能会独立一些文章进行讨论，这里我们记住，因为SVM只需要记忆很少的一部分样本信息，因此被称之为稀疏核方法(Sparse Kernel Method)[6]。 更进一步观察SVM我们这里更进一步对SVM的对偶优化任务和决策面，也即是式子(1.1)(1.2)进行观察，我们会发现，有一个项是相同作用的，那就是$(x_i \cdot x_j)$和$(x_i \cdot x)$，这两项都是在度量两个样本之间的距离。我们会发现，因为点积操作 x_i \cdot x_j = ||x_i|| \cdot ||x_j|| \cdot \cos(\theta) \tag{2.1}在两个向量模长相同的情况下，可以知道这个点积的结果越大，两个样本之间的相似度越高，因此可以看作是一种样本之间的度量(metric)。这个我们可以理解，SVM作为一种稀疏核方法的之前就是一个核方法，是需要纪录训练样本的原始信息的。 但是，我们注意到，我们是在原始的样本特征空间进行对比这个相似度的，这个很关键，因为在原始的样本特征空间里面，样本不一定是线性可分的，如果在这个空间里面，线性SVM将没法达到很好的效果。 开始我们的非线性之路那么，我们在回顾了之前的一些东西之后，我们便可以开始我们的非线性之路了，抓好扶手吧，我们要起飞了。 高维映射对于非线性的数据，如下图所示，显然我们没法通过一个线性平面对其进行分割。当然，那仅仅是在二维的情况下我们没法对齐进行线性分割，谁说我们不能在更高的维度进行“维度打击”呢？！我们不妨把整个数据上升一个维度，投射到三维空间，我们将红色数据“拉高”，而绿色数据“留在原地”，那么我们就有了：发现没有，在二维线性不可分的数据，在三维空间就变得线性可分了。这个时候我们可以纪录下在三维情况下的决策面，然后在做个逆操作，将其投射到原先的二维空间中，那么我们就有了:看来这种维度打击还真是有效！ $\nabla$我们其实还可以再举个更为简单的例子。$\nabla$假如我们现在有一些数据，满足$x_1^2+x_2^2=1$，是的，我们不难发现这其实就是个以原点为圆心半径为1的圆，其参数为$x_1$和$x_2$，但是显然的，这个是个非线性的关系，如果要转换成一个线性的关系要怎么操作呢？简单，用$x_3 = x_1^2$和$x_4 = x_2^2$，我们有变形等价式$x_3+x_4=1$，于是我们便有了关于$x_3$和$x_4$的线性关系式，其关键就是映射$\phi(x)=x^2$。 别小看这个例子哦，这个是我们核技巧的一个关键的直观想法哦。没晕吧？让我们继续吧。 基函数其实我们刚才举得例子中的$\phi(x) = x^2$就是一个基函数(basic function)，其作用很直接，就是将一个属于特征空间$\mathcal{M}$的样本$\mathbf{x} \in \mathcal{M}$映射到新的特征空间$\mathcal{N}$，使得有$\phi(\mathbf{x}) \in \mathcal{N}$。如果诸位看官熟悉深度学习，那么我们就会发现，其实深度学习中的激活函数无非也就是起着这种作用，将浅层的特征空间映射到深层的特征空间，使得其尽可能地容易区分。可以说，激活函数就是一种基函数。 那么我们能不能把这种映射应用到，我们刚才的第二节提到的度量测试中的原始特征空间中的样本呢？答案自然是可以的，这样，我们就会有： (\phi(\mathbf{x}_i) \cdot \phi(\mathbf{x_j})) \tag{3.1}通常为了后续讨论，我们会将式子(3.1)表示为(3.2): \mathcal{k}(\mathbf{x}_i, \mathbf{x}_j) = (\phi(\mathbf{x}_i) \cdot \phi(\mathbf{x_j})) = \phi(\mathbf{x}_i)^T\phi(\mathbf{x}_j) \tag{3.2}好的，这样我们就将原始特征空间的样本映射到新的特征空间了，这个特征空间一般来说是更高维的线性可分的空间。我们将这里的$\mathcal{k}(\cdot, \cdot)$称之为核函数(kernels)，哦噢，我们的核函数正式出场了哦。 在给定了核函数的情况下，我们的对偶优化问题和决策面变成了： \min_{\alpha} \frac{1}{2}\sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_jy_iy_j \mathcal{k}(x_i \cdot x_j)- \sum_{i=1}^N\alpha_i \\ s.t. \ \sum_{i=1}^N\alpha_iy_i=0 \\ \alpha_i \geq0,i=1,\cdots,N \tag{3.3 对偶问题} \theta(x) = \rm{sign}(\sum_{i=1}^N \alpha^*_iy_i \mathcal{k}(x_i \cdot x)+b^*) \tag{3.4 决策面}但是，实际上我们是人工很难找到这个合适的映射$\phi(\cdot)$的，特别是在数据复杂，而不是像例子那样的时候，那么我们该怎么办呢？我们能不能直接给定一个核函数$\mathcal{k}(\cdot, \cdot)$，然后就不用理会具体的基函数了呢？这样就可以隐式地在特征空间进行特征学习，而不需要显式地指定特征空间和基函数$\phi(\cdot)$[9]。答案是可以的！ 我们给定一个Mercer定理[10]： 如果函数$\mathcal{k}(\cdot, \cdot)$是$\mathbb{R}^n \times \mathbb{R}^n \rightarrow \mathbb{R}$上的映射（也就是从两个n维向量映射到实数域，既是进行样本度量计算）。那么如果$\mathcal{k}(\cdot, \cdot)$是一个有效核函数（也称为Mercer核函数），那么当且仅当对于训练样例$[x^{(1)}, x^{(2)}, \cdots, x^{(m)}]$，其相应的核函数矩阵是对称半正定(positive semidefinite)的，并且有$\mathcal{k}(x,y) = \mathcal{k}(y,x)$。 嗯，定理很长，人生很短，这个定理说人话就是，如果这个核函数$\mathcal{k}(\cdot, \cdot)$是一个对称半正定的，并且其是个对称函数（度量的基本条件），那么这个核函数就肯定对应了某个样本与样本之间的度量，其关系正如(3.2)所示，因此隐式地定义出了样本的映射函数$\phi(\cdot)$，因此是个有效的核函数。 诶，但是对称半正定不是矩阵才能判断吗？这里的核函数是个函数耶？嗯…也不尽然，休息下，我们下一节继续吧。 无限维向量与希尔伯特空间先暂时忘记之前的东西吧，清清脑袋，轻装上阵。我们在以前学习过得向量和矩阵都是有限维度的，那么是否存在无限维的向量和矩阵呢？其实，函数正是可以看成无限维的向量，想法其实很简单，假如有一个数值函数$f:x \rightarrow y$，假设其定义域是整个实数，如果对应每一个输入，都输出一个输出值，我们可以把所有输出值排列起来，也就形成了一个无限维的向量，表达为$\{y\}^{\infty}_i$。 而核函数$\mathcal{k}(\mathbf{x}_i, \mathbf{x}_j)$作为一个双变量函数，就可以看成一个行列都是无限维的矩阵了。这样我们就可以定义其正定性了： \int\int f(\mathbf{x})\mathcal{k}(\mathbf{x}, \mathbf{y})f(\mathbf{y}) \rm{d} \mathbf{x} \rm{d} \mathbf{y} \geq 0 \tag{3.5}既然是个矩阵，那么我们就可以对其进行特征分解对吧，只不过因为是无限维，我们需要使用积分，表达式类似于矩阵的特征值分解： \int \mathcal{k}(\mathbf{x}, \mathbf{y}) \Phi(\mathbf{x}) \rm{d} \mathbf{x} = \lambda \Phi(\mathbf{y}) \tag{3.6}这里的特征就不是特征向量了，而是特征函数（看成无限维向量也可以的）。对于不同的特征值$\lambda_1$和$\lambda_2$，和对应的特征函数$\Phi_1(\mathbf{x})$和$\Phi_2(\mathbf{x})$，有： \begin{aligned} \int \mathcal{k}(\mathbf{x}, \mathbf{y}) \Phi_1(\mathbf{x}) \Phi_2(\mathbf{x}) \rm{d} \mathbf{x} &= \int \mathcal{k}(\mathbf{x}, \mathbf{y}) \Phi_2(\mathbf{x}) \Phi_1(\mathbf{x}) \rm{d} \mathbf{x} \\ \rightarrow \int \lambda_1 \Phi_1(\mathbf{x}) \Phi_2(\mathbf{x}) \rm{d} \mathbf{x} &= \int \lambda_2 \Phi_2(\mathbf{x}) \Phi_1(\mathbf{x}) \rm{d} \mathbf{x} \end{aligned} \tag{3.7}因为特征值不为0，因此由(3.7)我们有: < \Phi_1, \Phi_2 > = \int \Phi_1(\mathbf{x}) \Phi_2(\mathbf{x}) \rm{d} \mathbf{x} = 0 \tag{3.8}也就是任意两个特征函数之间是正交(Orthogonal)的，一个核函数对应着无限个特征值$\{\lambda_i\}_{i=1}^{\infty}$和无限个特征函数$\{\Phi_i\}_{i=1}^{\infty}$，这个正是原先函数空间的一组正交基。 回想到我们以前学习到的矩阵分解，我们知道我们的矩阵$A$可以表示为： A = Q\Lambda Q^T \tag{3.9}其中$Q$是$A$的特征向量组成的正交矩阵，$\Lambda$是对角矩阵。特征值$\Lambda_{i,i}$对应的特征向量是矩阵$Q$的第$i$列。我们看到在有限维空间中可以将矩阵表示为特征向量和特征值的组合表达。同样的，在无限维空间中，也可以定义这种分解，因此可以将核函数$\mathcal{k}(\cdot,\cdot)$表示为: \mathcal{k}(\mathbf{x}, \mathbf{y}) = \sum_{i=0}^{\infty} \lambda_i \Phi_i(\mathbf{x}) \Phi_i(\mathbf{y}) \tag{3.10}重新整理下，将$\{\sqrt{\lambda_i}\Phi_i\}_{i=1}^{\infty}$作为一组正交基，构建出一个空间$\mathcal{H}$。不难发现，这个空间是无限维的，如果再深入探讨，还会发现他是完备的内积空间，因此被称之为希尔伯特空间(Hilbert space)[13]。别被名字给唬住了，其实就是将欧几里德空间的性质延伸到了无限维而已。 回到我们的希尔伯特空间，我们会发现，这个空间中的任意一个函数（向量）都可以由正交基进行线性表出： f = \sum_{i=1}^{\infty} f_i \sqrt{\lambda_i} \Phi_i \tag{3.11}所以$f$可以表示为空间$\mathcal{H}$中的一个无限维向量： f = (f_1, f_2, \cdots,)^T_{\mathcal{H}} \tag{3.12}再生性(Reproduce)前面3.3讨论了很多关于函数在希尔伯特空间上的表出形式，我们这里在仔细观察下核函数。我们发现，其实核函数可以拆分为： \mathcal{k}(\mathbf{x}, \mathbf{y}) = \sum_{i=0}^{\infty}\lambda_i\Phi_i(\mathbf{x})\Phi_i(\mathbf{y}) = < \mathcal{k}(\mathbf{x},\cdot), \mathcal{k}(\mathbf{y}, \cdot) >_{\mathcal{H}} \tag{3.13}其中: \mathcal{k}(\mathbf{x}, \cdot) = (\sqrt{\lambda_1}\Phi_1(\mathbf{x}),\sqrt{\lambda_2}\Phi_2(\mathbf{x}),\cdots)^T_{\mathcal{H}} \\ \mathcal{k}(\mathbf{y}, \cdot) = (\sqrt{\lambda_1}\Phi_1(\mathbf{y}),\sqrt{\lambda_2}\Phi_2(\mathbf{y}),\cdots)^T_{\mathcal{H}} \tag{3.14}发现没有，(3.13)将核函数表示为了两个函数的内积，是不是很想我们的式子(3.2)了呢。我们把这种可以用核函数来再生出两个函数的内积的这种性质称之为再生性(reproduce)，对应的希尔伯特空间称之为再生核希尔伯特空间(Reproducing Kernel Hilbert Space,RKHS)，有点吓人的名词，但是如果你能理解刚才的分解，这个其实还是蛮直接的。 我们更进一步吧，如果定义一个映射$\phi(\cdot)$: \phi(\mathbf{x}) = (\sqrt{\lambda_1}\Phi_1(\mathbf{x}),\sqrt{\lambda_2}\Phi_2(\mathbf{x}),\cdots)^T \tag{3.15}当然这是个无限维的向量。这个映射将样本点$\mathbf{x} \in \mathbb{R}^n$投射到无限维的特征空间$\mathcal{H}$中，我们有： < \phi(\mathbf{x}), \phi(\mathbf{y}) > = \mathcal{k}(\mathbf{x}, \mathbf{y}) = \phi(\mathbf{x})^T\phi(\mathbf{y}) \tag{3.16}因此，我们解决了3.2中提出的问题，我们根本就不需要知道具体的映射函数$\phi$是什么形式的，特征空间在哪里（我们甚至可以投射到无限维特征空间，比如我们接下来要讲到的高斯核函数），只要是一个对称半正定的核函数$K$，那么就必然存在映射$\phi$和特征空间$\mathcal{H}$，使得式子(3.16)成立。 这就是所谓的核技巧(Kernel trick)[12]。 PS: 为了理解为什么是从原始的有限维的特征空间映射到无限维的希尔伯特空间，我们从式子(3.15)其实不难发现，$\mathbf{x} \in \mathbb{R}^n$，$\sqrt{\lambda_i}\Phi_i(\mathbf{x}) \in \mathbb{R}$，而我们的$i \rightarrow \infty$，因此可以看成映射成了无限维的特征。 高斯核函数的无限维映射性质有效的核函数，也就是对称半正定的核函数有很多，而且有一定的性质可以扩展组合这些核函数[6]，这一块内容比较多，我们以后独立一篇文章继续讨论。这里我们主要看下使用最多的核函数，高斯核函数，也经常称之为径向基函数。 高斯核函数的数学表达形式如下所示： \mathcal{k}(\mathbf{x}, \mathbf{y}) = \exp(-||\mathbf{x}-\mathbf{y}||^2/2\sigma^2) \tag{3.17}我们现在对(3.17)进行变形(这里为了方便假设$\mathbf{x},\mathbf{y}$是一维的)： \begin{aligned} \exp(-||x-y||^2/2\sigma^2) &= \exp(-\lambda||x-y||^2) \\ &= \exp(-\lambda x^2+2\lambda xy-\lambda y^2) \\ &= \exp(-\lambda x^2) \exp(-\lambda y^2) \exp(2\lambda xy) \end{aligned} \tag{3.18}利用泰勒展开[14]对式子(3.18)中的$\exp(2\lambda xy)$进行展开，有: \begin{aligned} \exp(2\lambda xy) &= \sum_{i=1}^{\infty} \dfrac{(2\lambda xy)^i}{i!} \\ &= \sum_{i=1}^{\infty} \sqrt{\dfrac{2^i \lambda}{i!}}x \cdot \sqrt{\dfrac{2^i \lambda}{i!}}y \end{aligned} \tag{3.19}现在结合(3.18)和(3.19)，我们有： \begin{aligned} \exp(-||x-y||^2&/2\sigma^2) \\ &= \sum_{i=1}^{\infty} \sqrt{\dfrac{2^i \lambda}{i!}} \exp{(-\lambda x^2)}x \cdot \sqrt{\dfrac{2^i \lambda}{i!}} \exp{(-\lambda y^2)}y \end{aligned} \tag{3.20}用序列$\mathbf{x} = \{\sqrt{\dfrac{2^1 \lambda}{1!}} \exp{(-\lambda x^2)}x, \sqrt{\dfrac{2^2 \lambda}{2!}} \exp{(-\lambda x^2)}x, \cdots\}$, $\mathbf{y}=\{\sqrt{\dfrac{2^1 \lambda}{1!}} \exp{(-\lambda y^2)}y, \sqrt{\dfrac{2^2 \lambda}{2!}} \exp{(-\lambda y^2)}y,\cdots\}$这两个都是无限维向量，也即是一个映射函数$\phi(\cdot)$。于是式子(3.20)可以改写为: \exp(-||x-y||^2/2\sigma^2) = \mathbf{x}^T \mathbf{y} = \phi(\mathbf{x})^T \phi(\mathbf{y}) \tag{3.21}看，我们常用的高斯核函数正是一个无限维映射的核函数。 总结我们前面对再生核希尔伯特空间进行了简单的介绍，同时了解了无限维映射的核函数，高斯核函数，事实上，我们原始的SVM对偶问题推导中的$(x_i \cdot x_j)$也可以看成一种核函数，只不过这是个线性核函数而已，映射到了原始的特征空间，有： \mathcal{k}(\mathbf{x}, \mathbf{y}) = \mathbf{x}^T \mathbf{y} \tag{4.1}在后续的文章中，我们将会介绍更多的核函数，如多项式核函数对数sigmoid核函数等，同时在后续的文章中，我们也将继续探讨关于基函数的一些应用。 Reference[1]. 《SVM笔记系列之一》什么是支持向量机SVM[2]. 《SVM笔记系列之二》SVM的拉格朗日函数表示以及其对偶问题[3]. 《SVM笔记系列之三》拉格朗日乘数法和KKT条件的直观解释[4]. 《SVM笔记系列之四》最优化问题的对偶问题[5]. 《SVM笔记系列之五》软间隔线性支持向量机[6]. Bishop C M. Pattern recognition and machine learning (information science and statistics) springer-verlag new york[J]. Inc. Secaucus, NJ, USA, 2006.[7]. Zhang T. An introduction to support vector machines and other kernel-based learning methods[J]. AI Magazine, 2001, 22(2): 103.[8]. Everything You Wanted to Know about the Kernel Trick[9]. 李航. 统计学习方法[J]. 2012.[10]. 核函数（Kernels） [11]. 机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用[12]. A Story of Basis and Kernel – Part II: Reproducing Kernel Hilbert Space[13]. Hilbert space[14]. 函数的泰勒(Taylor)展开式]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>SVM</tag>
        <tag>Kernel Trick</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SVM沉思录5——软间隔线性支持向量机]]></title>
    <url>%2F2018%2F10%2F21%2Fsvm%2Fsvm5%2F</url>
    <content type="text"><![CDATA[前言在以前的文章中，我们介绍了支持向量机的基本表达式，那是基于硬间隔线性支持向量机的，即是假设数据是完全线性可分的，在数据是近似线性可分的时候，我们不能继续使用硬间隔SVM了，而是需要采用软间隔SVM，在这里我们简单介绍下软间隔线性支持向量机。本人无专业的数学学习背景，只能在直观的角度上解释这个问题，如果有数学专业的朋友，还望不吝赐教。如有误，请联系指正。转载请注明出处。联系方式：e-mail: FesianXu@163.comQQ: 973926198github: https://github.com/FesianXu有关代码开源: click 软间隔最大化在文章《SVM的拉格朗日函数表示以及其对偶问题》和《SVM支持向量机的目的和起源》中，我们推导了SVM的基本公式，那时的基本假设之一就是数据是完全线性可分的，即是总是存在一个超平面$W^TX+b$可以将数据完美的分开，但是正如我们在《SVM的拉格朗日函数表示以及其对偶问题》中最后结尾所说的： 但是，在现实生活中的数据往往是或本身就是非线性可分但是近似线性可分的，或是线性可分但是具有噪声的，以上两种情况都会导致在现实应用中，硬间隔线性支持向量机变得不再实用 因此，我们引入了软间隔线性支持向量机这个概念，硬间隔和软间隔的区别如下图所示： 我们的解决方案很简单，就是在软间隔SVM中，我们的分类超平面既要能够尽可能地将数据类别分对，又要使得支持向量到超平面的间隔尽可能地大。具体来说，因为线性不可分意味着某些样本点不能满足函数间隔大于等于1的条件，即是$\exists i, 1-y_i(W^Tx_i+b) &gt; 0$。解决方案就是通过对每一个样本点$(x_i, y_i)$引入一个松弛变量$\xi_i \geq 0$，对于那些不满足约束条件的样本点，使得函数间隔加上松弛变量之后大于等于1，于是我们的约束条件就变为了： y_i(W^T x_i+b)+\xi_i \geq 1 \\ = y_i(W^T x_i+b) \geq 1-\xi_i \tag{1.1}图像表示如： 超平面两侧对称的虚线为支持向量，支持向量到超平面的间隔为1。在硬间隔SVM中本应该是在虚线内侧没有任何的样本点的，而在软间隔SVM中，因为不是完全的线性可分，所以虚线内侧存在有样本点，通过向每一个在虚线内侧的样本点添加松弛变量$\xi_i$，将这些样本点搬移到支持向量虚线上。而本身就是在虚线外的样本点的松弛变量则可以设为0。于是，给每一个松弛变量赋予一个代价$\xi_i$，我们的目标函数就变成了： f(W, \xi) = \frac{1}{2} \Vert W \Vert ^2+C \sum_{i=1}^N\xi_i \\ i = 1,2, \cdots,N \tag{1.2}其中$C &gt; 0$称为惩罚参数，C值大的时候对误分类的惩罚增大，C值小的时候对误分类的惩罚减小，$(1.2)$有两层含义：使得$\frac{1}{2} \Vert W \Vert^2$尽量小即是间隔尽可能大，同时使得误分类的数量尽量小，C是调和两者的系数，是一个超参数。于是我们的软间隔SVM的问题可以描述为： \min_{W,b,\xi} \frac{1}{2} \Vert W \Vert^2+C \sum_{i=1}^N\xi_i \\ s.t. y_i(W^T x_i+b) \geq 1-\xi_i \\ \xi_i \geq 0 \\ i = 1,2, \cdots, N \tag{1.3}表述为标准形式： \min_{W,b,\xi} \frac{1}{2} \Vert W \Vert ^2+C \sum_{i=1}^N\xi_i \\ s.t. 1-\xi_i-y_i(W^T x_i+b) \leq 0 \\ -\xi_i \leq 0 \\ i = 1,2, \cdots, N \tag{1.4}软间隔SVM的拉格朗日函数表述和对偶问题我们采用《SVM的拉格朗日函数表示以及其对偶问题》中介绍过的相似的方法，将$(1.4)$得到其对偶问题。过程如下：将$(1.4)$转换为其拉格朗日函数形式： L(W, b, \xi, \alpha, \beta) = \frac{1}{2} \Vert W \Vert^2+C \sum_{i=1}^N\xi_i+\sum_{i=1}^N \alpha_i(1-\xi_i-y_i(W^T x_i+b))- \sum_{i=1}^N \beta_i \xi_i \\ = \frac{1}{2} \Vert W \Vert^2+C \sum_{i=1}^N\xi_i+ \sum_{i=1}^N \alpha_i - \sum_{i=1}^N \alpha_i \xi_i - \sum_{i=1}^N \alpha_i y_i(W^T x_i+b) - \sum_{i=1}^N \beta_i \xi_i \\ \tag{2.1}原问题可以表述为（具体移步《最优化问题的对偶问题》）: \min_{W,b,\xi} \max_{\alpha, \beta} L(W, b, \xi, \alpha, \beta) \tag{2.2}得到其对偶问题为： \max_{\alpha, \beta} \min_{W, b, \xi} L(W, b, \xi, \alpha, \beta) \tag{2.3}我们先求对偶问题$\theta_D(\alpha, \beta) = \min_{W, b, \xi} L(W, b, \xi, \alpha, \beta)$，根据KKT条件(具体移步《拉格朗日乘数法和KKT条件的直观解释》)，我们有： \nabla_{W} L(W, b, \xi, \alpha, \beta) = W-\sum_{i=1}^N\alpha_i y_i x_i = 0 \tag{2.4} \nabla_{b} L(W, b, \xi, \alpha, \beta) = \sum_{i=1}^N \alpha_i y_i = 0 \tag{2.5} \nabla_{\xi_i} L(W, b, \xi, \alpha, \beta) = C-\alpha_i-\beta_i = 0 \tag{2.6} \alpha_i \geq 0, \beta_i \geq 0 \tag{2.7}整理得到： W = \sum_{i=1}^N\alpha_i y_i x_i \\ \sum_{i=1}^N \alpha_i y_i = 0 \\ C = \alpha_i+\beta_i \tag{2.8}将$(2.8)$代入$(2.1)$，有： \begin{aligned} L(W, b, \xi, \alpha, \beta) &= \frac{1}{2} \sum_{i=1}^N\alpha_i y_i x_i \sum_{j=1}^N\alpha_j y_j x_j \\ &+(\alpha_i+\beta_i)\sum_{i=1}^N \xi_i+\sum_{i=1}^N \alpha_i - \sum_{i=1}^N \alpha_i \xi_i - \sum_{i=1}^N \beta_i \xi_i - \\ &\sum_{i=1}^N \alpha_iy_i(\sum_{j=1}^N\alpha_j y_j x_j \cdot x_i +b) \\ &= -\frac{1}{2} \sum_{i=1}^N\sum_{j=1}^N \alpha_i \alpha_j y_i y_j (x_i \cdot x_j)+\sum_{i=1}^N \alpha_i \end{aligned} \tag{2.9}所以问题变为： \max_{\alpha, \beta} \theta_D(\alpha, \beta) = \max_{\alpha} -\frac{1}{2} \sum_{i=1}^N\sum_{j=1}^N \alpha_i \alpha_j y_i y_j (x_i \cdot x_j)+\sum_{i=1}^N \alpha_i \tag{2.10}表述为最小化问题： \min_{\alpha} \frac{1}{2} \sum_{i=1}^N\sum_{j=1}^N \alpha_i \alpha_j y_i y_j (x_i \cdot x_j)-\sum_{i=1}^N \alpha_i \\ s.t. \sum_{i=1}^N \alpha_i y_i = 0 \\ \alpha_i \geq 0 \\ \beta_i \geq 0 \\ C = \alpha_i+\beta_i \tag{2.11}通过将$\beta_i = C-\alpha_i$，$(2.11)$可以化为： \min_{\alpha} \frac{1}{2} \sum_{i=1}^N\sum_{j=1}^N \alpha_i \alpha_j y_i y_j (x_i \cdot x_j)-\sum_{i=1}^N \alpha_i \\ s.t. \sum_{i=1}^N \alpha_i y_i = 0 \\ 0 \leq \alpha_i \leq C \tag{2.12}对比文章《SVM的拉格朗日函数表示以及其对偶问题》中的硬间隔SVM的最终的表达式： \min_{\alpha} \frac{1}{2}\sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_jy_iy_j(x_i \cdot x_j)- \sum_{i=1}^N\alpha_i s.t. \ \sum_{i=1}^N\alpha_iy_i=0 \alpha_i \geq0,i=1,\cdots,N \tag{2.13}不难发现软间隔SVM只是在对拉格朗日乘子$\alpha_i$的约束上加上了一个上界$C$。我们以后都会利用$(2.12)$求解，接下来我们在SMO算法中，也将对式子$(2.12)$进行求解。 引用 《SVM的拉格朗日函数表示以及其对偶问题》 CSDN 《SVM支持向量机的目的和起源》 CSDN 《最优化问题的对偶问题》 CSDN 《拉格朗日乘数法和KKT条件的直观解释》 CSDN 《统计学习方法》 豆瓣]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SVM沉思录4——最优化问题的对偶问题]]></title>
    <url>%2F2018%2F10%2F21%2Fsvm%2Fsvm4%2F</url>
    <content type="text"><![CDATA[前言在SVM的推导中，在得到了原问题的拉格朗日函数表达之后，是一个最小最大问题，通常会将其转化为原问题的对偶问题即是最大最小问题进行求解，我们这里简单介绍下最优化问题的对偶问题。本人无专业的数学学习背景，只能在直观的角度上解释这个问题，如果有数学专业的朋友，还望不吝赐教。注意，本文应用多限于SVM，因此会比较狭隘。如有谬误，请联系指正。转载请注明出处。联系方式：e-mail: FesianXu@163.comQQ: 973926198github: https://github.com/FesianXu有关代码开源: click 最优化问题最优化问题研究的是当函数（目标函数）在给定了一系列的约束条件下的最大值或最小值的问题，一般来说，一个最优化问题具有以下形式： \min_{x \in R^n} f(x) \\ s.t. g_i(x) \leq 0 ,i=1,2,\cdots, N\\ h_j(x) = 0, j=1,2,\cdots, M \tag{1.1}最优化问题可以根据目标函数和约束条件的类型进行分类: 如果目标函数和约束条件都为变量的线性函数, 称该最优化问题为线性规划; 如果目标函数为变量的二次函数, 约束条件为变量的仿射函数, 称该最优化问题为二次规划; 如果目标函数或者约束条件为变量的非线性函数, 称该最优化问题为非线性规划. 对偶问题最优化问题存在对偶问题，所谓对偶问题，源于这个思想： 原始问题比较难以求解，通过构建其对偶问题，期望解决这个对偶问题得到其原问题的下界（在弱对偶情况下，对于最小化问题来说），或者得到原问题的解（强对偶情况下）。 在SVM中，因为其属于凸优化问题，因此是强对偶问题，可以通过构建对偶问题解决得到原问题的解。我们举一个线性规划中一个经典问题，描述如下： 某工厂有两种原料A、B，而且能用其生产两种产品： 生产第一种产品需要2个A和4个B，能够获利6； 生产第二种产品需要3个A和2个B，能够获利4；此时共有100个A和120个B，问该工厂最多获利多少？ 可以简单得到其问题的数学表达式为： \max_{x_1, x_2} 6x_1+4x_2 \\ s.t. 2x_1+3x_2 \leq 100 \\ 4x_1+2x_2 \leq 120 \tag{2.1}当然，得到这个式子的根据就是最大化其卖出去的产品的利润。但是，如果只问收益的话，明显地，还可以考虑卖出原材料A和B的手段，前提就是卖出原材料的盈利会比生产商品盈利高，假设产品A和产品B的单价为$w_1$和$w_2$，从这个角度看，只要最小化购买原材料的价格，我们就可以得出另一个数学表达式： \min_{w_1, w_2} {100w_1+120w_2} \\ s.t. 2w_1+4w_2 \geq 6 \\ 3w_1+2w_2 \geq 4 \tag{2.2}其实，我们可以发现这其实是极大极小问题和其对偶问题，极小极大问题。 一些定义原始问题我们要讨论原问题和对偶问题，就需要一些定义，我们给出原始问题的非拉格朗日函数表达形式如式子$(1.1)$所示，引进其广义拉格朗日函数（详见文章《拉格朗日乘数法和KKT条件的直观解释》）： L(x, \alpha, \beta)_{x \in R^n} = f(x)+\sum_{i=1}^N \alpha_i g_i(x)+\sum_{j=1}^M \beta_j h_j(x) \tag{3.1}其中$x=(x^{(1)}, x^{(2)}, \cdots, x^{(n)})^T \in R^n$，而$\alpha_i$和$\beta_j$是拉格朗日乘子，其中由KKT条件有$\alpha_i \geq 0$，考虑关于x的函数： \theta_P(x) = \max_{\alpha, \beta; \alpha_i \geq 0} L(x, \alpha, \beta) \tag{3.2}这里下标$P$用以表示这个是原始问题。联想到我们在文章《SVM的拉格朗日函数表示以及其对偶问题》中一些关于对偶问题的讨论，我们知道其实$(3.2)$中的$\theta_P(x)$其实就表示了$(1.1)$中的原问题的目标函数和其约束条件，这里再探讨一下：假设我们存在一个x，使得x违反原始问题的约束条件，从而有$g_i(x) &gt; 0$或者$h_j(x) \neq 0$，那么我们可以推论出： \theta_P(x) = \max_{\alpha, \beta; \alpha_i \geq 0} [f(x)+\sum_{i=1}^N \alpha_i g_i(x)+ \sum_{j=1}^M \beta_j h_j(x)] = + \infty \tag{3.3}为什么呢？因为若存在某个i使得$g_i(x) &gt; 0$， 那么就可以令$\alpha_i \rightarrow +\infty$使得$\theta_P(x$)取得无穷大这个“最大值”；同样的，若存在一个j使得$h_j(x) \neq 0$， 那么就总是可以使得$\beta_j$让$\beta_j h_j(x) \rightarrow +\infty$， 而其他各个$\alpha_i$和$\beta_j$均取为0（满足约束条件的拉格朗日乘子取为0）。这样，只有对于满足约束条件的i和j，才会有$\theta_P(x)=f(x)$成立。于是我们有这个分段表达式： \theta_P(x) = \begin{cases} f(x) & x满足原始问题约束 \\ + \infty & 其他 \end{cases} \tag{3.4}所以，如果是最小化问题，我们有极小极大问题$(3.5)$: \min_{x} \theta_P(x) = \min_{x} \max_{\alpha, \beta; \alpha_i \geq 0} L(x, \alpha, \beta) \tag{3.5}其与式子$(1.1)$是完全等价的，有着同样的解。这样一来，我们就把原始的最优化问题转换为了广义拉格朗日函数的极小极大问题，为了后续讨论方便，我们记： p^* = \min_{x} \theta_P(x) \tag{3.6}其中$p^*$为问题的解。 极小极大问题的对偶， 极大极小问题我们定义： \theta_{D} (\alpha, \beta) = \min_{x} L(x, \alpha, \beta) \tag{3.7}在考虑极大化$(3.7)$有： \max_{\alpha, \beta; \alpha_i \geq 0} \theta_D(\alpha, \beta)=\max_{\alpha, \beta; \alpha_i \geq 0} \min_{x} L(x, \alpha, \beta) \tag{3.8}式子$(3.8)$称为广义拉格朗日函数的极大极小问题，将其变成约束形式，为： \max_{\alpha, \beta} \theta_D(\alpha, \beta)=\max_{\alpha, \beta} \min_{x} L(x, \alpha, \beta) \\ s.t. \alpha_i \geq 0 \tag{3.9}式子$(3.9)$被称为原问题的对偶问题，定义其最优解为： d^* = \max_{\alpha, \beta} \theta_D(\alpha, \beta) \tag{3.10}实际上，通过这种方法我们可以将式子$(2.1)$转化为式子$(2.2)$，也就是将原问题转化为对偶问题，有兴趣的朋友可以自行尝试。 原始问题和对偶问题的关系正如前面所谈到的，原始问题的解和对偶问题的解存在一定的关系，对于任意的$\alpha, x, \beta$，我们有： \theta_D(\alpha, \beta)=\min_{x} L(x, \alpha, \beta) \leq L(x, \alpha, \beta) \leq \max_{\alpha, \beta; \alpha_i \geq 0} L(x, \alpha, \beta)=\theta_P(x) \tag{4.1}等价于： \theta_D(\alpha, \beta) \leq \theta_P(x) \tag{4.2}注意，式子$(4.2)$对于所有的$x, \alpha, \beta$都成立，因为原始问题和对偶问题均有最优解，所以有： \max_{\alpha, \beta; \alpha_i \geq 0} \theta_D(\alpha, \beta) \leq \min_{x} \theta_P(x) \tag{4.3}容易得到： d^* = \max_{\alpha, \beta; \alpha_i \geq 0} \min_{x} L(x, \alpha, \beta) \leq \min_{x} \max_{\alpha, \beta; \alpha_i \geq 0} L(x, \alpha, \beta) = p^* \tag{4.4}由此我们得到了在最小化问题中$d^* \leq p^*$的结论，这个称为弱对偶。弱对偶指出，解决最小化问题的对偶问题可以得到原问题的解的下界。 既然有弱对偶就会存在强对偶。强对偶指的是$d^*=p^*$的情况，在某些情况下，原始问题和对偶问题的解相同，这时可以用解决对偶问题来代替原始问题，下面以定理的方式给出强对偶成立的重要条件而不予以证明： 考虑原始问题$(1.1)$和对偶问题$(3.9)$，假设$f(x)$和$g_i(x)$都是凸函数， $h_j(x)$是仿射函数，并且不等式约束$g_i(x)$是严格可行的，既存在$x$，对所有$i$有$g_i(x) &lt; 0 $，则存在$x^*,\alpha^*,\beta^*$，使得$x^*$是原始问题的解，$\alpha^*,\beta^*$是对偶问题的解（满足这个条件的充分必要条件就是$x^*, \alpha^*, \beta^*$满足KKT条件1），并且：$p^* = d^* = L(x^*, \alpha^*, \beta^*)$ 引用 最优化问题学习笔记1-对偶理论 CSDN 《统计学习方法》 豆瓣 如何理解对偶问题？ feng liu的回答 《拉格朗日乘数法和KKT条件的直观解释》 CSDN SVM的拉格朗日函数表示以及其对偶问题 CSDN 1. 见《拉格朗日乘数法和KKT条件的直观解释》 &#8617;]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SVM沉思录3——拉格朗日乘数法和KKT条件的直观解释]]></title>
    <url>%2F2018%2F10%2F21%2Fsvm%2Fsvm3%2F</url>
    <content type="text"><![CDATA[前言在SVM的推导中，出现了核心的一个最优化问题，这里我们简单介绍下最优化问题，特别是带有约束的最优化问题，并且引入拉格朗日乘数法和广义拉格朗日乘数法，介绍并且直观解释了KKT条件，用于解决带约束的最优化问题。本人无专业的数学学习背景，只能在直观的角度上解释这个问题，如果有数学专业的朋友，还望不吝赐教。如有误，请联系指正。转载请注明出处。联系方式：e-mail: FesianXu@163.comQQ: 973926198github: https://github.com/FesianXu有关代码开源: click 最优化问题我们在高中，包括在高数中都会经常遇到求解一个函数的最小值，最大值之类的问题，这类问题就是属于最优化问题。比如给出下列一个不带有约束的最优化问题： \min_{x} 3x^2+4x+5, x \in R \tag{1.1}其中的$3x^2+4x+5$我们称为目标函数(objective function)。这样的问题，直接利用罗尔定理（Rolle’s theorem）求出其鞍点，又因为其为凸函数而且可行域是整个$R$，求出的鞍点便是最值点，这个是对于无约束最优化问题的解题套路。如果问题带有约束条件，那么就变得不一样了，如： \min_{x, y} 3xy^2 \\ s.t. 4x+5y = 10 \tag{1.2}因为此时的约束条件是仿射函数（affine function）1，所以可以利用换元法将$x$表示为$y$的函数，从而将目标函数变为无约束的形式，然后利用罗尔定理便可以求出最值点了。然而如果约束条件一般化为$g(x, y) = c$，那么$x$就不一定可以用其他变量表示出来了，这个时候就要利用拉格朗日乘数法(Lagrange multipliers )了。 拉格朗日乘数法(Lagrange multipliers)我们先一般化一个二元最优化问题为$(2.1)$形式： \min_{x, y} f(x, y) \\ s.t. g(x, y) = c \tag{2.1}将目标函数$f(x, y)$和等式约束条件$g(x, y)=c$画出来就如下图所示： 其中的$f(x, y)$虚线为等高线，而红线为$g(x, y)=c$这个约束函数曲线与$f(x,y)$的交点的连线在$x-y平面$的映射。其中，假设有$d_3 &gt; d_2 &gt; d_1$， $d_1$点为最小值点（最优值点）。从直观上可以发现，在$g(x,y)=c$与$f(x,y)$的非最优化交点$A$,$B$,$C$,$D$上，其$f(x,y)$和$g(x,y)$的法线方向并不是共线的，注意，这个相当关键，因为如果不是共线的，说明$g(x,y)=c$与$f(x,y)$的交点中，还存在可以取得更小值的点存在。对于A点来说，B点就是更为小的存在。因此，我们从直觉上推论出只有当$g(x,y)=c$与$f(x,y)$的法线共线时，才是最小值点的候选点（鞍点）。推论到多元变量的问题的时候，法线便用梯度表示。于是，我们有原问题取得最优值的必要条件： \nabla f(x,y) = \nabla \lambda (g(x, y)-c) \tag{2.2}$(2.2)$其中的$\lambda$表示两个梯度共线。可以简单的变形为 \nabla L(x, y, \lambda) = \nabla f(x,y) - \nabla \lambda (g(x, y)-c) = 0 \tag{2.3}让我们去掉梯度算子，得出 L(x, y, \lambda) = f(x, y) - \lambda(g(x, y) - c) \tag{2.4}这个时候$\lambda$取个负号也是不影响的，所以式子$(2.4)$通常写作： L(x, y, \lambda) = f(x, y) + \lambda(g(x, y) - c) \tag{2.5}看！我们得出了我们高数中经常见到的等式约束下的拉格朗日乘数函数的表示方法。 多约束的拉格朗日乘数法以上，我们讨论的都是单约束的拉格朗日乘数法，当存在多个等式约束时（其实不等式约束也是一样的），我们进行一些推广。先一般化一个二元多约束最小化问题： \min_{x, y} f(x, y) \\ s.t. g_i(x, y) = 0, i = 1,2, \cdots,N \tag{2.6}对于每个目标函数和约束配对，我们有: L_1(x ,y ,\lambda_1) = f(x,y)+\lambda_1 g_1(x,y) \\ \vdots \\ L_N(x, y, \lambda_N) = f(x,y)+\lambda_N g_N(x,y)将上式相加有： \sum_{i=1}^N L_i(x,y,\lambda_i)=N f(x, y)+\sum_{i=1}^N \lambda_ig_i(x,y) \tag{2.7}定义多约束的拉格朗日函数为： L(x,y,\lambda) = f(x,y)+\frac{1}{N} \sum_{i=1}^N \lambda_ig_i(x,y) \tag{2.8}因为$\lambda_i$是常数，表示共线的含义而已，所以乘上一个常数$\frac{1}{N}$也不会有任何影响，我们仍然用$\lambda_i$表示，因此式子$(2.8)$变成： L(x,y,\lambda) = f(x,y)+\sum_{i=1}^N \lambda_ig_i(x,y) \tag{2.9}这就是多约束拉格朗日乘数法的函数表达形式。 一个计算例子让我们举一个单约束的拉格朗日乘数法的计算例子，例子来源于引用3。给出一个最大化任务： \max_{x,y} xy^2 \\ s.t. g(x,y):x^2+y^2-3=0 \tag{2.10}图像如： 只有一个约束，使用一个乘子$\lambda$，有拉格朗日函数： L(x,y,\lambda)=xy^2+\lambda(x^2+y^2-3)按照条件求解候选点： \nabla_{x,y,\lambda} L(x,y,\lambda) = (\frac{\partial L}{\partial x}, \frac{\partial L}{\partial y}, \frac{\partial L}{\partial \lambda})=(2xy+2\lambda x, x^2+2 \lambda y, x^2+y^2-3)=0有 x(y+\lambda)=0 \tag{i} x^2+2 \lambda y = 0 \tag{ii} x^2+y^2=3 \tag{iii}根据式子$(i)(ii)(iii)$， 解得有： (\pm \sqrt{2}, 1, -1); (\pm \sqrt{2}, -1, 1); (0, \pm \sqrt{3}, 0)代入$f(x,y)$，得到：2， -2， 0，也就是我们需要求得的最大值，最小值。可以从图中看出，我们观察到其等高线与约束投影线的确是相切的。 广义拉格朗日乘数法(Generalized Lagrange multipliers)上面我们的拉格朗日乘数法解决了等式约束的最优化问题，但是在存在不等式约束的最优化问题（包括我们SVM中需要求解的最优化问题）上，普通的拉格朗日乘数法并不能解决，因此学者提出了广义拉格朗日乘数法（Generalized Lagrange multipliers）， 用于解决含有不等式约束的最优化问题。这一章，我们谈一谈广义拉格朗日乘数法。 首先，我们先一般化我们的问题，规定一个二元标准的带有不等式约束的最小化问题(当然可以推广到多元的问题)，如： \min_{x, y} f(x, y) \\ s.t. g_i(x, y) \leq 0, i = 1,2,\cdots,N \\ h_i(x, y) = 0, i = 1,2, \cdots, M \tag{3.1}类似于拉格朗日乘数法，参照式子$(2.9)$，我们使用$\alpha_i$和$\beta_i$作为等式约束和不等式约束的拉格朗日乘子，得出下式： L(x, y, \alpha, \beta) = f(x, y)+\sum_{i=1}^N \alpha_ig_i(x ,y)+\sum_{i=1}^M \beta_i h_i(x, y) \tag{3.2}KKT条件（Karush–Kuhn–Tucker conditions）指出，当满足以下几个条件的时候，其解是问题最优解的候选解(摘自wikipedia)。 Stationarity 对于最小化问题就是：$\nabla f(x,y)+\sum_{i=1}^N \alpha_i \nabla g_i(x ,y)+\sum_{i=1}^M \beta_i \nabla h_i(x, y) = 0 \tag{3.3}$ 对于最大化问题就是：$\nabla f(x,y)-(\sum_{i=1}^N \alpha_i \nabla g_i(x ,y)+\sum_{i=1}^M \beta_i \nabla h_i(x, y)) = 0 \tag{3.4}$ Primal feasibility $g_i(x,y) \leq0, i = 1,2,\cdots,N \tag{3.5}$ $h_i(x, y) = 0, i = 1,2,\cdots,M \tag{3.6}$ Dual feasibility $\alpha_i \geq 0, i = 1,2, \cdots, N \tag{3.7}$ Complementary slackness $\alpha_i g_i(x, y) = 0, i = 1,2,\cdots,N \tag{3.8}$ 其中的第一个条件和我们的拉格朗日乘数法的含义是相同的，就是梯度共线的意思；而第二个条件就是主要约束条件，自然是需要满足的；有趣的和值得注意的是第三个和第四个条件，接下来我们探讨下这两个条件，以及为什么不等式约束会多出这两个条件。 为了接下来的讨论方便，我们将N设为3，并且去掉等式约束，这样我们的最小化问题的广义拉格朗日函数就变成了： L(x, y, \alpha, \beta) = f(x, y)+\sum_{i=1}^3 \alpha_ig_i(x ,y) \tag{3.9}绘制出来的示意图如下所示： 其中$d_i &gt; d_j, when i &gt; j$，而蓝线为最优化寻路过程。 让我们仔细观察式子$(3.7)$和$(3.8)$，我们不难发现，因为$\alpha_i \geq 0$而$g_i(x, y) \leq 0$，并且需要满足$\alpha_i g_i(x, y) = 0$，所以$\alpha_i$和$g_i(x,y)$之中必有一个为0，那为什么会这样呢？ 我们从上面的示意图入手理解并且记好公式$(3.3)$。让我们假设初始化一个点A， 这个点A明显不处于最优点，也不在可行域内，可知$g_2(x,y)&gt;0$违背了$(3.5)$，为了满足约束$(3.8)$，有$\alpha_2=0$，导致$\alpha_2 \nabla g_2(x,y)=0$，而对于$i=1,3$，因为满足约束条件而且$g_1(x,y) \neq 0, g_3(x,y) \neq 0$，所以$\alpha_1 = 0, \alpha_3 = 0$。这样我们的式子$(3.3)$就只剩下$\nabla f(x,y)$，因此对着$\nabla f(x,y)$进行优化，也就是沿着$f(x,y)$梯度方向下降即可，不需考虑其他的条件（因为还完全处于可行域之外）。因此，A点一直走啊走，从A到B，从B到C，从C到D，这个时候因为D点满足$g_2(x,y)=0$，因此$\alpha_2 &gt; 0$，所以$\alpha_2\nabla g_2(x,y) \neq 0$，因此$(3.3)$就变成了$\nabla f(x,y)+\alpha_2\nabla g_2(x,y)$所以在优化下一个点E的时候，就会考虑到需要满足约束$g_2(x,y) \leq 0$的条件，朝着向$g_2(x,y)$减小，而且$f(x,y)$减小的方向优化。因此下一个优化点就变成了E点，而不是G点。因此没有约束的情况下其优化路径可能是$A \rightarrow B \rightarrow C \rightarrow D \rightarrow G \rightarrow H$，而添加了约束之后，其路径变成了$A \rightarrow B \rightarrow C \rightarrow D \rightarrow E \rightarrow F$。 这就是为什么KKT条件引入了条件3和条件4，就是为了在满足不等式约束的情况下对目标函数进行优化。让我们记住这个条件，因为这个条件中某些$\alpha_i=0$的特殊性质，将会在SVM中广泛使用，而且正是这个性质定义了支持向量(SV)。 引用 拉格朗日乘子法如何理解？ 知乎 《统计学习方法》 豆瓣 《【直观详解】拉格朗日乘法和KKT条件》 微信公众号 《解密SVM系列（一）：关于拉格朗日乘子法和KKT条件》 CSDN Karush–Kuhn–Tucker conditions wikipedia 1. 最高次数为1的多项式，形如 $f(x) = AX+B$，其中$X$是$m \times k$的仿射矩阵，其与线性函数的区别就是，线性函数是$f(x) = AX$没有偏置项$B$。 &#8617;]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SVM沉思录2——SVM的拉格朗日函数表示以及其对偶问题]]></title>
    <url>%2F2018%2F10%2F21%2Fsvm%2Fsvm2%2F</url>
    <content type="text"><![CDATA[前言支持向量机的对偶问题比原问题容易解决，在符合KKT条件的情况下，其对偶问题和原问题的解相同，这里我们结合李航博士的《统计学习方法》一书和林轩田老师的《机器学习技法》中的内容，介绍下SVM的对偶问题。本人无专业的数学学习背景，只能直观上理解一些问题，请数学专业的朋友不吝赐教。如有谬误，请联系指正。转载请注明出处。联系方式：e-mail: FesianXu@163.comQQ: 973926198github: https://github.com/FesianXu有关代码开源: click SVM的原问题的拉格朗日乘数表示 我们在上一篇博文《SVM笔记系列1，SVM起源与目的》中，谈到了SVM的原问题，这里摘抄如下： \min_{W,b} \frac{1}{2} \Vert W \Vert^2 \\ s.t. 1-y_i(W^Tx_i+b) \leq 0, \ i=1,\cdots,N \tag{1.1}其满足形式: \min_{W,b} f(x) \\ s.t. c_i(x) \leq0, i=1,\cdots,k \\ h_j(x) = 0, j=1,\cdots,l \tag{1.2}假设原问题为$\theta_P(x)$，并且其最优解为$p^*=\theta_P(x^*)$。这是一个有约束的最优化问题，我们利用广义拉格朗日乘子法(具体移步《拉格朗日乘数法和KKT条件的直观解释》)，将其转换为无约束的形式： L(W,b,\alpha) = \frac{1}{2} \Vert W \Vert ^2 + \sum_{i=1}^N \alpha_i (1-y_i(W^Tx_i+b)), \ \alpha_i \geq 0 \tag{1.3}变形为： L(W,b,\alpha) = \frac{1}{2}||W||^2 + \sum_{i=1}^N {\alpha_i}-\sum_{i=1}^N{\alpha_iy_i(W^Tx_i+b)} , \ \alpha_i \geq 0 \tag{1.4}我们将会得到原问题的另一个表述为： \begin{aligned} f(x) &= \max_{\alpha} L(W, b, \alpha) \\ &=\max_{\alpha} \frac{1}{2} \Vert W \Vert ^2 + \sum_{i=1}^N {\alpha_i}-\sum_{i=1}^N{\alpha_iy_i(W^Tx_i+b)}, \\ &\alpha_i \geq 0 \end{aligned} \tag{1.5} \begin{aligned} \theta_P(x) &= \min_{W,b}f(x) = \min_{W,b} \max_{\alpha} L(W, b, \alpha) \\ &= \min_{W,b} \max_{\alpha} \frac{1}{2}||W||^2 + \sum_{i=1}^N {\alpha_i}-\sum_{i=1}^N{\alpha_iy_i(W^Tx_i+b)},, \ \alpha_i \geq 0 \end{aligned} \tag{1.6}这里我觉得有必要解释下为什么$f(x)$可以表述为$\max_{\alpha} L(W, b, \alpha)$这种形式。假设我们有一个样本点$x_i$是不满足原问题的约束条件$1-y_i(W^Tx_i+b) \leq 0$的，也就是说$1-y_i(W^Tx_i+b) \gt 0$，那么在$\max_{\alpha}$这个环节就会使得$\alpha_i \rightarrow +\infty$从而使得$L(W,b,\alpha) \rightarrow +\infty$。如果$x_i$是满足约束条件的，那么为了求得最大值，因为$1-y_i(W^Tx_i+b) \leq 0$而且$\alpha_i \geq 0$，所以就会使得$\alpha_i = 0$。由此我们得知： \max_{\alpha}L(W,b,\alpha) = \begin{cases} \frac{1}{2} \Vert W \Vert^2 & 1-y_i(W^Tx_i+b) \leq 0 满足约束条件\\ +\infty & 1-y_i(W^Tx_i+b) \gt 0 不满足约束条件 \end{cases} \tag{1.7}因此在满足约束的情况下， \max_{\alpha}L(W,b,\alpha)=\frac{1}{2} \Vert W \Vert ^2不满足约束条件的样本点则因为无法对正无穷求最小值而自然抛弃。这个时候，我们试图去解$\max_{\alpha}L(W,b,\alpha)$中的$\max_{\alpha}$我们会发现因为$L(W,b,\alpha)=\frac{1}{2}||W||^2 + \sum_{i=1}^N {\alpha_i}-\sum_{i=1}^N{\alpha_iy_i(W^Tx_i+b)}$对于$\alpha$是线性的，非凸的1，因此无法通过梯度的方法求得其最大值点，其最大值点应该处于可行域边界上，因此我们需要得到SVM的对偶问题进行求解。至此，我们得到了原问题的最小最大表述： \begin{aligned} \theta_P(x) &= \min_{W,b} \max_{\alpha} L(W, b, \alpha) \\ &=\min_{W,b} \max_{\alpha} \frac{1}{2} \Vert W \Vert^2 + \sum_{i=1}^N {\alpha_i}-\sum_{i=1}^N{\alpha_iy_i(W^Tx_i+b)} \\ &\alpha_i \geq0,i=1,\cdots,N \end{aligned} \tag{1.8} SVM的对偶问题从上面的讨论中，我们得知了SVM的原问题的最小最大表达形式为： \begin{aligned} \theta_P(x) &= \min_{W,b} \max_{\alpha} L(W, b, \alpha) \\ &=\min_{W,b} \max_{\alpha} \frac{1}{2}||W||^2 + \sum_{i=1}^N {\alpha_i}-\sum_{i=1}^N{\alpha_iy_i(W^Tx_i+b)} \\ &\alpha_i \geq0,i=1,\cdots,N \end{aligned} \tag{2.1}设SVM的对偶问题为$\theta_D(\alpha)$，其最优解为$d^*=\theta_D(\alpha^*)$，可知道其为： \begin{aligned} g(x) &= \min_{W,b} L(W,b,\alpha) \\ &=\min_{W,b} \frac{1}{2} \Vert W \Vert^2 + \sum_{i=1}^N {\alpha_i}-\sum_{i=1}^N{\alpha_iy_i(W^Tx_i+b)} \end{aligned} \tag{2.2} \begin{aligned} \theta_D(\alpha) &= \max_{\alpha}g(x) = \max_{\alpha} \min_{W,b} L(W,b,\alpha)\\ &=\max_{\alpha} \min_{W,b} \frac{1}{2} \Vert W \Vert^2 + \sum_{i=1}^N {\alpha_i}-\sum_{i=1}^N{\alpha_iy_i(W^Tx_i+b)} \end{aligned} \tag{2.3}此时，我们得到了对偶问题的最大最小表述，同样的，我们试图去求解$\theta_D(\alpha)$中的$\min_{W,b}$，我们会发现由于$L(W,b,\alpha)=\frac{1}{2} \Vert W \Vert^2 + \sum_{i=1}^N {\alpha_i}-\sum_{i=1}^N{\alpha_iy_i(W^Tx_i+b)}$对于$W$来说是凸函数，因此可以通过梯度的方法求得其最小值点（即是其极小值点）。 求解$\min_{W,b} L(W,b,\alpha)$，因为$L(W,b,\alpha)$是凸函数，我们对采用求梯度的方法求解其最小值（也是KKT条件中的，$\nabla_WL(W,b,\alpha)=0$和$\nabla_b L(W,b,\alpha)=0$）： \frac{\partial{L}}{\partial{W}}=W-\sum_{i=1}^N\alpha_iy_ix_i=0, i=1,\cdots,N \tag{2.4} \frac{\partial{L}}{\partial{b}}=\sum_{i=1}^N\alpha_iy_i=0,i=1,\cdots,N \tag{2.5}得出： W=\sum_{i=1}^N\alpha_iy_ix_i, \sum_{i=1}^N\alpha_iy_i=0, \alpha_i \geq0,i=1,\cdots,N \tag{2.6}将其代入$g(x)$，注意到$\sum_{i=1}^N\alpha_iy_i=0$,得： \begin{aligned} g(x) &= \frac{1}{2} \sum_{i=1}^N \alpha_iy_ix_i \sum_{j=1}^N a_jy_jx_j+\sum_{i=1}^N\alpha_i -\sum_{i=1}^N\alpha_iy_i(\sum_{j=1}^N \alpha_jy_jx_j \cdot x_i+b) \\ &= -\frac{1}{2}\sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_jy_iy_j(x_i \cdot x_j)+ \sum_{i=1}^N\alpha_i \end{aligned}整理为: \max_{\alpha}g(x) = \max_{\alpha} -\frac{1}{2}\sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_jy_iy_j(x_i \cdot x_j)+ \sum_{i=1}^N\alpha_i \\ s.t. \ \sum_{i=1}^N\alpha_iy_i=0 \\ \alpha_i \geq0,i=1,\cdots,N \tag{2.7}等价为求最小问题: \min_{\alpha}g(x) = \min_{\alpha} \frac{1}{2}\sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_jy_iy_j(x_i \cdot x_j)- \sum_{i=1}^N\alpha_i \\ s.t. \ \sum_{i=1}^N\alpha_iy_i=0 \\ \alpha_i \geq0,i=1,\cdots,N \tag{2.8}根据Karush–Kuhn–Tucker(KKT)条件2,我们有： \nabla_WL(W^*,b^*,\alpha^*)=W^*-\sum_{i=1}^N\alpha_i^*y_ix_i=0 \Longrightarrow W^* = \sum_{i=1}^N\alpha_i^*y_ix_i \tag{2.9} \nabla_bL(W^*,b^*,\alpha^*) = -\sum_{i=1}^N \alpha^*_i y_i=0 \tag{2.10} \alpha^*_i(1-y_i(W^*x_i+b^*))=0 \tag{2.11} 1-y_i(W^*x_i+b^*) \leq 0 \tag{2.12} \alpha^*_i \geq 0 \tag{2.13}前两个式子我们已经在求极值的时候利用了，得知: W^* = \sum_{i=1}^N\alpha_i^*y_ix_i \tag{2.14}并且其中至少有一个$\alpha_j^* \gt 0$，对此$j$有，$y_j(W^*x_j+b^*)-1=0$代入刚才的$W^*$，我们有 b^*=y_j-\sum_{i=1}^N\alpha^*_iy_i(x_i \cdot x_j) \tag{2.15}所以决策超平面为： \sum_{i=1}^N \alpha^*_iy_i(x_i \cdot x)+b^*=0 \tag{2.16}分类超平面为： \theta(x)=sign(\sum_{i=1}^N \alpha^*_iy_i(x_i \cdot x)+b^*) \tag{2.17}其中，我们可以观察到超平面只是依赖于$\alpha_i^*&gt;0$的样本点$x_i$，而其他样本点对其没有影响，所以这些样本是对决策超平面起着决定性作用的，因此我们将$\alpha_i^*&gt;0$对应的样本点集合$x_i$称为支持向量。同时，我们可以这样理解当$\alpha^*_i &gt;0$时，我们有$1-y_i(W^*x_i+b)=0$，这个恰恰是表明了支持向量的函数间隔都是1，恰好和我们之前的设定一致。 至此，我们得到了硬间隔线性支持向量机的数学表述形式，所谓硬间隔线性支持向量机，就是满足我之前的假设 两类样本是线性可分的，总是存在一个超平面$W^Tx+b$可以将其完美分割开。 但是，在现实生活中的数据往往是或本身就是非线性可分但是近似线性可分的，或是线性可分但是具有噪声的，以上两种情况都会导致在现实应用中，硬间隔线性支持向量机变得不再实用，因此我们将会在后续讨论用以解决近似线性可分的软间隔线性支持向量机和基于kernel的支持向量机，后者可以解决非线性可分的问题。下图表示了硬间隔线性支持向量机和软间隔支持向量机之间的区别。 在下一篇中，我们紧接着现在的内容，介绍序列最小最优化算法（Sequential Minimal Optimization,SMO），用于求解$\theta_D(x)$，得到$\alpha^*_i$以便于得到超平面的$W^*$和$b$。我们将在其他文章中介绍软间隔线性支持向量机，广义拉格朗日乘数法，KKT条件和基于kernel的支持向量机。 这里我们要记住我们需要最优化的目的式子，我们以后将会反复提到这个式子。 \min_{\alpha} \frac{1}{2}\sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_jy_iy_j(x_i \cdot x_j)- \sum_{i=1}^N\alpha_i s.t. \ \sum_{i=1}^N\alpha_iy_i=0 \alpha_i \geq0,i=1,\cdots,N 1. 易证明。参考wikipedia的凸函数定义。 &#8617; 2. 事实上，如果$\theta_D(x)$的$L(W,b,\alpha)$满足KKT条件，那么在SVM这个问题中，$W^*$和$b^*$和$\alpha^*_i$同时是原问题和对偶问题的解的充分必要条件是满足KKT条件，具体见《统计学习方法》附录和《拉格朗日乘数法和KKT条件的直观解释》。 &#8617;]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SVM沉思录1——什么是支持向量机SVM]]></title>
    <url>%2F2018%2F10%2F21%2Fsvm%2Fsvm1%2F</url>
    <content type="text"><![CDATA[前言支持向量机是常用的，泛化性能佳的，而且可以应用核技巧的机器学习算法，在深度学习流行前是最被广泛使用的机器学习算法之一，就算是深度学习流行的现在，支持向量机也由于其高性能，较低的计算复杂度而被人们广泛应用。这里结合李航博士的《统计学习方法》一书的推导和林轩田老师在《机器学习技法》中的讲解，谈谈自己的认识。如有谬误，请联系指正。转载请注明出处。联系方式：e-mail: FesianXu@163.comQQ: 973926198github: https://github.com/FesianXu有关代码开源: click SVM的起源 支持向量机(Support Vector Machine, SVM)是一种被广泛使用的机器学习算法，自从被Vapnik等人提出来之后便被广泛使用和发展。传统的支持向量机一般是二类分类器，其基本出发点很简单，就是找到一个策略，能够让线性分类器的分类超平面能够最大程度的把两类的样本最好地分割开，这里我们讨论下什么叫做最好地分割开，和实现这个对整个分类器的意义。 最好地分割数据 在进行接下来的讨论之前，为了简化我们的讨论从而直面问题所在，我们进行以下假设： 1） 我们现在的两类数据是线性可分的， 也就是总是存在一个超平面$W^TX+b$可以将数据完美的分开。2） 我们的数据维度是二维的，也就是特征维只有两个，类标签用+1， -1表示，这样方便我们绘制图像。 我在前篇博文《机器学习系列之 感知器模型》中已经介绍到了感知器这一简单的线性分类器。感知器很原始，只能对线性可分的数据进行准确分割，而且由于其激活函数选用的是阶跃函数，因此不能通过梯度的方法进行参数更新，而是只能采用错误驱动的策略进行参数更新，这样我们的超平面其实是不确定的，因为其取决于具体随机到的是哪个样本点进行更新，这是一个不稳定的结果。而且，由于采用了这种参数更新策略，感知器的超平面即使是能够将线性数据完美地分割开，也经常会出现超平面非常接近某一个类的样本，而偏离另一个类的样本的这种情况，特别是在真实情况下的数据是叠加了噪声的情况下。 如下图所示，其中绿线是感知器的运行结果，因为其算法的不稳定性，所以每次的结果都可能不同，选中的这一次我们可以看出来虽然绿线将两类数据完美地分割开了，但是和蓝色样本很接近，如果新来的测试样本叠加一个噪声，这个超平面就很容易将它分类错误，而最佳分类面粉色线则对噪声有着更好地容忍。 样本噪声 刚才我们谈到了样本集上叠加的噪声，噪声广泛存在于真实数据集中，无法避免，因此我们的分类超平面要能够对噪声进行一定的容忍。一般我们假设噪声为高斯噪声，如下图所示： 其中红点为实际的采样到的样本位置$P_{sample}$，而蓝点是可能的样本的实际位置$P_{actual}$，因为噪声$N$的叠加才使得其偏离到了红点位置，其中蓝点的位置满足高斯分布。 P_{sample} = P_{actual}+N, N \sim N(\mu, \sigma^2) \tag{1.1}最佳分类超平面 也就是说我们根据$P_{sample}$点训练出来的感知器的分类器超平面很可能会出现可以完美地划分$P_{sample}$点，但是却不能正确地划分对新来的测试样本的现象。因为新来的样本很可能位于蓝色的样本点的位置，也就是表现出了严重的过拟合现象， 而我们的支持向量机的机制可以很好地减免这种现象，具有更好的泛化能力。我们用几张图来表述下导致这种过拟合的原因：Figure 1, 感知器分类超平面能将线性可分的样本完美分割，但是由于样本叠加了高斯噪声$N$，所以当测试样本的数据出现在超平面“穿过”的“绿圈”之内时，就可能会出现错分的情况，这就是过拟合的一种表现。Figure 2,假设我们的样本集都是独立同分布采样的，那么其叠加的高斯噪声$N$应该都是相同分布$N \sim N(\mu, \sigma^2)$的，因此这个绿圈的大小应该都是相同的，因此最佳的分类超平面应该是可以和距离它最接近的若干个样本的边界相切的。我们把最接近超平面的若干个样本点称为支持向量，支持向量和超平面的距离越远，相当于我们可以容忍的噪声的高斯分布的方差越小，泛化性能越好。注意，这里的高斯分布的方差是我们假设的，不一定是实际数据集叠加的高斯噪声分布的方差，但是假设的越大，总是能带来更好的泛化能力。 SVM提出 我们在上面谈到了最佳分类超平面应能够使得支持向量距离超平面的距离最大，这个就是支持向量机的基本机制的最优化的目标，我们需要解决这个问题就必须要先数学形式化我们这个目的，只有这样才能进行最优化和求解。 数学形式化表述 我们这里对SVM问题进行数学上的形式化表述，以便于求解，这里主要讨论SVM的原问题，实际上，SVM通常转化为对偶问题进行求解，我们将在下一篇文章里讨论SVM的对偶问题。 函数间隔和几何间隔 我们刚才的表述中，我们知道了SVM的关键就是：使得支持向量距离分类超平面的距离之和最小，这里涉及到了“距离”这个概念，因此我们就必须要定义这个“距离”。这个距离可以定义为函数间隔(functional margin)和几何间隔(geometric margin)。我们分别来观察下这两个间隔。 函数间隔 我们表征一个样本点$x_i$到达一个超平面$\theta(x)=W^Tx+b$的距离，直接可以表述为: \gamma_i = y_i\theta(x_i) = y_i(W^Tx_i+b), \ x_i \in R^n \tag{2.1} 其中$y_i$为正确的标签，为$+1$或$-1$，乘上$y_i$的目的是当$x_i$分类正确的时候$\gamma_i$为正，而当分类错误的时候，$\gamma_i$为负，负数的最大值不超过0，所以也就不存在最大间隔了。整个式子也很好理解，当$x_i$使得$\theta(x)=0$时，该样本点就处于超平面上，当$x_i$使得$\theta(x)$大于0时，该样本点处于超平面之外，该值越大离超平面就越远。 几何间隔 函数间隔可以在一定程度上表征距离，但是存在一个问题，就是在$W$和$b$同时增大一个相同的倍数$\alpha$时，变成$\theta(x)=\alpha W^Tx+\alpha b$时，因为当$\alpha \neq 0$时，其零点还是相同的，所以表示的还是相同的超平面。但是我们从函数间隔的定义中可以看出，如果两者都放大$\alpha$倍，那么其函数间隔也被放大了$\alpha$倍，这个就不符合我们的需求了，我们希望的是只要是相同的一个样本点和一个固定的超平面，那么它们之间的距离应该是一定的，这个也是符合我们直观的。因此我们将函数间隔标准化，定义了几何间隔： \hat{\gamma_i} = \frac{y_i(W^Tx_i+b)}{||W||_{L2}} \tag{2.2} $||W||_{L2}$是L2范式，由于这个标准化因子的作用，使得$\hat{\gamma_i}$的值不会随着放大因子$\alpha$的变化而变化了。很容易看出： \gamma_i = \hat{\gamma_i}||W||_{L2} \tag{2.3}最大化最小距离 定义了几何间隔和函数间隔之后，我们就需要最大化最小距离了，这个听起来挺绕口的，其实意思很简单，就是求得一组$W$和$b$的情况下的最小样本距离，然后在不同的$W$和$b$的情况下最大化这个最小样本距离，最后得出的结果就是能够使得支持向量到超平面的距离最大的超平面了。我们观察下公式可能会更直观一些： \gamma = \min_{N=1,\cdots,N} \gamma_i, \ i=1,\cdots,N \tag{2.4} \hat{\gamma} = \min_{N=1,\cdots,N} \hat{\gamma_i}, \ i=1,\cdots,N \tag{2.5}这个就是最小几何间隔距离，我们现在最大化它，有： \max_{W,b} \hat{\gamma} \tag{2.6}将两者写在一起，可以表述为： \max_{W,b} \hat{\gamma} \\ s.t. \ \frac{y_i(W^Tx_i+b)}{ \Vert W \Vert} \geq \hat{\gamma}, \ i=1,\cdots,N \tag{2.7}容易看出其中的$\frac{y_i(W^Tx_i+b)}{ \Vert W \Vert} \geq \hat{\gamma}, \ i=1,\cdots,N$ 其实是和约束条件$\hat{\gamma} = \min_{N=1,\cdots,N} \hat{\gamma_i}, \ i=1,\cdots,N$等价的。我们做一些恒等变换有: \max_{W,b} \frac{\gamma}{ \Vert W \Vert} \\ s.t. \ y_i(W^Tx_i+b) \geq \gamma, \ i=1,\cdots,N \tag{2.8} 这里我们要想一下：$\gamma$的具体取值会不会影响到最优化后的$W$和$b$的取值呢？答案是不会的，因为我们只要令所有支持向量，也就是距离超平面最近的若干个样本点到超平面的距离为单位量，比如为1即可，这个可以通过等比例调整$W$和$b$容易地做到，其他样本也会随着进行相应的缩放。这样对整个超平面的最优化点是没有任何影响的。所以我们现在将$\gamma$设为常数1。现在有： \max_{W,b} \frac{1}{ \Vert W \Vert} \\ s.t. \ y_i(W^Tx_i+b) \geq 1, \ i=1,\cdots,N \tag{2.9}此时最大化问题转化为最小化问题： \min_{W,b} \frac{1}{2} \Vert W \Vert^2 \\ s.t. 1-y_i(W^Tx_i+b) \leq 0, \ i=1,\cdots,N \tag{2.10}至此，我们得到了SVM的标准原问题表达。注意到这个式子里的$1-y_i(W^Tx_i+b) \leq 0,$，当存在$x_i$使得$1-y_i(W^Tx_i+b) = 0$时，这个$x_i$就被称之为支持向量。如下图的虚线上的红色样本和蓝色样本所示，虽然样本有很多个，但是有效的，决定超平面的样本，也就是支持向量一共就只有五个，其到决策面的距离被标准化为了1。 我们接下来将会讨论SVM原问题拉格朗日函数形式以及其对偶问题，以便于更好地解决这个最优化问题。]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习Debug沉思录]]></title>
    <url>%2F2018%2F10%2F21%2Fdl_debug_thinking%2Fdl_debug%2F</url>
    <content type="text"><![CDATA[前言接触深度学习也有一两年了，一直没有将一些实战经验整理一下形成文字。本文打算用来纪录一些在深度学习实践中的调试过程，纪录一些经验之谈。因为目前深度学习业界的理论基础尚且薄弱，很多工程实践中的问题没法用理论解释得很好，这里的只是实践中的一些经验之谈，以供参考以及排错。本文将持续更新。需要强调的是，本文的很多单纯只是经验，在尽可能列出参考文献的同时却并无严格理论验证，希望大家见谅。欢迎大家集思广益，共同维护这个经验集，为整个社区贡献微弱力量。 如有问题请指出，转载请标注出处，联系方式： e-mail: FesianXu@163.comQQ: 973926198github: https://github.com/FesianXu 在分类问题中，损失函数及其快速得下降为0.0000在分类问题中，我们一般采用的是交叉熵[1]作为损失函数，如式(1.1)所示 \begin{aligned} \mathcal{L}_{cls} &= - \sum_{i=1}^n y_i \log{\hat{y}_i} = - \mathbf{y}^T \log{\mathbf{\hat{y}}} \\ \mathbf{y} &\in \mathbb{R}^n, \mathbf{\hat{y}} \in \mathbb{R}^n \end{aligned} \tag{1.1}其中$\hat{y}$和$\mathbf{\hat{y}}$是预测结果，以概率分布的形式表达，如$[0.2,0.3,0.3,0.2]$等，一般是通过softmax层实现，$y$和$\mathbf{ {y} }$是样本真实标签，在单分类问题中，采用的是独热编码[2]，只有一个分量是为1的，如$[0.0,1.0,0.0,0.0]$。（公式第二行是向量化表达） 我们发现，交叉熵损失的下确界是0，但是永远都不可能达到0，因为要达到0，那么所有的预测向量分布就必须完全和真实标签一致，退化为独热编码。但是实际上在神经网络中，经过了softmax层之后，是不可能使得除了目标分量的其他所有分量为0的（这个这里只是抛出了结论，讨论需要比较长的篇幅。），因此永远不可能达到0的，正是因为如此，交叉熵损失可以一直优化，这也是其比MSE损失优的一个点之一。 既然注意到了不可能为0，我们就可以分析，这肯定是自己程序问题，我们将经过softmax之前的logit打印出，如： [1023,-201,1021,124]发现了没有，这些值都很大，而softmax函数为: P(x_i) = \dfrac{\exp(x_i)}{\sum_{i=1}^n \exp{(x_i)}} \tag{1.2}我们会发现，过大或者过小的指数项，比如1023，会涉及到计算$e^{1023}$，这个数值在TensorFlow或者大部分框架中是溢出的，显示为inf，因此就会把该分量拉成1，而其他变成了0。这种操作是会导致严重的过拟合的。因此，一般来说，logit值不能太大，否则将会出现数值计算问题。 那么如何解决？出现这种问题的情况很多时候是因为参数初始化导致的数值计算问题，比如都采用了方差过小的高斯分布进行初始化，那么就会把网络的输出的范围拉的特别大，导致以上的问题。因此在参数初始化中，确保每一层的初始化都是在一定范围内的，可以考虑采用Xavier初始化，Kaiming初始化等。（这个初始化的影响我们将会以后讨论，这是一个新的话题。） 在正则化的过程中对神经网络的偏置也进行了正则一般来说，我们常用的是二范数正则，也即是岭回归，如式子(2.1) \mathcal{L} = \gamma_{\mathbf{w}}(y,\hat{y})+\dfrac{1}{2}\mathbf{w}^T\mathbf{w} \tag{2.1}一般来说，我们只会对神经网络的权值进行正则操作，使得权值具有一定的稀疏性[21]或者控制其尺寸，使得其不至于幅度太大，减少模型的容量以减少过拟合的风险。同时，我们注意到神经网络中每一层的权值的作用是调节每一层超平面的方向（因为$\mathbf{w}$就是其法向量），因此只要比例一致，不会影响超平面的形状的。但是，我们必须注意到，每一层中的偏置是调节每一层超平面的平移长度的，如果你对偏置进行了正则，那么我们的$b$可能就会变得很小，或者很稀疏，这样就导致你的每一层的超平面只能局限于很小的一个范围内，使得模型的容量大大减少，一般会导致欠拟合[7]的现象。 因此，一般我们不会对偏置进行正则的，注意了。 学习率太大导致不收敛不收敛是个范围很大的问题，有很多可能性，其中有一种是和网络结构无关的原因，就是学习率设置的太大了，如下图所示，太大的学习率将会导致严重的抖动，使得无法收敛，甚至在某些情况下可能使得损失变得越来越大直到无穷。这个时候请调整你的学习率，尝试是否可以收敛。当然，这里的“太大”目前没有理论可以衡量，不过我喜欢从$10^{-3} \sim 10^{-4}$的Adam优化器[4]开始进行尝试优化。 下图展示了过大过小的学习率对模型性能的影响曲线图： 别在softmax层前面的输入施加了激活函数softmax函数如式(4.1)所示： S(x_i)=\dfrac{\exp{(x_i)}}{\sum_{j=1}^n \exp(x_j)} \tag{4.1}假设我们的网络提取出来的最后的特征向量是$\tilde{\mathbf{y}} = f_{\theta}(\mathbf{x}), \tilde{\mathbf{y}}\in\mathbb{R}^m$，如果我们最后的分类的类别有$n$类，那么我们会用一个全连接层将其映射到对应维度的空间里面，如式(4.2)。 \mathbf{y}=g_{\mathbf{w}}(\tilde{\mathbf{y}}) \\ \mathbf{y} \in \mathbb{R}^n \tag{4.2}那么，这个全连接层虽然说可以看成是分类器，但是我们最好把它看成是上一层的“近线性可分特征”的一个维度转换（有点绕，意思是我们这里只是一个维度的转换，而不涉及到kernel），不管怎么说，这个时候，我们的输出是不能有激活函数的，如下式是不可以的： \mathbf{y}=\sigma(g_{\mathbf{w}}(\tilde{\mathbf{y}}) ) \\ \sigma(\cdot) 为激活函数\\ \mathbf{y} \in \mathbb{R}^n \tag{4.3}这时候的输出，具有和分类类别相同的维度，在很多框架中被称之为logits值，这个值一般是在实数范围内的，一般不会太大，参考笔记第一点的情况。 检查原数据输入的值范围原始数据输入可能千奇百怪，每个特征维的值范围可能有着数量级上的差别，这个时候如果我们不对数据进行预处理，将会大大增大设计网络的负担。一般来说我们希望输入的数据是中心对齐的，也即是0均值的[5]，可以加速网络收敛的速度。同时，我们希望不同维度上的数值范围是一致的，可以采用一些归一化[6]的手段进行处理（这个时候假设每个维度重要性是一样的，比如我们图片的三个通道等）。 别忘了对你的训练数据进行打乱经常，你的训练过程非常完美，能够很好地拟合训练数据，但是在测试过程中确实一塌糊涂，是的，你的模型这个时候过拟合[7]了。这个时候你会检查模型的有效性，不过在进行这一步之前，不妨先检查下你的数据加载器(Data Loader)是否是正常设计的。 一般来说，我们的训练数据在训练过程中，每一个epoch[8]中，都是需要进行打乱(shuffle)的，很多框架的数据加载器参数列表中都会有这项选项，比如pytorch的DataLoader类[9]。为什么需要打乱呢？那是因为如果不打乱我们的训练数据，我们的模型就有可能学习到训练数据的个体与个体之间特定的排列顺序，而这种排列顺序，在很多情况下是无用的，会导致过拟合的糟糕现象。因此，我们在训练过程中，在每一个epoch训练中都对训练集进行打乱，以确保模型不能“记忆”样本之间的特定排序。这其实也是正则的一种手段。 在训练中，大概如： epoch\_1 \rightarrow shuffle \rightarrow epoch\_2 \rightarrow shuffle \cdots 一个batch中，label不要全部相同这个情况有点类似与笔记的第六点，我们需要尽量给训练过程中人为引入不确定性，这是很多正则手段，包括dropout，stochastic depth等的思路，这样能够有效地减少过拟合的风险。因此，一个batch中，尽量确保你的样本是来自于各个类的（针对分类问题而言），这样你的模型会减少执着与某个类别的概率，减少过拟合风险，同时也会加快收敛速度。 少用vanilla SGD优化器在高维度情况下的优化，其优化平面会出现很多鞍点（既是梯度为0，但却不是极点），通常，鞍点会比局部极值更容易出现（直观感受就是，因为高维度情况下，一个点周围有很多维度，如果是极值点，那么就需要其他所有维度都是朝向同一个方向“弯曲”的，而这个要比鞍点的各个方向“弯曲”的情况可能要小），因此这个时候我们更担心陷于鞍点，而不是局部极小值点（当然局部极小值点也是一个大麻烦，不过鞍点更麻烦）。如果采用普通的SGD优化器，那么就会陷于任何一个梯度为0的点，也就是说，极有可能会陷于鞍点。如果要使用SGD方法，建议使用带有momentum的SGD方法，可以有效避免陷入鞍点的风险。 下图是某个函数的三维曲线图和等高线图，我们可以看到有若干个局部最优点和鞍点，这些点对于vanilla SGD来说是不容易处理的。 检查各层梯度，对梯度爆炸进行截断有些时候，你会发现在训练过程中，你的损失突然变得特别大，或者特别小，这个时候不妨检查下每一层的梯度（用tensorboard的distribution可以很好地检查），很可能是发生了梯度爆炸(gradient explosion)的情况，特别是在存在LSTM等时序的网络中，很容易出现这种情况。因此，这个时候我们会用梯度截断进行处理，操作很简单粗暴，就是设置一个阈值，把超过这个阈值的梯度全部拉到这个阈值，如下图所示： 在tensorflow中也提供了相应的API供梯度截断使用[10]，如：123456tf.clip_by_value( t, clip_value_min, # 指定截断最小值 clip_value_max, # 指定截断最大值 name=None) 具体使用见[11]，在应用梯度之前，对梯度截断进行处理。 检查你的样本label有些时候，你的训练过程可以很好地收敛，当使用MSE损失[12]的时候甚至可能达到0.0000的情况。但是，当你把模型拿到测试集中评估的时候，却发现性能极差，仿佛没有训练一样。这是过拟合吗？显然是的，但是这可能并不是你的模型的问题，请检查你的数据加载中训练集的样本标签是否正确对应。 这个问题很白痴，但是却真的很容易在数据加载过程中因为种种原因把label信息和对应样本给混掉。根据文献[13]中的实验，用MSE损失的情况下，就算是你的label完全随机的，和样本一点关系都没有，也可以通过基于SGD的优化算法达到0.0000损失的。因此，请务必确保你的样本label是正确的。 分类问题中的分类置信度问题在分类问题中我们一般都是采用的是交叉熵损失，如式子(1.1)所示，在一些实验中，如果我们绘制出训练损失和分类准确度的曲线图，我们可能会有下图这种情况[14]： 其中上图为分类损失，紫色为训练损失，蓝色为测试损失，下图为分类准确度，绿色为训练准确度，蓝色为测试准确度。我们不难发现一个比较有意思的现象，就是当测试损失开始到最低点，开始向上反弹的时候，其测试准确度却还是上升的，而不是下降。 这是为什么呢？为什么分类准确度不会顺着分类损失的增大而减少呢？ 这个涉及到了分类过程中对某个类的“置信程度”的多少，比如： [0.9,0.01,0.02,0.07]模型是对第一类相当确信的，但是在第二种情况： [0.5,0.4,0.05,0.05]这对第一类的置信程度就很低了，虽然按照贝叶斯决策，还是会选择第一类作为决策结果。因此这就是导致以上现象的原因，在那个拐点后面，这个模型对于分类的置信程度其实已经变得很差了，虽然对于准确度而言，其还能分类正确。 但是这其实正是过拟合的一种表现，模型已经对自己的分类结果不确信了。 少在太小的批次中使用BatchNorm层Batch Normalization[15]，中文译作批规范化，在深度学习中是一种加快收敛速度，提高性能的一个利器，其本质和我们对输入的原数据进行0均值单位方差规范化差不多，是以batch为单位，对中间层的输出进行规范化，可以缓和内部协方差偏移(Internal Covariate Shift)的现象。其基本公式很简单，如下： \begin{aligned} \tilde{x} &= \dfrac{x_i-\mu}{\sigma_i} \\ x_i^{\rm{norm}} &= \gamma_i \cdot \tilde{x} + \beta_i \end{aligned} \tag{12.1}不过这里并不打算对BN进行详细讲解，只是想告诉大家，因为BN操作在训练过程中是对每个batch进行处理的，从每个batch中求得均值和方差才能进行操作。如果你的batch特别小（比如是受限于硬件条件或者网络要求小batch），那么BN层的batch均值和方差可能就会不能很好符合整个训练集的统计特征，导致差的性能。实际上，实验[16]说明了这个关系，当batch小于16时，性能大幅度下降。因此，少在太小的batch中使用BN层，如果实在要使用，在发生性能问题时优先检查BN层。 数值计算问题，出现NanNan(Not An Number)是一个在数值计算中容易出现的问题，在深度学习中因为涉及到很多损失函数，有些损失函数的定义域并不是整个实数，比如常用的对数，因此一不小心就会出现Nan。在深度学习中，如果某一层出现了Nan，那么是具有传递性的，后面的层也会出现Nan，因此可以通过二分法对此进行排错。 一般来说，在深度学习中出现Nan是由于除0异常或者是因为损失函数中的（比如交叉熵，KL散度）对数操作中，输入小于或者等于0了，一般等于0的情况比较多，因此通常会： \log(x+\epsilon) \approx \log(x)这里的$\epsilon$是个很小的值，一般取$10^{-6}$即可，可以防止因为对数操作中输入0导致的Nan异常。 需要注意的是，有些时候因为参数初始化或者学习率太大也会导致数值计算溢出，这也是会出现Nan的，一般这样会出现在较前面的层里面。 BN层放置的位置问题BN层有两种常见的放置位置，如下图所示：第一个是放在激活函数之前：第二个是放在激活函数之后： 在原始BN的论文[15]中，Batch Norm(BN)层是位于激活层之前的，因为是对原始的，未经过激活的logit数据进行数据分布的重整。然而，不少实验证实似乎BN层放在激活层之后效果会更好，这个原因目前不明。 dropout层应用在卷积层中可能导致更差的性能dropout[19]是hinton大神与2012年提出的一种神经网络正则手段，其可以简单解释为在训练过程中，按一定概率让神经网络中的某些神经元输出为0，其原因可以有几个解释，一个是作为一种集成模型进行解释，另一个可以看成是在特征提取学习过程中给数据加入噪声，可以看成是一种数据增强的正则手段。 在原始论文中，dropout被应用于全连接层中，而没有应用在卷积层中，Hinton的解释是因为卷积层参数并不多，过拟合风险较小不适合采用dropout这种大杀器的正则手段。有人也认为因为卷积网络是局部感知的，用dropout正则对于其在后层中对于全局信息的获取可能具有负作用[20]。 不过在一些工作中，也有人将dropout层应用在卷积层中的[17-18]，其层次安排为:$CONV-&gt;RELU-&gt;DROP$，不过其丢弃率$p$都是选择的较小数如$0.1$，$0.2$等，个人觉得这里的作用大概是对中间数据进行加入噪声，以便于数据增强的正则手段。 个人建议是可以尝试在卷积层中使用少量的dropout，用较小的丢弃率，但是最后别忘了扔掉这些dropout再进行一些探索，也许可以具有更好的效果。 较小的batch size可以提供较好的泛化现代的深度学习优化器基本上都是基于SGD算法进行修改而成的，在每一次训练中都是以一个batch size为单位进行训练的，在这个过程中相当于在统计这个batch中样本的一些统计特性，因此batch size是会影响模型的超曲线形状的。 一般来说较大的batch size比如128，256会和整个训练集的统计性质更相近，从而使得具有较少的多样性，而较小的batch size 比如16，32，因为batch size较小，不同batch之间的差异性较大，这种差异性可以看成是正则手段，有机会提高模型的泛化性能。（不过有些文章似乎不同意这个观点，认为较大batch size有较好性能，个人建议是大batch size和小batch size都可以跑跑，有可能能提升性能。） Reference[1]. Janocha K, Czarnecki W M. On loss functions for deep neural networks in classification[J]. arXiv preprint arXiv:1702.05659, 2017.(Overview about loss function used in DNN)[2]. tf.one_hot()进行独热编码[3]. 曲线拟合问题与L2正则[4]. Kinga D, Adam J B. A method for stochastic optimization[C]//International Conference on Learning Representations (ICLR). 2015, 5.[5]. &lt;深度学习系列&gt;深度学习中激活函数的选择[6]. 机器学习之特征归一化（normalization）[7]. 机器学习模型的容量，过拟合与欠拟合[8]. 在机器学习中epoch, iteration, batch_size的区别[9]. Pytorch Dataloader doc[10]. tf.clip_by_value[11]. 梯度截断的tensorflow实现[12]. 均方误差(MSE)和均方根误差(RMSE)和平均绝对误差(MAE)[13]. Du S S, Zhai X, Poczos B, et al. Gradient Descent Provably Optimizes Over-parameterized Neural Networks[J]. arXiv preprint arXiv:1810.02054, 2018.[14]. The inconsistency between the loss curve and metric curve?[15]. Ioffe S, Szegedy C. Batch normalization: accelerating deep network training by reducing internal covariate shift[C]// International Conference on International Conference on Machine Learning. JMLR.org, 2015:448-456.[16]. Wu Y, He K. Group normalization[J]. arXiv preprint arXiv:1803.08494, 2018.[17]. Park S, Kwak N. Analysis on the dropout effect in convolutional neural networks[C]//Asian Conference on Computer Vision. Springer, Cham, 2016: 189-204.[18]. Where should I place dropout layers in a neural network?[19]. Hinton G E, Srivastava N, Krizhevsky A, et al. Improving neural networks by preventing co-adaptation of feature detectors[J]. arXiv preprint arXiv:1207.0580, 2012.[20]. Why do I never see dropout applied in convolutional layers?[21]. L1正则]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Debug Practice</tag>
      </tags>
  </entry>
</search>
