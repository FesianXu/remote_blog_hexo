<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>机器学习杂货铺2号店</title>
  
  <subtitle>Machine Learning Store Num. 2. The records of my learning and life chores.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://blog.csdn.net/LoseInVain/"/>
  <updated>2018-10-21T15:56:56.950Z</updated>
  <id>https://blog.csdn.net/LoseInVain/</id>
  
  <author>
    <name>徐飞翔(Fesian Xu)</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>《贝叶斯之旅》第二讲，分类问题的两大过程，推理和决策</title>
    <link href="https://blog.csdn.net/LoseInVain/2018/10/21/bayesian/cls_stage/"/>
    <id>https://blog.csdn.net/LoseInVain/2018/10/21/bayesian/cls_stage/</id>
    <published>2018-10-21T15:53:57.665Z</published>
    <updated>2018-10-21T15:56:56.950Z</updated>
    
    <content type="html"><![CDATA[<p><font size="6"><b>前言</b></font><br><strong>前面[1]我们介绍了贝叶斯决策的一些知识，介绍了基于最小化分类错误率和最小化分类损失的两种决策准则，接下来，我们简单讨论下分类问题中的二个步骤，推理和决策。</strong></p><p><strong>如有谬误，请联系指正。转载请注明出处。</strong></p><p><em>联系方式：</em><br><strong>e-mail</strong>: <code>FesianXu@163.com</code><br><strong>QQ</strong>: <code>973926198</code><br><strong>github</strong>: <code>https://github.com/FesianXu</code></p><hr><h1 id="分类问题"><a href="#分类问题" class="headerlink" title="分类问题"></a>分类问题</h1><p>我们在之前的文章中已经介绍过分类问题了，简单的说就是给定一个样本$\bf{x} \in \mathbb{R}^n$，将其划分到有限的标签集$\bf{Y} \in {0,1,\cdots,m}$中。通常来说，我们可以将整个分类问题划分为两个独立的过程，分别是<strong>推理（inference）</strong>和<strong>决策(decision)</strong>阶段。在推理阶段，我们通过已有的训练集，学习到后验概率$p(\mathcal{C}_k|\bf{x})$，或者也可以通过学习联合概率分布$p(\mathcal{C}_k, \bf{x})$，然后也可以得到后验概率。而接下来，在决策阶段，就根据这个后验概率，对样本的类别进行判断决策。这个决策过程可以参考文章[1]的讨论。</p><p><strong>注意到，很多时候，这两个过程可以合在一起，将问题简化为成：学习一个映射$f(\bf{x}) \in \mathbb{R}^m, \bf{x} \in \mathbb{R}^n$，直接将样本映射到类别标签。</strong>这个过程中，将不会涉及到任何的后验概率等，而是直接得出预测结果，这个函数因此称之为<strong>判别函数(Discriminant function)</strong>。[2] page 43</p><p>事实上，这些讨论过的方法都可以用来解决分类问题，并且在实际应用中都有所应用，我们按照复杂程度进行降序排列之后，有：</p><ol><li><p>通过解决推理问题之后，我们可以给每一个类别估计出类条件概率$p(\mathbf{x}|\mathcal{C}_k)$，同时，先验概率$p(\mathcal{C}_k)$也很容易可以估计出来，然后通过贝叶斯公式我们可以得到后验概率：</p><script type="math/tex; mode=display">p(\mathcal{C}_k | \mathbf{x}) = \frac{p(\mathbf{x}|\mathcal{C}_k)p(\mathcal{C}_k)}{p(\mathbf{x})}\tag{1.1}</script><p>我们有:</p><script type="math/tex; mode=display">p(\mathbf{x}) = \sum_{k} p(\mathbf{x}|\mathcal{C}_k)p(\mathcal{C}_k)\tag{1.2 对输入分布进行建模}</script><p>等价地，我们可以对联合概率密度$p(\mathbf{x},\mathcal{C}_k)$进行建模，然后进行标准化后得到后验概率。像这种显式地或者隐式地对输入和输出进行概率分布建模的模型，称之为<strong>生成模型(generative models)</strong>，因为从这个联合分布中进行采样可以生成输入空间中的一些<strong>虚假生成数据(synthetic data)</strong>。</p></li><li><p>通过解决推理问题后，得到后验概率$p(\mathcal{C}_k|\mathbf{x})$，然后通过决策论进行类别判断。这种模型称之为<strong>判别模型(Discriminative model)</strong>。</p></li><li>寻找一个函数$f(\mathbf{x})$，称之为判别函数，直接将输入的$\mathbf{x}$映射到一个类别标签上，比如SVM分类器等。在这个情形下，并没有用到任何概率，也就是说我们对预测的结果其实是没有办法判断可靠程度的。</li></ol><p>我们接下来分别讨论下这三种方法的优劣点。</p><hr><h1 id="孰优孰劣，判别模型和生成模型"><a href="#孰优孰劣，判别模型和生成模型" class="headerlink" title="孰优孰劣，判别模型和生成模型"></a>孰优孰劣，判别模型和生成模型</h1><h2 id="生成模型"><a href="#生成模型" class="headerlink" title="生成模型"></a>生成模型</h2><p>生成模型是对于数据量需求最高的，同时运算量也是最大的，因为其需要训练出包含$\mathbf{x}$和$\mathcal{C}_k$的联合分布，如果数据量不够，将会导致严重的过拟合现象[3]。对于很多应用下来说，$\mathbf{x}$是一个维度很高的特征向量，因此为了使得类条件概率得到一个较为合理的精度，就需要很多的数据量进行计算。但是，生成模型也有一些很好的性质，比如说可以从中进行采样生成出一些假数据，这个应用目前在很多image inpainting[4]，style transfer[5]任务中经常用到。而且，因为通过联合概率分布可以通过式子(1.2)计算出边缘概率分布$p(\mathbf{x})$。这个输入空间的边缘概率分布很有用，因为其可以判断输入的新数据是否是一个所谓的<strong>离群点(outlier)</strong>，离群点如下图所示。这个就是所谓的<strong>离群点检测(outlier detection)</strong>或者称之为<strong>异常检测(novelty detection)</strong>，这个在网络欺诈预测，银行欺诈预测，电子垃圾邮件检测中很有用。</p><p><img src="/imgs/bayesian/outlier.jpg" alt="outlier"></p><h2 id="判别模型"><a href="#判别模型" class="headerlink" title="判别模型"></a>判别模型</h2><p>在分类任务中，很多时候你只是做个分类而已，并不用进行离群点检测，也不需要生成虚假样本.这个时候，如果还用生成模型去进行后验概率的估计，就浪费了很多资源。我们观察下图，我们可以发现，类条件概率其实和后验概率并没有必然的影响。这个时候，你就需要采用判别模型。<br><img src="/imgs/bayesian/posterior_class.png" alt="posterior_class"><br>不仅如此，采用了判别模型还有一个好处就是，可以利用所谓的<strong>拒绝域(reject option)</strong>把一些过于边缘的判断拒绝掉。比如我们仅有10%的把握判断某人为癌症患者，那么我们就情愿不做这个判断，交给更为权威的人或者系统进行下一步的处理。如下图所示，绿色的水平线表示拒绝水平，只有后验概率高于这个水平线，才能认为是可靠的判断。我们将会看到，在基于判别函数的情况下，因为并没有概率的存在，因此并不能进行这种操作。</p><p><img src="/imgs/bayesian/reject.png" alt="reject"></p><h2 id="判别函数方法"><a href="#判别函数方法" class="headerlink" title="判别函数方法"></a>判别函数方法</h2><p>有比以上俩种方法更为简单，计算量更少的方法，那就是判别函数法。在这个情况下，因为是直接用训练数据拟合一个函数$f(\mathbf{x})$对样本进行分类，因此无法得到后验概率$p(\mathcal{C}_k|\mathbf{x})$。在这个方法中，只能最小化分类错误率，而没法给不同类型的分类错误进行区别[1]，采用最小化分类风险，这是个遗憾的地方。</p><hr><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p>[1] <a href="https://blog.csdn.net/LoseInVain/article/details/82780472">《贝叶斯之旅||第一讲，贝叶斯决策》</a></p><p>[2] Bishop C M. Pattern recognition and machine learning (information science and statistics) springer-verlag new york[J]. Inc. Secaucus, NJ, USA, 2006.</p><p>[3] <a href="https://blog.csdn.net/LoseInVain/article/details/78108990">《机器学习模型的容量，过拟合与欠拟合》</a></p><p>[4] <a href="https://blog.csdn.net/gavinmiaoc/article/details/80802967">《基于深度学习的Image Inpainting (图像修复)论文推荐(持续更新)》</a></p><p>[5] <a href="https://www.jianshu.com/p/b1189448eb2e" target="_blank" rel="noopener">《Image Style Transfer》</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;font size=&quot;6&quot;&gt;&lt;b&gt;前言&lt;/b&gt;&lt;/font&gt;&lt;br&gt;&lt;strong&gt;前面[1]我们介绍了贝叶斯决策的一些知识，介绍了基于最小化分类错误率和最小化分类损失的两种决策准则，接下来，我们简单讨论下分类问题中的二个步骤，推理和决策。&lt;/strong&gt;&lt;/p&gt;
&lt;p
      
    
    </summary>
    
      <category term="Bayesian Theory" scheme="https://blog.csdn.net/LoseInVain/categories/Bayesian-Theory/"/>
    
    
      <category term="贝叶斯理论" scheme="https://blog.csdn.net/LoseInVain/tags/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%90%86%E8%AE%BA/"/>
    
      <category term="统计学习方法" scheme="https://blog.csdn.net/LoseInVain/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"/>
    
      <category term="概率论" scheme="https://blog.csdn.net/LoseInVain/tags/%E6%A6%82%E7%8E%87%E8%AE%BA/"/>
    
  </entry>
  
  <entry>
    <title>《贝叶斯之旅》第一讲，贝叶斯决策</title>
    <link href="https://blog.csdn.net/LoseInVain/2018/10/21/bayesian/bayesian_decision/"/>
    <id>https://blog.csdn.net/LoseInVain/2018/10/21/bayesian/bayesian_decision/</id>
    <published>2018-10-21T15:40:47.917Z</published>
    <updated>2018-10-21T15:52:41.246Z</updated>
    
    <content type="html"><![CDATA[<p><font size="6"><b>前言</b></font><br><strong>在机器学习中，有两大门派，分别是频率学派和贝叶斯学派，在现在深度学习大行其道的时代下，数据量空前庞大，频率学派占据了比较大的优势，而贝叶斯学派似乎有点没落，然而，贝叶斯理论在机器学习中是有着很重要的地位的，它从理论上揭示了模型为什么可以工作，为什么会fail，在数据量必须小的一些任务中，通常也可以表现得比频率学派的好，让我们开始我们的贝叶斯之旅吧。这一讲，主要阐述的是在贝叶斯的观点中，我们如何根据现有的数据和假设，对未知的样本进行分类决策。</strong></p><p><strong>如有谬误，请联系指正。转载请注明出处。</strong></p><p><em>联系方式：</em><br><strong>e-mail</strong>: <code>FesianXu@163.com</code><br><strong>QQ</strong>: <code>973926198</code><br><strong>github</strong>: <code>https://github.com/FesianXu</code></p><hr><h1 id="为什么要贝叶斯"><a href="#为什么要贝叶斯" class="headerlink" title="为什么要贝叶斯"></a>为什么要贝叶斯</h1><p>我们在以前的文章<a href="https://blog.csdn.net/LoseInVain/article/details/80499147">《概率派和贝叶斯派的区别》</a>中，曾经讨论过频率学派和贝叶斯学派看待未知模型参数的一些观点，我们这里简单描述下就是：</p><blockquote><p>频率学派相信我们的模型参数尽管未知，但是其是有一个真实的值$\theta$的，只要我们的样本足够多，我们就可以准确无偏地估计出这个真实的值出来；而贝叶斯学派相信我们的模型的未知参数是一个随机变量，而不是一个简简单单的值，因此是符合一个分布的。也就是说，基于我们现有的样本数据，我们对模型中的未知参数的估计都是估计出这些未知参数先验分布的一些参数而已，比如高斯分布的均值和协方差矩阵等等，在贝叶斯学派眼中，模型的参数本身就不是确定的，因此只能用随机变量表达。</p></blockquote><p>我们从以上的区别中可以看出，在贝叶斯模型中，因为每个参数都是一个随机变量，也即是符合某个分布的，如果我们对数据的来源有一定的自信（比如我们的数据是关于电子科技大学的男女比例，我们就会知道这个比例将会大到爆炸，这个我们是很有自信的，因此可以作为先验概率引入的。），那么你将可以通过假设参数分布的形式，引入你对数据的<strong>先验知识</strong>（prior knowledge），我们称之为对参数的先验假设，表示为$p(\theta)$。我们以后将会发现，如果这个先验知识<strong>足够合理</strong>，将会使得模型即使是在小规模的数据上训练，都可以获得较为理想的效果，这点是频率学派模型较难做到的。</p><p>总结来说，也就是<strong>贝叶斯模型在小数据集上具有更好的泛化性能</strong>，至于什么叫泛化性能，参考以前文章<a href="https://blog.csdn.net/LoseInVain/article/details/78746520">《经验误差，泛化误差》</a>。</p><hr><h1 id="利用贝叶斯理论进行分类"><a href="#利用贝叶斯理论进行分类" class="headerlink" title="利用贝叶斯理论进行分类"></a>利用贝叶斯理论进行分类</h1><p>在进行进一步讨论之前，我们对我们接下来需要用的的符号进行统一的规定表示和解释：</p><ol><li><strong>样本(sample)</strong>，$\mathbf{x} \in \mathbb{R}^n$，其中的$n$称之为样本的<strong>维度(dimension)</strong>。</li><li><strong>状态(state)</strong>，第一类：$\omega = \omega_1$;第二类：$\omega = \omega_2$，在其他文献中，这个通常也称之为<strong>类别(class)</strong>，指的是某个样本配对的类别属性。</li><li><strong>先验概率(prior)</strong>，$p(\omega_1)$，$p(\omega_2)$，指的是对某些类别的预先知道的知识，比如在预测某个病人是否是癌症病人的例子，在没有得到任何关于这个病人的信息之前，因为我们知道得癌症是一个较为低概率的事件，因此其先验概率$p(\omega=癌症)$是一个很小的值。先验概率表现了我们对于某个知识的“信仰”。</li><li><strong>样本分布密度(sample distribution density)</strong>，$p(\mathbf{x})$。</li><li><strong>类条件概率密度(class-conditional probablity density)</strong>，$p(\mathbf{x}|\omega_1)$,$p(\mathbf{x}|\omega_2)$，这个概率也经常被称之为<strong>似然概率(likelihood probablity)</strong>。</li></ol><p>以上的术语将会在以后的文章中经常见到，我们届时再做更加深入的讨论。</p><hr><p>让我们考虑一个情景：</p><blockquote><p>给你$n$个样本作为已知的训练集，$\mathbf{X}={\mathbf{x}_1,\mathbf{x}_2,\cdots,\mathbf{x}_n}$，其对应的标签为，$\mathbf{Y}={\mathbf{y}_1}$，先给你一个新的样本$\mathbf{x}$，其需要预测其标签。</p></blockquote><p>这个就是基本的分类问题的情景，为了简便，不妨将这里的标签看成是二分类标签$\mathbf{y}_i \in {+1,-1}$。我们可以将这个分类问题等价为求$p(\omega_1|\mathbf{x})$和$p(\omega_2|\mathbf{x})$的概率大小，一般来说，如果$p(\omega_1|\mathbf{x}) &gt; p(\omega_2|\mathbf{x})$，那么就可以将其判断为第一类了对吧！反之亦然。</p><p>因为有概率论中的贝叶斯公式，我们有：</p><script type="math/tex; mode=display">p(\omega_i|\mathbf{x}) = \frac{p(\omega_i,\mathbf{x})}{p(\mathbf{x})}=\frac{p(\omega_i)p(\mathbf{x}|\omega_i)}{\sum_{j=1}^M p(\omega_i)p(\mathbf{x}|\omega_i)}\tag{1.1 贝叶斯公式}</script><p>因为$p(\mathbf{x})$在$p(\omega_1|\mathbf{x})$和$p(\omega_2|\mathbf{x})$都是一样的，因此在分类问题中，一般可以忽略这个项，我们有：</p><script type="math/tex; mode=display">p(\omega_i|\mathbf{x}) \propto p(\omega_i)p(\mathbf{x}|\omega_i)\tag{1.2 贝叶斯公式常用形式}</script><p>其中，$p(\omega_i)$称之为<strong>先验概率</strong>；$p(\mathbf{x}|\omega_i)$称之为<strong>似然概率</strong>，或者称之为<strong>类条件概率</strong>；$p(\omega_i|\mathbf{x})$称之为<strong>后验概率(posterior)</strong>。其中，因为我们已经有了先前样本$\mathbf{X}$以及其对应的标签$\mathbf{Y}$，因此可以估计出先验概率和似然概率出来（一般情况下，需要对似然概率进行建模，我们后续再讨论）。</p><p>$\nabla$<strong>总而言之，我们通过人工的先验概率，和从已有数据中学习到的似然概率中，可以得到后验概率，而后验概率为我们的分类提供了很重要的依据。</strong>$\nabla$</p><hr><h1 id="决策论，如何做出一个合理的选择"><a href="#决策论，如何做出一个合理的选择" class="headerlink" title="决策论，如何做出一个合理的选择"></a>决策论，如何做出一个合理的选择</h1><p>机器学习整个过程可以分为两个阶段，一是<strong>推理(inference)</strong>阶段，二是<strong>决策(decision)</strong>阶段。推理阶段主要是从训练样本集中估计出$p(\mathbf{x}, \mathbf{t})$分布，决策阶段是根据这个联合概率分布，如何作出一个合理的决策，对样本进行分类。</p><p><strong>决策论(Decision Theory)</strong>[1]指导我们如何根据在推理阶段得出的$p(\mathbf{x}, \mathbf{t})$分布进行合理的分类。一般来说，决策策略可分为<strong>最小错误分类率</strong>策略和<strong>最小期望损失</strong>策略，我们分别介绍下。</p><h2 id="最小错误分类率"><a href="#最小错误分类率" class="headerlink" title="最小错误分类率"></a>最小错误分类率</h2><p><strong>最小分类错误率(minimizing the misclassification rate)</strong>策略的主要目的就是让分类错误率最小化，这个在大多数情况下是适用的。我们先对分类错误率这个概念进行定义，显然，考虑二分类情况，将类别1的物体分类到了2或者相反就是误分类了，用数学表达式表达就是：</p><script type="math/tex; mode=display">p(\rm{mistake}) = p(\mathbf{x} \in \mathcal{R}_1,\mathcal{C}_2)+ p(\mathbf{x} \in \mathcal{R}_2,\mathcal{C}_1) \\= \int_{\mathcal{R}_1} p(\mathbf{x},\mathcal{C}_2) \rm{d}\mathbf{x}+\int_{\mathcal{R}_2} p(\mathbf{x},\mathcal{C}_1) \rm{d}\mathbf{x}</script><p>其中的$\mathcal{R}_k$称之为<strong>决策区域(decision regions)</strong>，如果输入向量在决策区域$k$下，那么该输入向量的所有样本都是被预测为了$k$类。$p(\mathbf{x} \in \mathcal{R}_i, \mathcal{C}_j)$表示将属于类别$j$的样本分类为了类别$i$。对于一个新样本$\mathbf{x}$，为了最小化$p(\rm{mistake})$，我们应该将其类别分到式子(2.1)中的被积函数中较小的一个，因为这样，较大的一项就会因为决策区域不适合而变为0了，因此只会剩下一项较小的。换句话说，就是如果$p(\mathbf{x},\mathcal{C}_1) &gt; p(\mathbf{x},\mathcal{C}_2)$，那么就将其预测为$\mathcal{C}_1$。</p><hr><p>我们这里引用[1] <em>page 40</em> 给出的图示进行理解，如下图所示，其中$\hat{x}$表示决策边界，大于$\hat{x}$将会被预测为第二类，小于则会被预测为第一类，于是，我们的决策错误率就是红色区域，绿色区域和蓝色区域的面积了。我们可以清楚的发现，不管$\hat{x}$怎么移动，绿色和蓝色区域的和是一个常数，只有红色区域会在变化，因此直观上看，只有当$\hat{x} = x_0$的时候，也就是$p(x,\mathcal{C_1})=p(x,\mathcal{C_2})$的时候，才会有最小分类错误率。我们有：</p><script type="math/tex; mode=display">p(x,\mathcal{C_1})=p(x,\mathcal{C_2}) \\\Rightarrow p(\mathcal{C}_1|x)p(x) = p(\mathcal{C}_2|x)p(x) \\\Rightarrow p(\mathcal{C_1|x}) = p(\mathcal{C_2|x})\tag{2.2 最优决策}</script><p>也就是说，当$p(\mathcal{C_1|x}) &gt; p(\mathcal{C_2|x})$时，选择$\mathcal{C}_1$作为理论分类错误率最小的选择。我们可以发现，选择具有最大后验概率的类别作为预测结果能够达到最小分类错误率的效果，这个原则我们称之为<strong>最大后验概率原则</strong>，同时，我们留意，在参数估计中也有一个称之为<strong>最大后验概率估计(maximize a posterior probablity, MAP)</strong>的原则，请不要混淆。<br><img src="/imgs/bayesian/map.png" alt="map"></p><hr><p>当类别多于2类时，比如有$K$类时，计算正确率将会更加方便，我们有：</p><script type="math/tex; mode=display">p(\rm{correct}) = \sum_{k=1}^K p(\mathbf{x} \in \mathcal{R}_k, \mathcal{C}_k) \\= \sum_{k=1}^K \int_{\mathcal{R}_k} p(\mathbf{x},\mathcal{C}_k) \rm{d} \mathbf{x} \tag{2.3 多类分类的正确率}</script><p>同理的，同样是选择具有最大后验概率的类别作为预测结果，能够达到最小分类错误率。</p><p><strong>注意到，这个原则有一些等价的表达形式，我们将会在这个系列的附录中进行补充。</strong></p><hr><h2 id="最小期望损失"><a href="#最小期望损失" class="headerlink" title="最小期望损失"></a>最小期望损失</h2><p>按道理来说，最小分类错误已经可以在绝大多数任务中使用了，但是有一些任务，比如医生根据CT影像对病人进行癌症的诊断，在这些任务中，<strong>错报</strong>和<strong>漏报</strong>可有着不同的后果。如果只是<strong>错报</strong>，将没有疾病的人诊断为病人，顶多再去进行一次体检排查，但是如果将有癌症的患者漏报成没有疾病的人，那么就可能错失了最佳的治疗时机，因此这种情况下，这两种错误方式可有着不同的<strong>代价</strong>。</p><p>为了对这个代价进行数学描述，我们引入了一个<strong>损失矩阵(loss matrix)</strong>用来描述不同错误分类带来的不同代价：</p><div class="table-container"><table><thead><tr><th></th><th>cancer</th><th>normal</th></tr></thead><tbody><tr><td>cancer</td><td>0</td><td>1000</td></tr><tr><td>normal</td><td>1</td><td>0</td></tr></tbody></table></div><p>这个矩阵很好的描述了我们刚才的需求，让我们用$L$表示，其中$L_{i,j}$表示其第$i$行,$j$列的元素。与最小化分类错误率不同的，我们定义一个代价函数：</p><script type="math/tex; mode=display">\mathbb{E}[L] = \sum_{k} \sum_{j} \int_{\mathcal{R}_j} L_{k,j}p(\mathbf{x}, \mathcal{C}_k) \rm{d} \mathbf{x}\tag{3.1 代价函数}</script><p>我们的目标是最小化(3.1)。<br>当然，如果你需要对一个样本$\mathbf{x}$作出决策，你也许需要将其分解为：</p><script type="math/tex; mode=display">R(\alpha_k|\mathbf{x}) = \sum_j  L_{k,j}p(\mathcal{C}_k|\mathbf{x})\tag{3.2 分类风险}</script><p>这里的$R(\cdot)$表示Risk，表示分类为$k$类的风险，当然是越小越好。</p><p>因此总结来说，最小化风险的计算步骤为：</p><ol><li><strong>计算后验概率</strong>： <script type="math/tex; mode=display">p(\mathcal{C}_k|x) = \frac{p(x|\mathcal{C}_k) p(\mathcal{C}_k)}{\sum_{i=1}^c p(x|\mathcal{C}_i)p(\mathcal{C}_i)} \\k = 1,2,\cdots,c</script></li><li><strong>计算风险</strong>：<script type="math/tex; mode=display">R(\mathcal{R}_k|\mathbf{x}) = \sum_j  L_{k,j}p(\mathcal{C}_k|\mathbf{x})</script></li><li><strong>决策</strong>：<script type="math/tex; mode=display">\alpha = \arg \min _{i = 1, \cdots, c} R(\alpha_i | x)</script></li></ol><p><strong>显然，当损失矩阵是一个单位矩阵的时候，最小分类错误率和最小分类风险等价。</strong></p><hr><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p>[1] Bishop C M. Pattern recognition and machine learning (information science and statistics) springer-verlag new york[J]. Inc. Secaucus, NJ, USA, 2006.<br>[2] 张学工. 模式识别[J]. 2010.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;font size=&quot;6&quot;&gt;&lt;b&gt;前言&lt;/b&gt;&lt;/font&gt;&lt;br&gt;&lt;strong&gt;在机器学习中，有两大门派，分别是频率学派和贝叶斯学派，在现在深度学习大行其道的时代下，数据量空前庞大，频率学派占据了比较大的优势，而贝叶斯学派似乎有点没落，然而，贝叶斯理论在机器学习中是有
      
    
    </summary>
    
      <category term="Bayesian Theory" scheme="https://blog.csdn.net/LoseInVain/categories/Bayesian-Theory/"/>
    
    
      <category term="贝叶斯理论" scheme="https://blog.csdn.net/LoseInVain/tags/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%90%86%E8%AE%BA/"/>
    
      <category term="统计学习方法" scheme="https://blog.csdn.net/LoseInVain/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"/>
    
      <category term="概率论" scheme="https://blog.csdn.net/LoseInVain/tags/%E6%A6%82%E7%8E%87%E8%AE%BA/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow高阶函数之 tf.foldl()和tf.foldr()</title>
    <link href="https://blog.csdn.net/LoseInVain/2018/10/21/tensorflow/tf.foldr()/"/>
    <id>https://blog.csdn.net/LoseInVain/2018/10/21/tensorflow/tf.foldr()/</id>
    <published>2018-10-21T13:24:54.518Z</published>
    <updated>2018-10-21T13:25:35.316Z</updated>
    
    <content type="html"><![CDATA[<p><font size="6"><b>前言</b></font><br>在TensorFlow中有着若干高阶函数，如之前已经介绍过了的<code>tf.map_fn()</code>，见博文<a href="https://blog.csdn.net/LoseInVain/article/details/78815130">TensorFlow中的高阶函数：tf.map_fn()</a>，此外，还有几个常用的高阶函数，分别是<code>tf.foldl()</code>，<code>tf.foldr()</code>，我们简要介绍下。</p><hr><p><code>tf.foldl()</code>类似于python中的<code>reduce()</code>函数，假设<code>elems</code>是一个大于等于一阶的张量或者列表，形状如<code>[n,...]</code>，那么该函数将会重复地调用<code>fn</code>与这个列表上，从左到右进行处理，这里讲得不清楚，我们看看官方的API手册和一些例子理解一下，地址：<a href="https://www.tensorflow.org/api_docs/python/tf/foldl" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/foldl</a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tf.foldl(</span><br><span class="line">    fn,</span><br><span class="line">    elems,</span><br><span class="line">    initializer=<span class="keyword">None</span>,</span><br><span class="line">    parallel_iterations=<span class="number">10</span>,</span><br><span class="line">    back_prop=<span class="keyword">True</span>,</span><br><span class="line">    swap_memory=<span class="keyword">False</span>,</span><br><span class="line">    name=<span class="keyword">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure></p><p>其中<code>fn</code>是一个可调用函数，也可以用lambda表达式；<code>elems</code>是需要处理的列表；<code>initializer</code>是一个可选参数，可以作为<code>fn</code>的初始累加值（accumulated value）；至于<code>parallel_iterations</code>是并行数，其他的函数可以查询官网，就不累述了。<br>最重要的参数无非是<code>fn</code>,<code>elems</code>和<code>initializer</code>，其中<code>fn</code>是一个具有两个输入参数的函数，需要返回一个值作为累计结果，<code>elems</code>是一个列表或者张量，用于被<code>fn</code>累计处理。形象的来说，就是<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.foldl(fn,elems=[x1,x2,x3,x4]) = fn(fn(fn(x1,x2),x3),x4)</span><br></pre></td></tr></table></figure></p><p>如果给定了<code>initializer</code>，那么初始的累计参数（也就是<code>fn</code>的第一个参数）就是他了，如果没有给定，也即是<code>initializer=None</code>那么<code>elems</code>中必须至少有一个值，第一个值将会被作为初始值。例子：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">elems = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]</span><br><span class="line">sum = tf.foldl(<span class="keyword">lambda</span> a, x: a + x, elems)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  print(sess.run(sum))</span><br></pre></td></tr></table></figure></p><p>将会输出21，也即是(((((1+2)+3)+4)+5)+6)，如果给定了一个初始化值，就变为：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">elems = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]</span><br><span class="line">sum = tf.foldl(<span class="keyword">lambda</span> a, x: a + x, elems,initializer=<span class="number">10</span>)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  print(sess.run(sum))</span><br></pre></td></tr></table></figure></p><p>输出变为31。此处的累加是最简单的应用，还可以有更多复杂的应用，就看应用场景了。至于<code>tf.foldr()</code>和这个函数是基本上一样的，无非就是从右边开始计算到左边而已。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;font size=&quot;6&quot;&gt;&lt;b&gt;前言&lt;/b&gt;&lt;/font&gt;&lt;br&gt;在TensorFlow中有着若干高阶函数，如之前已经介绍过了的&lt;code&gt;tf.map_fn()&lt;/code&gt;，见博文&lt;a href=&quot;https://blog.csdn.net/LoseInVain/a
      
    
    </summary>
    
      <category term="TensorFlow Basic API" scheme="https://blog.csdn.net/LoseInVain/categories/TensorFlow-Basic-API/"/>
    
    
      <category term="Deep Learning" scheme="https://blog.csdn.net/LoseInVain/tags/Deep-Learning/"/>
    
      <category term="TensorFlow" scheme="https://blog.csdn.net/LoseInVain/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>tf.group()用于组合多个操作</title>
    <link href="https://blog.csdn.net/LoseInVain/2018/10/21/tensorflow/tf_group/"/>
    <id>https://blog.csdn.net/LoseInVain/2018/10/21/tensorflow/tf_group/</id>
    <published>2018-10-21T13:23:56.724Z</published>
    <updated>2018-10-21T13:24:25.326Z</updated>
    
    <content type="html"><![CDATA[<p><font size="6"><b>前言</b></font><br><code>tf.group()</code>用于创造一个操作，可以将传入参数的所有操作进行分组。<a href="https://www.tensorflow.org/api_docs/python/tf/group" target="_blank" rel="noopener">API手册</a>如:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tf.group(</span><br><span class="line">    *inputs,</span><br><span class="line">    **kwargs</span><br><span class="line">)</span><br></pre></td></tr></table></figure></p><p><code>ops = tf.group(tensor1, tensor2,...)</code><br>其中<code>*inputs</code>是0个或者多个用于组合tensor，一旦<code>ops</code>完成了，那么传入的<code>tensor1,tensor2,...</code>等等都会完成了，经常用于组合一些训练节点，如在Cycle GAN中的多个训练节点，例子如：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">generator_train_op = tf.train.AdamOptimizer(g_loss, ...)</span><br><span class="line">discriminator_train_op = tf.train.AdamOptimizer(d_loss,...)</span><br><span class="line">train_ops = tf.groups(generator_train_op ,discriminator_train_op)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  sess.run(train_ops) </span><br><span class="line">  <span class="comment"># 一旦运行了train_ops,那么里面的generator_train_op和discriminator_train_op都将被调用</span></span><br></pre></td></tr></table></figure></p><p><strong>注意的是，tf.group()返回的是个操作，而不是值，如果你想下面一样用，返回的将不是值</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">a = tf.Variable([<span class="number">5</span>])</span><br><span class="line">b = tf.Variable([<span class="number">6</span>])</span><br><span class="line">c = a+b</span><br><span class="line">d = a*b</span><br><span class="line">e = a/b</span><br><span class="line">ops = tf.group(c,d,e)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    ee = sess.run(ops)</span><br></pre></td></tr></table></figure></p><p>返回的将不是c,d,e的运算结果，而是一个<code>None</code>，就是因为这个是一个操作，而不是一个张量。如果需要返回结果，请参考<a href="https://www.tensorflow.org/api_docs/python/tf/tuple" target="_blank" rel="noopener"><code>tf.tuple()</code></a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;font size=&quot;6&quot;&gt;&lt;b&gt;前言&lt;/b&gt;&lt;/font&gt;&lt;br&gt;&lt;code&gt;tf.group()&lt;/code&gt;用于创造一个操作，可以将传入参数的所有操作进行分组。&lt;a href=&quot;https://www.tensorflow.org/api_docs/python/t
      
    
    </summary>
    
      <category term="TensorFlow Basic API" scheme="https://blog.csdn.net/LoseInVain/categories/TensorFlow-Basic-API/"/>
    
    
      <category term="Deep Learning" scheme="https://blog.csdn.net/LoseInVain/tags/Deep-Learning/"/>
    
      <category term="TensorFlow" scheme="https://blog.csdn.net/LoseInVain/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>tf.tuple()用于组合多个张量输入</title>
    <link href="https://blog.csdn.net/LoseInVain/2018/10/21/tensorflow/tf_tuple/"/>
    <id>https://blog.csdn.net/LoseInVain/2018/10/21/tensorflow/tf_tuple/</id>
    <published>2018-10-21T13:22:49.287Z</published>
    <updated>2018-10-21T13:23:39.206Z</updated>
    
    <content type="html"><![CDATA[<p><font size="6"><b>前言</b></font><br><code>tf.tuple()</code>用于组合多个张量输入组成的列表<code>[tensor1,tensor2,...]</code>，然后返回一个计算过后的张量列表<code>[cal_tensor1,cal_tensor2,...]</code>，这点和<code>tf.group()</code>是不同的，<a href="https://www.tensorflow.org/api_docs/python/tf/tuple" target="_blank" rel="noopener">API手册</a>如：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tf.tuple(</span><br><span class="line">    tensors,</span><br><span class="line">    name=<span class="keyword">None</span>,</span><br><span class="line">    control_inputs=<span class="keyword">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure></p><p><code>ops = tf.tuple([tensor1,tensor2,...],control_inputs=c_ops)</code><br>其中<code>tensors</code>是由多个<code>tensor</code>组成的列表，其中的<code>control_inputs</code>是添加额外的控制输入，添加的输入<code>c_ops</code>必须在整个<code>ops</code>完成之前得到执行，但是<code>c_ops</code>的输出是不会返回的。</p><p>API上描述，这个可以作为提供一种并行处理的机制，所有的输入的tensor可以并行计算，但是所有tensor的计算出来的值将会以<code>tuple</code>的形式返回，并且这个只能在并行计算完成之后得到。(<em>This can be used as a “join” mechanism for parallel computations: all the argument tensors can be computed in parallel, but the values of any tensor returned by tuple are only available after all the parallel computations are done.</em>)</p><p>使用例子：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">a = tf.Variable([<span class="number">5</span>])</span><br><span class="line">b = tf.Variable([<span class="number">6</span>])</span><br><span class="line">c = a+b</span><br><span class="line">d = a*b</span><br><span class="line">e = a/b</span><br><span class="line">ops = tf.tuple([c,d,e])</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    ee = sess.run(ops)</span><br><span class="line">    print(ee)</span><br></pre></td></tr></table></figure></p><p>输出<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[array([<span class="number">11</span>], dtype=int32), array([<span class="number">30</span>], dtype=int32), array([<span class="number">0.83333333</span>])]</span><br></pre></td></tr></table></figure></p><p>可以和<a href="https://blog.csdn.net/LoseInVain/article/details/81703786">tf.group()用于组合多个操作</a>的例子进行对比。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;font size=&quot;6&quot;&gt;&lt;b&gt;前言&lt;/b&gt;&lt;/font&gt;&lt;br&gt;&lt;code&gt;tf.tuple()&lt;/code&gt;用于组合多个张量输入组成的列表&lt;code&gt;[tensor1,tensor2,...]&lt;/code&gt;，然后返回一个计算过后的张量列表&lt;code&gt;[cal_ten
      
    
    </summary>
    
      <category term="TensorFlow Basic API" scheme="https://blog.csdn.net/LoseInVain/categories/TensorFlow-Basic-API/"/>
    
    
      <category term="Deep Learning" scheme="https://blog.csdn.net/LoseInVain/tags/Deep-Learning/"/>
    
      <category term="TensorFlow" scheme="https://blog.csdn.net/LoseInVain/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>pytorch中的L2和L1正则化，自定义优化器设置等操作</title>
    <link href="https://blog.csdn.net/LoseInVain/2018/10/21/pytorch/pytorch_reg/"/>
    <id>https://blog.csdn.net/LoseInVain/2018/10/21/pytorch/pytorch_reg/</id>
    <published>2018-10-21T13:19:27.311Z</published>
    <updated>2018-10-21T13:20:31.437Z</updated>
    
    <content type="html"><![CDATA[<p><font size="6"><b>前言</b></font><br>在pytorch中进行L2正则化，最直接的方式可以直接用优化器自带的<code>weight_decay</code>选项指定权值衰减率，相当于L2正则化中的$\lambda$，也就是：</p><script type="math/tex; mode=display">\mathcal{L}_{reg} = ||y-\hat{y}||^2+\lambda||W||^2\tag{1}</script><p>中的$\lambda$。但是有一个问题就是，这个指定的权值衰减是会对网络中的所有参数，包括权值$w$和偏置$b$同时进行的，很多时候如果对$b$进行L2正则化将会导致严重的欠拟合<sup><a href="#fn_1" id="reffn_1">1</a></sup>，因此这个时候一般只需要对权值进行正则即可，当然，你可以获取模型中的所有权值，然后按照定义的方法显式地进行处理，得到一个正则损失之后在交给优化器优化，这是一个通用的方法。但是其实还有更为简单的方法，同样在优化器中提供了。</p><hr><p><code>torch.optim</code>中包含了很多现成的优化器，包括SGD，Adadelta，Adam，Adagrad，RMSprop等，使用它很简单，你需要传入一个可迭代的参数列表（里面必须都是Variable类型的）进行优化，然后你可以指定一些优化器的参数，如学习率，动量，权值衰减等。例子如：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">optimizer = optim.SGD(model.parameters(), lr = <span class="number">0.01</span>, momentum=<span class="number">0.9</span>,weight_decay=<span class="number">1e-5</span>)</span><br><span class="line">optimizer = optim.Adam([var1, var2], lr = <span class="number">0.0001</span>)</span><br></pre></td></tr></table></figure></p><p>此外，优化器还支持一种称之为<strong>Per-parameter options</strong>的操作，就是对每一个参数进行特定的指定，以满足更为细致的要求。做法也很简单，与上面不同的，我们传入的待优化变量不是一个<code>Variable</code>而是一个可迭代的字典，字典中必须有<code>params</code>的key，用于指定待优化变量，而其他的key需要匹配优化器本身的参数设置。我们看一下例子：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">optim.SGD([</span><br><span class="line">                &#123;<span class="string">'params'</span>: model.base.parameters()&#125;,</span><br><span class="line">                &#123;<span class="string">'params'</span>: model.classifier.parameters(), <span class="string">'lr'</span>: <span class="number">1e-3</span>&#125;</span><br><span class="line">            ], lr=<span class="number">1e-2</span>, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure></p><p>其中，我们可以看到，传入的list中有两个字典，每一个都是一个独立的参数组，其中每一组中都有一个<code>params</code>key，用于指定需要训练的参数，如<code>model.base.parameters()</code>就是base网络中的所有参数，尔后，也可以在每一组内单独设置学习率，权值衰减等。如果不显式地在组内设定，那么就会继承优化器的全局参数，如<code>lr=1e-2</code>,<code>momentum=0.9</code>等，如果组内指定了，那么全局的将不会覆盖掉组内的参数设置。<br>这样我们就可以灵活的给每一个子网络设定不同的学习率，权值衰减，momentum了，我们也可以给权值设定权值衰减，而不作用与偏置，如：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">weight_p, bias_p = [],[]</span><br><span class="line"><span class="keyword">for</span> name, p <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">  <span class="keyword">if</span> <span class="string">'bias'</span> <span class="keyword">in</span> name:</span><br><span class="line">     bias_p += [p]</span><br><span class="line">   <span class="keyword">else</span>:</span><br><span class="line">     weight_p += [p]</span><br><span class="line"><span class="comment"># 这里的model中每个参数的名字都是系统自动命名的，只要是权值都是带有weight，偏置都带有bias，</span></span><br><span class="line"><span class="comment"># 因此可以通过名字判断属性，这个和tensorflow不同，tensorflow是可以用户自己定义名字的，当然也会系统自己定义。</span></span><br><span class="line">optim.SGC([</span><br><span class="line">          &#123;<span class="string">'params'</span>: weight_p, <span class="string">'weight_decay'</span>:<span class="number">1e-5</span>&#125;,</span><br><span class="line">          &#123;<span class="string">'params'</span>: bias_p, <span class="string">'weight_decay'</span>:<span class="number">0</span>&#125;</span><br><span class="line">          ], lr=<span class="number">1e-2</span>, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure></p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p>[1]. <a href="https://pytorch.org/docs/stable/optim.html" target="_blank" rel="noopener">PyTorch Documentation -&gt; torch.optim</a></p><blockquote id="fn_1"><sup>1</sup>. Goodfellow I, Bengio Y, Courville A, et al. Deep learning[M]. Cambridge: MIT press, 2016.<a href="#reffn_1" title="Jump back to footnote [1] in the text."> &#8617;</a></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;font size=&quot;6&quot;&gt;&lt;b&gt;前言&lt;/b&gt;&lt;/font&gt;&lt;br&gt;在pytorch中进行L2正则化，最直接的方式可以直接用优化器自带的&lt;code&gt;weight_decay&lt;/code&gt;选项指定权值衰减率，相当于L2正则化中的$\lambda$，也就是：&lt;/p&gt;
&lt;scr
      
    
    </summary>
    
      <category term="Pytorch Basic API" scheme="https://blog.csdn.net/LoseInVain/categories/Pytorch-Basic-API/"/>
    
    
      <category term="Deep Learning" scheme="https://blog.csdn.net/LoseInVain/tags/Deep-Learning/"/>
    
      <category term="Pytorch" scheme="https://blog.csdn.net/LoseInVain/tags/Pytorch/"/>
    
  </entry>
  
  <entry>
    <title>在TensorFlow中自定义梯度的两种方法</title>
    <link href="https://blog.csdn.net/LoseInVain/2018/10/21/tensorflow/tf_gradient_define/"/>
    <id>https://blog.csdn.net/LoseInVain/2018/10/21/tensorflow/tf_gradient_define/</id>
    <published>2018-10-21T12:53:51.927Z</published>
    <updated>2018-10-21T12:58:21.704Z</updated>
    
    <content type="html"><![CDATA[<p><font size="6"><b>前言</b></font><br>在深度学习中，有时候我们需要对某些节点的梯度进行一些定制，特别是该节点操作不可导（比如阶梯除法如$10 // 3 = 3$），如果实在需要对这个节点进行操作，而且希望其可以反向传播，那么就需要对其进行自定义反向传播时的梯度。在有些场景，如[2]中介绍到的<strong>梯度反转</strong>(gradient inverse)中，就必须在某层节点对反向传播的梯度进行反转，也就是需要更改正常的梯度传播过程，如下图的$-\lambda \dfrac{\partial L_d}{\partial \theta_f}$所示。<br><img src="/imgs/tensorflow/tf_gradient/inversenet.png" alt="inversenet"></p><p>在tensorflow中有若干可以实现定制梯度的方法，这里介绍两种。</p><hr><h1 id="1-重写梯度法"><a href="#1-重写梯度法" class="headerlink" title="1. 重写梯度法"></a>1. 重写梯度法</h1><p>重写梯度法指的是通过tensorflow自带的机制，将某个节点的梯度重写(override)，这种方法的适用性最广。我们这里举个例子[3].</p><p>符号函数的前向传播采用的是阶跃函数$y = \rm{sign}(x)$，如下图所示，我们知道阶跃函数不是连续可导的，因此我们在反向传播时，将其替代为一个可以连续求导的函数$y = \rm{Htanh(x)}$，于是梯度就是大于1和小于-1时为0，在-1和1之间时是1。<br><img src="/imgs/tensorflow/tf_gradient/ops.png" alt="ops"></p><p>使用重写梯度的方法如下，主要是涉及到<code>tf.RegisterGradient()</code>和<code>tf.get_default_graph().gradient_override_map()</code>，前者注册新的梯度，后者重写图中具有名字<code>name=&#39;Sign&#39;</code>的操作节点的梯度，用在新注册的<code>QuantizeGrad</code>替代。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用修饰器，建立梯度反向传播函数。其中op.input包含输入值、输出值，grad包含上层传来的梯度</span></span><br><span class="line"><span class="meta">@tf.RegisterGradient("QuantizeGrad")</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sign_grad</span><span class="params">(op, grad)</span>:</span></span><br><span class="line">    input = op.inputs[<span class="number">0</span>] <span class="comment"># 取出当前的输入</span></span><br><span class="line">    cond = (input&gt;=<span class="number">-1</span>)&amp;(input&lt;=<span class="number">1</span>) <span class="comment"># 大于1或者小于-1的值的位置</span></span><br><span class="line">    zeros = tf.zeros_like(grad) <span class="comment"># 定义出0矩阵用于掩膜</span></span><br><span class="line">    <span class="keyword">return</span> tf.where(cond, grad, zeros) </span><br><span class="line">    <span class="comment"># 将大于1或者小于-1的上一层的梯度置为0</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">#使用with上下文管理器覆盖原始的sign梯度函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">binary</span><span class="params">(input)</span>:</span></span><br><span class="line">    x = input</span><br><span class="line">    <span class="keyword">with</span> tf.get_default_graph().gradient_override_map(&#123;<span class="string">"Sign"</span>:<span class="string">'QuantizeGrad'</span>&#125;):</span><br><span class="line">    <span class="comment">#重写梯度</span></span><br><span class="line">        x = tf.sign(x)</span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"> </span><br><span class="line"><span class="comment">#使用</span></span><br><span class="line">x = binary(x)</span><br></pre></td></tr></table></figure><p>其中的<code>def sign_grad(op, grad):</code>是注册新的梯度的套路，其中的<code>op</code>是当前操作的输入值/张量等，而<code>grad</code>指的是从反向而言的上一层的梯度。</p><hr><p>通常来说，在tensorflow中自定义梯度，函数<code>tf.identity()</code>是很重要的，其API手册如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tf.identity(</span><br><span class="line">    input,</span><br><span class="line">    name=<span class="keyword">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure></p><p>其会返回一个形状和内容都和输入完全一样的输出，但是你可以自定义其反向传播时的梯度，因此在梯度反转等操作中特别有用。<br>这里再举个反向梯度[2]的例子，也就是梯度为$-\lambda \dfrac{\partial L_d}{\partial \theta_f}$而不是$\lambda \dfrac{\partial L_d}{\partial \theta_f}$。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">x1 = tf.Variable(<span class="number">1</span>)</span><br><span class="line">x2 = tf.Variable(<span class="number">3</span>)</span><br><span class="line">x3 = tf.Variable(<span class="number">6</span>)</span><br><span class="line"><span class="meta">@tf.RegisterGradient('CustomGrad')</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">CustomGrad</span><span class="params">(op, grad)</span>:</span></span><br><span class="line"><span class="comment">#     tf.Print(grad)</span></span><br><span class="line">    <span class="keyword">return</span> -grad</span><br><span class="line">    </span><br><span class="line">g = tf.get_default_graph()</span><br><span class="line">oo = x1+x2</span><br><span class="line"><span class="keyword">with</span> g.gradient_override_map(&#123;<span class="string">"Identity"</span>: <span class="string">"CustomGrad"</span>&#125;):</span><br><span class="line">    output = tf.identity(oo)</span><br><span class="line">grad_1 = tf.gradients(output, oo)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    print(sess.run(grad_1))</span><br></pre></td></tr></table></figure></p><p>因为<code>-grad</code>，所以这里的梯度输出是[-1]而不是[1]。有一个我们需要注意的是，在自定义函数<code>def CustomGrad()</code>中，返回的值得是一个张量，而不能返回一个参数，比如<code>return 0</code>，这样会报错，如：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">AttributeError: <span class="string">'int'</span> object has no attribute <span class="string">'name'</span></span><br></pre></td></tr></table></figure></p><p>显然，这是因为tensorflow的内部操作需要取返回值的名字而<code>int</code>类型没有名字。</p><p><strong>PS:</strong><code>def CustomGrad()</code>这个函数签名是随便你取的。</p><hr><h1 id="2-stop-gradient法"><a href="#2-stop-gradient法" class="headerlink" title="2. stop_gradient法"></a>2. stop_gradient法</h1><p>对于自定义梯度，还有一种比较简洁的操作，就是利用<code>tf.stop_gradient()</code>函数，我们看下例子[1]：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t = g(x)</span><br><span class="line">y = t + tf.stop_gradient(f(x) - t)</span><br></pre></td></tr></table></figure></p><p>这里，我们本来的前向传递函数是f(x)，但是想要在反向时传递的函数是g(x)，因为在前向过程中，<code>tf.stop_gradient()</code>不起作用，因此<code>+t</code>和<code>-t</code>抵消掉了，只剩下f(x)前向传递；而在反向过程中，因为<code>tf.stop_gradient()</code>的作用，使得f(x)-t的梯度变为了0，从而只剩下g(x)在反向传递。<br>我们看下完整的例子：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">x1 = tf.Variable(<span class="number">1</span>)</span><br><span class="line">x2 = tf.Variable(<span class="number">3</span>)</span><br><span class="line">x3 = tf.Variable(<span class="number">6</span>)</span><br><span class="line"></span><br><span class="line">f = x1+x2*x3</span><br><span class="line">t = -f</span><br><span class="line"></span><br><span class="line">y1 =  t + tf.stop_gradient(f-t)</span><br><span class="line">y2 = f</span><br><span class="line"></span><br><span class="line">grad_1 = tf.gradients(y1, x1)</span><br><span class="line">grad_2 = tf.gradients(y2, x1)</span><br><span class="line"><span class="keyword">with</span> tf.Session(config=config) <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line"></span><br><span class="line">    print(sess.run(grad_1))</span><br><span class="line">    print(sess.run(grad_2))</span><br></pre></td></tr></table></figure></p><p>第一个输出为[-1]，第二个输出为[1]，显然也实现了梯度的反转。</p><hr><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p>[1]. <a href="https://stackoverflow.com/questions/36456436/how-can-i-define-only-the-gradient-for-a-tensorflow-subgraph/36480182#36480182" target="_blank" rel="noopener">How Can I Define Only the Gradient for a Tensorflow Subgraph?</a><br>[2]. Ganin Y, Ustinova E, Ajakan H, et al. Domain-adversarial training of neural networks[J]. Journal of Machine Learning Research, 2017, 17(1):2096-2030.<br>[3]. <a href="https://blog.csdn.net/qq_16234613/article/details/82937867">tensorflow 实现自定义梯度反向传播</a><br>[4]. <a href="https://uoguelph-mlrg.github.io/tensorflow_gradients/" target="_blank" rel="noopener">Custom Gradients in TensorFlow</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;font size=&quot;6&quot;&gt;&lt;b&gt;前言&lt;/b&gt;&lt;/font&gt;&lt;br&gt;在深度学习中，有时候我们需要对某些节点的梯度进行一些定制，特别是该节点操作不可导（比如阶梯除法如$10 // 3 = 3$），如果实在需要对这个节点进行操作，而且希望其可以反向传播，那么就需要对其进行自定
      
    
    </summary>
    
      <category term="TensorFlow Basic API" scheme="https://blog.csdn.net/LoseInVain/categories/TensorFlow-Basic-API/"/>
    
    
      <category term="Deep Learning" scheme="https://blog.csdn.net/LoseInVain/tags/Deep-Learning/"/>
    
      <category term="TensorFlow" scheme="https://blog.csdn.net/LoseInVain/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>SVM沉思录6——支持向量机中的核技巧那些事儿</title>
    <link href="https://blog.csdn.net/LoseInVain/2018/10/21/svm/svm6/"/>
    <id>https://blog.csdn.net/LoseInVain/2018/10/21/svm/svm6/</id>
    <published>2018-10-21T04:16:07.946Z</published>
    <updated>2018-10-21T02:12:10.976Z</updated>
    
    <content type="html"><![CDATA[<font size="6"><b>前言</b></font> <p><strong>我们在前文[1-5]中介绍了线性支持向量机的原理和推导，涉及到了软和硬的线性支持向量机，还有相关的广义拉格朗日乘数法和KKT条件等。然而，光靠着前面介绍的这些内容，只能够对近似于线性可分的数据进行分割，而不能对非线性的数据进行处理，这里我们简单介绍下支持向量机中使用的核技巧，使用了核技巧的支持向量机就具备了分割非线性数据的能力。本篇可能是我们这个系列的最后一篇了，如果有机会我们在SMO中再会吧。</strong></p><p><strong>如有谬误，请联系指正。转载请注明出处。</strong></p><p><em>联系方式：</em><br><strong>e-mail</strong>: <code>FesianXu@163.com</code><br><strong>QQ</strong>: <code>973926198</code><br><strong>github</strong>: <code>https://github.com/FesianXu</code></p><hr><h1 id="重回SVM"><a href="#重回SVM" class="headerlink" title="重回SVM"></a>重回SVM</h1><p>我们在前文[1-5]中就线性SVM做了比较系统的介绍和推导，我们这里做个简单的小回顾。<strong>支持向量机</strong>(Support Vector Machine,SVM)，是一种基于最大间隔原则进行推导出来的线性分类器，如果引入松弛项，则可以处理近似线性可分的一些数据，其最终的对偶问题的数学表达形式为(1.1)，之所以用对偶形式求解是因为可以轻松地引入所谓的核技巧，我们后面将会看到这个便利性。</p><script type="math/tex; mode=display">\min_{\alpha}\frac{1}{2}\sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_jy_iy_j(x_i \cdot x_j)- \sum_{i=1}^N\alpha_i \\s.t. \ \sum_{i=1}^N\alpha_iy_i=0 \\\alpha_i \geq0,i=1,\cdots,N\tag{1.1}</script><p>其最终的分类超平面如(1.2):</p><script type="math/tex; mode=display">\theta(x) = \rm{sign}(\sum_{i=1}^N \alpha^*_iy_i(x_i \cdot x)+b^*)\tag{1.2}</script><p>从KKT条件[3]中我们知道，除了支持向量SV会影响到决策面之外，其他所有的样本都是不会对决策面产生影响的，因此只有支持向量对应的$\alpha_i^* &gt; 0$，其他所有的$\alpha_j^*$都是等于0的。也就是说，我们的支持向量机只需要记住某些决定性的样本就可以了。<strong>实际上，这种需要“记住样本”的方法，正是一类核方法(kernel method)。</strong>这个我们后面可能会独立一些文章进行讨论，这里我们记住，因为SVM只需要记忆很少的一部分样本信息，因此被称之为<strong>稀疏核方法</strong>(Sparse Kernel Method)[6]。</p><hr><h1 id="更进一步观察SVM"><a href="#更进一步观察SVM" class="headerlink" title="更进一步观察SVM"></a>更进一步观察SVM</h1><p>我们这里更进一步对SVM的对偶优化任务和决策面，也即是式子(1.1)(1.2)进行观察，我们会发现，有一个项是相同作用的，那就是$(x_i \cdot x_j)$和$(x_i \cdot x)$，这两项都是在<strong>度量两个样本之间的距离</strong>。我们会发现，因为点积操作</p><script type="math/tex; mode=display">x_i \cdot x_j = ||x_i|| \cdot ||x_j|| \cdot \cos(\theta)\tag{2.1}</script><p>在两个向量模长相同的情况下，可以知道这个点积的结果越大，两个样本之间的相似度越高，因此可以看作是一种样本之间的<strong>度量</strong>(metric)。这个我们可以理解，SVM作为一种稀疏核方法的之前就是一个核方法，是需要纪录训练样本的原始信息的。</p><p>但是，我们注意到，我们是在<strong>原始的样本特征空间进行对比这个相似度的</strong>，这个很关键，因为在原始的样本特征空间里面，<strong>样本不一定是线性可分的，如果在这个空间里面，线性SVM将没法达到很好的效果。</strong></p><hr><h1 id="开始我们的非线性之路"><a href="#开始我们的非线性之路" class="headerlink" title="开始我们的非线性之路"></a>开始我们的非线性之路</h1><p>那么，我们在回顾了之前的一些东西之后，我们便可以开始我们的非线性之路了，抓好扶手吧，我们要起飞了。</p><h2 id="高维映射"><a href="#高维映射" class="headerlink" title="高维映射"></a>高维映射</h2><p>对于非线性的数据，如下图所示，显然我们没法通过一个线性平面对其进行分割。<br><img src="/imgs/svm/raw_feature.png" alt="raw_feature"><br>当然，那仅仅是在二维的情况下我们没法对齐进行线性分割，谁说我们不能在更高的维度进行“维度打击”呢？！<strong>我们不妨把整个数据上升一个维度，投射到三维空间</strong>，我们将红色数据“拉高”，而绿色数据“留在原地”，那么我们就有了：<br><img src="/imgs/svm/projected_feature.png" alt="projected_feature"><br>发现没有，在二维线性不可分的数据，在三维空间就变得线性可分了。这个时候我们可以纪录下在三维情况下的决策面，然后在做个逆操作，将其投射到原先的二维空间中，那么我们就有了:<br><img src="/imgs/svm/recover.png" alt="recover"><br>看来这种维度打击还真是有效！</p><p>$\nabla$<strong>我们其实还可以再举个更为简单的例子。</strong>$\nabla$<br>假如我们现在有一些数据，满足$x_1^2+x_2^2=1$，是的，我们不难发现这其实就是个以原点为圆心半径为1的圆，其参数为$x_1$和$x_2$，但是显然的，这个是个非线性的关系，如果要转换成一个线性的关系要怎么操作呢？简单，用$x_3 = x_1^2$和$x_4 = x_2^2$，我们有变形等价式$x_3+x_4=1$，于是我们便有了关于$x_3$和$x_4$的线性关系式，其关键就是映射$\phi(x)=x^2$。</p><p>别小看这个例子哦，这个是我们核技巧的一个关键的直观想法哦。没晕吧？让我们继续吧。</p><h1 id="基函数"><a href="#基函数" class="headerlink" title="基函数"></a>基函数</h1><p>其实我们刚才举得例子中的$\phi(x) = x^2$就是一个<strong>基函数</strong>(basic function)，其作用很直接，就是将一个属于特征空间$\mathcal{M}$的样本$\mathbf{x} \in \mathcal{M}$映射到新的特征空间$\mathcal{N}$，使得有$\phi(\mathbf{x}) \in \mathcal{N}$。如果诸位看官熟悉深度学习，那么我们就会发现，其实<strong>深度学习中的激活函数无非也就是起着这种作用，将浅层的特征空间映射到深层的特征空间，使得其尽可能地容易区分。可以说，激活函数就是一种基函数。</strong></p><p>那么我们能不能把这种映射应用到，我们刚才的第二节提到的度量测试中的原始特征空间中的样本呢？答案自然是可以的，这样，我们就会有：</p><script type="math/tex; mode=display">(\phi(\mathbf{x}_i) \cdot \phi(\mathbf{x_j}))\tag{3.1}</script><p>通常为了后续讨论，我们会将式子(3.1)表示为(3.2):</p><script type="math/tex; mode=display">\mathcal{k}(\mathbf{x}_i, \mathbf{x}_j) = (\phi(\mathbf{x}_i) \cdot \phi(\mathbf{x_j})) = \phi(\mathbf{x}_i)^T\phi(\mathbf{x}_j)\tag{3.2}</script><p>好的，这样我们就将原始特征空间的样本映射到新的特征空间了，这个特征空间一般来说是更高维的线性可分的空间。我们将这里的$\mathcal{k}(\cdot, \cdot)$称之为<strong>核函数</strong>(kernels)，哦噢，我们的核函数正式出场了哦。</p><p>在给定了核函数的情况下，我们的对偶优化问题和决策面变成了：</p><script type="math/tex; mode=display">\min_{\alpha}\frac{1}{2}\sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_jy_iy_j \mathcal{k}(x_i \cdot x_j)- \sum_{i=1}^N\alpha_i \\s.t. \ \sum_{i=1}^N\alpha_iy_i=0 \\\alpha_i \geq0,i=1,\cdots,N\tag{3.3 对偶问题}</script><script type="math/tex; mode=display">\theta(x) = \rm{sign}(\sum_{i=1}^N \alpha^*_iy_i \mathcal{k}(x_i \cdot x)+b^*)\tag{3.4 决策面}</script><p>但是，实际上我们是人工很难找到这个合适的映射$\phi(\cdot)$的，特别是在数据复杂，而不是像例子那样的时候，那么我们该怎么办呢？我们能不能直接给定一个核函数$\mathcal{k}(\cdot, \cdot)$，然后就不用理会具体的基函数了呢？这样就可以隐式地在特征空间进行特征学习，而不需要显式地指定特征空间和基函数$\phi(\cdot)$[9]。答案是可以的！</p><p>我们给定一个<strong>Mercer定理</strong>[10]：</p><blockquote><p>如果函数$\mathcal{k}(\cdot, \cdot)$是$\mathbb{R}^n \times \mathbb{R}^n \rightarrow \mathbb{R}$上的映射（也就是从两个n维向量映射到实数域，既是进行样本度量计算）。那么如果$\mathcal{k}(\cdot, \cdot)$是一个有效核函数（也称为Mercer核函数），那么当且仅当对于训练样例$[x^{(1)}, x^{(2)}, \cdots, x^{(m)}]$，其相应的核函数矩阵是对称半正定(positive semidefinite)的，并且有$\mathcal{k}(x,y) = \mathcal{k}(y,x)$。</p></blockquote><p>嗯，定理很长，人生很短，这个定理说人话就是，如果这个核函数$\mathcal{k}(\cdot, \cdot)$是一个对称半正定的，并且其是个对称函数（度量的基本条件），那么这个核函数就肯定对应了某个样本与样本之间的度量，其关系正如(3.2)所示，因此隐式地定义出了样本的映射函数$\phi(\cdot)$，因此是个有效的核函数。</p><p>诶，但是对称半正定不是矩阵才能判断吗？这里的核函数是个函数耶？嗯…也不尽然，休息下，我们下一节继续吧。</p><h2 id="无限维向量与希尔伯特空间"><a href="#无限维向量与希尔伯特空间" class="headerlink" title="无限维向量与希尔伯特空间"></a>无限维向量与希尔伯特空间</h2><p>先暂时忘记之前的东西吧，清清脑袋，轻装上阵。我们在以前学习过得向量和矩阵都是有限维度的，那么是否存在<strong>无限维</strong>的向量和矩阵呢？其实，<strong>函数</strong>正是可以看成<strong>无限维的向量</strong>，想法其实很简单，假如有一个数值函数$f:x \rightarrow y$，假设其定义域是整个实数，如果对应每一个输入，都输出一个输出值，我们可以把所有输出值排列起来，也就形成了一个无限维的向量，表达为${y}^{\infty}_i$。</p><p>而核函数$\mathcal{k}(\mathbf{x}_i, \mathbf{x}_j)$作为一个双变量函数，就可以看成一个行列都是无限维的矩阵了。这样我们就可以定义其正定性了：</p><script type="math/tex; mode=display">\int\int f(\mathbf{x})\mathcal{k}(\mathbf{x}, \mathbf{y})f(\mathbf{y}) \rm{d} \mathbf{x} \rm{d} \mathbf{y} \geq 0\tag{3.5}</script><p>既然是个矩阵，那么我们就可以对其进行特征分解对吧，只不过因为是无限维，我们需要使用积分，表达式类似于矩阵的特征值分解：</p><script type="math/tex; mode=display">\int \mathcal{k}(\mathbf{x}, \mathbf{y}) \Phi(\mathbf{x}) \rm{d} \mathbf{x} = \lambda \Phi(\mathbf{y})\tag{3.6}</script><p>这里的特征就不是<strong>特征向量</strong>了，而是<strong>特征函数</strong>（看成无限维向量也可以的）。对于不同的特征值$\lambda_1$和$\lambda_2$，和对应的特征函数$\Phi_1(\mathbf{x})$和$\Phi_2(\mathbf{x})$，有：</p><script type="math/tex; mode=display">\begin{aligned}\int \mathcal{k}(\mathbf{x}, \mathbf{y}) \Phi_1(\mathbf{x}) \Phi_2(\mathbf{x}) \rm{d} \mathbf{x} &= \int \mathcal{k}(\mathbf{x}, \mathbf{y}) \Phi_2(\mathbf{x}) \Phi_1(\mathbf{x}) \rm{d} \mathbf{x} \\\rightarrow \int \lambda_1 \Phi_1(\mathbf{x}) \Phi_2(\mathbf{x}) \rm{d} \mathbf{x} &= \int \lambda_2 \Phi_2(\mathbf{x}) \Phi_1(\mathbf{x}) \rm{d} \mathbf{x}\end{aligned}\tag{3.7}</script><p>因为特征值不为0，因此由(3.7)我们有:</p><script type="math/tex; mode=display">< \Phi_1,  \Phi_2 > = \int \Phi_1(\mathbf{x}) \Phi_2(\mathbf{x}) \rm{d} \mathbf{x} = 0\tag{3.8}</script><p>也就是任意两个特征函数之间是<strong>正交</strong>(Orthogonal)的，一个核函数对应着无限个特征值${\lambda_i}_{i=1}^{\infty}$和无限个特征函数${\Phi_i}_{i=1}^{\infty}$，这个正是原先函数空间的一组正交基。</p><p>回想到我们以前学习到的矩阵分解，我们知道我们的矩阵$A$可以表示为：</p><script type="math/tex; mode=display">A = Q\Lambda Q^T\tag{3.9}</script><p>其中$Q$是$A$的特征向量组成的正交矩阵，$\Lambda$是对角矩阵。特征值$\Lambda_{i,i}$对应的特征向量是矩阵$Q$的第$i$列。我们看到在有限维空间中可以将矩阵表示为特征向量和特征值的组合表达。同样的，在无限维空间中，也可以定义这种分解，因此可以将核函数$\mathcal{k}(\cdot,\cdot)$表示为:</p><script type="math/tex; mode=display">\mathcal{k}(\mathbf{x}, \mathbf{y}) = \sum_{i=0}^{\infty} \lambda_i \Phi_i(\mathbf{x}) \Phi_i(\mathbf{y})\tag{3.10}</script><p>重新整理下，将${\sqrt{\lambda_i}\Phi_i}_{i=1}^{\infty}$作为一组正交基，构建出一个空间$\mathcal{H}$。不难发现，这个空间是无限维的，如果再深入探讨，还会发现他是完备的内积空间，因此被称之为<strong>希尔伯特空间</strong>(Hilbert space)[13]。别被名字给唬住了，其实就是将欧几里德空间的性质延伸到了无限维而已。</p><p>回到我们的希尔伯特空间，我们会发现，这个空间中的任意一个函数（向量）都可以由正交基进行线性表出：</p><script type="math/tex; mode=display">f = \sum_{i=1}^{\infty} f_i \sqrt{\lambda_i} \Phi_i\tag{3.11}</script><p>所以$f$可以表示为空间$\mathcal{H}$中的一个无限维向量：</p><script type="math/tex; mode=display">f = (f_1, f_2, \cdots,)^T_{\mathcal{H}}\tag{3.12}</script><h2 id="再生性-Reproduce"><a href="#再生性-Reproduce" class="headerlink" title="再生性(Reproduce)"></a>再生性(Reproduce)</h2><p>前面3.3讨论了很多关于函数在希尔伯特空间上的表出形式，我们这里在仔细观察下核函数。我们发现，其实核函数可以拆分为：</p><script type="math/tex; mode=display">\mathcal{k}(\mathbf{x}, \mathbf{y}) = \sum_{i=0}^{\infty}\lambda_i\Phi_i(\mathbf{x})\Phi_i(\mathbf{y}) = < \mathcal{k}(\mathbf{x},\cdot), \mathcal{k}(\mathbf{y}, \cdot) >_{\mathcal{H}}\tag{3.13}</script><p>其中:</p><script type="math/tex; mode=display">\mathcal{k}(\mathbf{x}, \cdot) = (\sqrt{\lambda_1}\Phi_1(\mathbf{x}),\sqrt{\lambda_2}\Phi_2(\mathbf{x}),\cdots)^T_{\mathcal{H}} \\\mathcal{k}(\mathbf{y}, \cdot) = (\sqrt{\lambda_1}\Phi_1(\mathbf{y}),\sqrt{\lambda_2}\Phi_2(\mathbf{y}),\cdots)^T_{\mathcal{H}}\tag{3.14}</script><p>发现没有，(3.13)将核函数表示为了两个函数的内积，是不是很想我们的式子(3.2)了呢。我们把这种可以用核函数来再生出两个函数的内积的这种性质称之为<strong>再生性</strong>(reproduce)，对应的希尔伯特空间称之为<strong>再生核希尔伯特空间</strong>(Reproducing Kernel Hilbert Space,RKHS)，有点吓人的名词，但是如果你能理解刚才的分解，这个其实还是蛮直接的。</p><p>我们更进一步吧，如果定义一个映射$\phi(\cdot)$:</p><script type="math/tex; mode=display">\phi(\mathbf{x}) = (\sqrt{\lambda_1}\Phi_1(\mathbf{x}),\sqrt{\lambda_2}\Phi_2(\mathbf{x}),\cdots)^T\tag{3.15}</script><p>当然这是个无限维的向量。这个映射将样本点$\mathbf{x} \in \mathbb{R}^n$投射到无限维的特征空间$\mathcal{H}$中，我们有：</p><script type="math/tex; mode=display">< \phi(\mathbf{x}), \phi(\mathbf{y}) > = \mathcal{k}(\mathbf{x}, \mathbf{y}) = \phi(\mathbf{x})^T\phi(\mathbf{y})\tag{3.16}</script><p>因此，我们解决了3.2中提出的问题，我们根本就不需要知道具体的映射函数$\phi$是什么形式的，特征空间在哪里（我们甚至可以投射到无限维特征空间，比如我们接下来要讲到的高斯核函数），只要是一个对称半正定的核函数$K$，那么就必然存在映射$\phi$和特征空间$\mathcal{H}$，使得式子(3.16)成立。</p><p>这就是所谓的<strong>核技巧</strong>(Kernel trick)[12]。</p><p><strong>PS:</strong> 为了理解为什么是从原始的有限维的特征空间映射到无限维的希尔伯特空间，我们从式子(3.15)其实不难发现，$\mathbf{x} \in \mathbb{R}^n$，$\sqrt{\lambda_i}\Phi_i(\mathbf{x}) \in \mathbb{R}$，而我们的$i \rightarrow \infty$，因此可以看成映射成了无限维的特征。</p><h2 id="高斯核函数的无限维映射性质"><a href="#高斯核函数的无限维映射性质" class="headerlink" title="高斯核函数的无限维映射性质"></a>高斯核函数的无限维映射性质</h2><p>有效的核函数，也就是对称半正定的核函数有很多，而且有一定的性质可以扩展组合这些核函数[6]，这一块内容比较多，我们以后独立一篇文章继续讨论。这里我们主要看下使用最多的核函数，<strong>高斯核函数</strong>，也经常称之为<strong>径向基函数</strong>。</p><p>高斯核函数的数学表达形式如下所示：</p><script type="math/tex; mode=display">\mathcal{k}(\mathbf{x}, \mathbf{y}) = \exp(-||\mathbf{x}-\mathbf{y}||^2/2\sigma^2)\tag{3.17}</script><p>我们现在对(3.17)进行变形(这里为了方便假设$\mathbf{x},\mathbf{y}$是一维的)：</p><script type="math/tex; mode=display">\begin{aligned}\exp(-||x-y||^2/2\sigma^2) &= \exp(-\lambda||x-y||^2) \\&= \exp(-\lambda x^2+2\lambda xy-\lambda y^2) \\&= \exp(-\lambda x^2) \exp(-\lambda y^2) \exp(2\lambda xy)\end{aligned}\tag{3.18}</script><p>利用泰勒展开[14]对式子(3.18)中的$\exp(2\lambda xy)$进行展开，有:</p><script type="math/tex; mode=display">\begin{aligned}\exp(2\lambda xy) &= \sum_{i=1}^{\infty} \dfrac{(2\lambda xy)^i}{i!} \\&= \sum_{i=1}^{\infty} \sqrt{\dfrac{2^i \lambda}{i!}}x \cdot \sqrt{\dfrac{2^i \lambda}{i!}}y\end{aligned}\tag{3.19}</script><p>现在结合(3.18)和(3.19)，我们有：</p><script type="math/tex; mode=display">\begin{aligned}\exp(-||x-y||^2&/2\sigma^2) \\&= \sum_{i=1}^{\infty} \sqrt{\dfrac{2^i \lambda}{i!}} \exp{(-\lambda x^2)}x \cdot \sqrt{\dfrac{2^i \lambda}{i!}} \exp{(-\lambda y^2)}y\end{aligned}\tag{3.20}</script><p>用序列$\mathbf{x} = {\sqrt{\dfrac{2^1 \lambda}{1!}} \exp{(-\lambda x^2)}x, \sqrt{\dfrac{2^2 \lambda}{2!}} \exp{(-\lambda x^2)}x, \cdots}$, $\mathbf{y}={\sqrt{\dfrac{2^1 \lambda}{1!}} \exp{(-\lambda y^2)}y, \sqrt{\dfrac{2^2 \lambda}{2!}} \exp{(-\lambda y^2)}y,\cdots}$<br>这两个都是无限维向量，也即是一个映射函数$\phi(\cdot)$。于是式子(3.20)可以改写为:</p><script type="math/tex; mode=display">\exp(-||x-y||^2/2\sigma^2) = \mathbf{x}^T \mathbf{y} = \phi(\mathbf{x})^T \phi(\mathbf{y})\tag{3.21}</script><p>看，我们常用的高斯核函数正是一个无限维映射的核函数。</p><hr><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>我们前面对再生核希尔伯特空间进行了简单的介绍，同时了解了无限维映射的核函数，高斯核函数，事实上，我们原始的SVM对偶问题推导中的$(x_i \cdot x_j)$也可以看成一种核函数，只不过这是个线性核函数而已，映射到了原始的特征空间，有：</p><script type="math/tex; mode=display">\mathcal{k}(\mathbf{x}, \mathbf{y}) = \mathbf{x}^T \mathbf{y}\tag{4.1}</script><p>在后续的文章中，我们将会介绍更多的核函数，如多项式核函数对数sigmoid核函数等，同时在后续的文章中，我们也将继续探讨关于基函数的一些应用。</p><hr><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p>[1]. <a href="https://blog.csdn.net/LoseInVain/article/details/78636176">《SVM笔记系列之一》什么是支持向量机SVM</a><br>[2]. <a href="https://blog.csdn.net/LoseInVain/article/details/78636285">《SVM笔记系列之二》SVM的拉格朗日函数表示以及其对偶问题</a><br>[3]. <a href="https://blog.csdn.net/LoseInVain/article/details/78624888">《SVM笔记系列之三》拉格朗日乘数法和KKT条件的直观解释</a><br>[4]. <a href="https://blog.csdn.net/LoseInVain/article/details/78636341">《SVM笔记系列之四》最优化问题的对偶问题</a><br>[5]. <a href="https://blog.csdn.net/LoseInVain/article/details/78646479">《SVM笔记系列之五》软间隔线性支持向量机</a><br>[6]. Bishop C M. Pattern recognition and machine learning (information science and statistics) springer-verlag new york[J]. Inc. Secaucus, NJ, USA, 2006.<br>[7]. Zhang T. An introduction to support vector machines and other kernel-based learning methods[J]. AI Magazine, 2001, 22(2): 103.<br>[8]. <a href="http://www.eric-kim.net/eric-kim-net/posts/1/kernel_trick.html" target="_blank" rel="noopener">Everything You Wanted to Know about the Kernel Trick</a><br>[9]. 李航. 统计学习方法[J]. 2012.<br>[10]. <a href="https://www.cnblogs.com/jerrylead/archive/2011/03/18/1988406.html" target="_blank" rel="noopener">核函数（Kernels） </a><br>[11]. <a href="http://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html" target="_blank" rel="noopener">机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用</a><br>[12]. <a href="http://iera.name/a-story-of-basis-and-kernel-part-ii-reproducing-kernel-hilbert-space/" target="_blank" rel="noopener">A Story of Basis and Kernel – Part II: Reproducing Kernel Hilbert Space</a><br>[13]. <a href="https://en.wikipedia.org/wiki/Hilbert_space" target="_blank" rel="noopener">Hilbert space</a><br>[14]. <a href="https://blog.csdn.net/qq_38906523/article/details/79851654">函数的泰勒(Taylor)展开式</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;font size=&quot;6&quot;&gt;&lt;b&gt;前言&lt;/b&gt;&lt;/font&gt; 

&lt;p&gt;&lt;strong&gt;我们在前文[1-5]中介绍了线性支持向量机的原理和推导，涉及到了软和硬的线性支持向量机，还有相关的广义拉格朗日乘数法和KKT条件等。然而，光靠着前面介绍的这些内容，只能够对近似于线性可分的数
      
    
    </summary>
    
      <category term="Machine Learning" scheme="https://blog.csdn.net/LoseInVain/categories/Machine-Learning/"/>
    
    
      <category term="Machine Learning" scheme="https://blog.csdn.net/LoseInVain/tags/Machine-Learning/"/>
    
      <category term="SVM" scheme="https://blog.csdn.net/LoseInVain/tags/SVM/"/>
    
      <category term="Kernel Trick" scheme="https://blog.csdn.net/LoseInVain/tags/Kernel-Trick/"/>
    
  </entry>
  
  <entry>
    <title>SVM沉思录5——软间隔线性支持向量机</title>
    <link href="https://blog.csdn.net/LoseInVain/2018/10/21/svm/svm5/"/>
    <id>https://blog.csdn.net/LoseInVain/2018/10/21/svm/svm5/</id>
    <published>2018-10-21T04:16:07.945Z</published>
    <updated>2018-10-21T02:04:36.067Z</updated>
    
    <content type="html"><![CDATA[<p><font size="6"><b>前言</b></font><br><strong>在以前的文章中，我们介绍了支持向量机的基本表达式，那是基于硬间隔线性支持向量机的，即是假设数据是完全线性可分的，在数据是近似线性可分的时候，我们不能继续使用硬间隔SVM了，而是需要采用软间隔SVM，在这里我们简单介绍下软间隔线性支持向量机。本人无专业的数学学习背景，只能在直观的角度上解释这个问题，如果有数学专业的朋友，还望不吝赐教。</strong><br><strong>如有误，请联系指正。转载请注明出处。</strong><br><em>联系方式：</em><br><strong>e-mail</strong>: <a href="FesianXu@163.com"><code>FesianXu@163.com</code></a><br><strong>QQ</strong>: <code>973926198</code><br><strong>github</strong>: <a href="https://github.com/FesianXu" target="_blank" rel="noopener"><code>https://github.com/FesianXu</code></a><br><strong>有关代码开源</strong>: <a href="https://github.com/FesianXu/AI_Blog/tree/master/SVM相关" target="_blank" rel="noopener">click</a></p><hr><h1 id="软间隔最大化"><a href="#软间隔最大化" class="headerlink" title="软间隔最大化"></a>软间隔最大化</h1><p>在文章<a href="http://blog.csdn.net/LoseInVain/article/details/78636285">《SVM的拉格朗日函数表示以及其对偶问题》</a>和<a href="http://blog.csdn.net/LoseInVain/article/details/78636176">《SVM支持向量机的目的和起源》</a>中，我们推导了SVM的基本公式，那时的基本假设之一就是<strong>数据是完全线性可分的</strong>，即是总是存在一个超平面$W^TX+b$可以将数据完美的分开，但是正如我们在<a href="http://blog.csdn.net/LoseInVain/article/details/78636285">《SVM的拉格朗日函数表示以及其对偶问题》</a>中最后结尾所说的：</p><blockquote><p>但是，在现实生活中的数据往往是或本身就是非线性可分但是近似线性可分的，或是线性可分但是具有噪声的，以上两种情况都会导致在现实应用中，硬间隔线性支持向量机变得不再实用</p></blockquote><p>因此，我们引入了<strong>软间隔线性支持向量机</strong>这个概念，<strong>硬间隔</strong>和<strong>软间隔</strong>的区别如下图所示：<br><img src="/imgs/svm/hard_margin_svm.png" alt="hard_margin"></p><p><img src="/imgs/svm/soft_margin_svm.png" alt="soft_margin"></p><p>我们的解决方案很简单，就是在软间隔SVM中，我们的分类超平面既要<strong>能够尽可能地将数据类别分对，又要使得支持向量到超平面的间隔尽可能地大</strong>。具体来说，因为线性不可分意味着某些样本点不能满足函数间隔大于等于1的条件，即是$\exists i, 1-y_i(W^Tx_i+b) &gt; 0$。解决方案就是通过对每一个样本点$(x_i, y_i)$引入一个松弛变量$\xi_i \geq 0$，对于那些不满足约束条件的样本点，使得函数间隔加上松弛变量之后大于等于1，于是我们的约束条件就变为了：</p><script type="math/tex; mode=display">y_i(W^T x_i+b)+\xi_i \geq 1 \\= y_i(W^T x_i+b) \geq 1-\xi_i\tag{1.1}</script><p>图像表示如：<br><img src="/imgs/svm/soft_margin_xi.png" alt="soft_margin_xi"></p><p>超平面两侧对称的虚线为支持向量，支持向量到超平面的间隔为1。<strong>在硬间隔SVM中本应该是在虚线内侧没有任何的样本点的，而在软间隔SVM中，因为不是完全的线性可分，所以虚线内侧存在有样本点，通过向每一个在虚线内侧的样本点添加松弛变量$\xi_i$，将这些样本点搬移到支持向量虚线上。而本身就是在虚线外的样本点的松弛变量则可以设为0。</strong><br>于是，给每一个松弛变量赋予一个代价$\xi_i$，我们的目标函数就变成了：</p><script type="math/tex; mode=display">f(W, \xi) = \frac{1}{2} \Vert W \Vert ^2+C \sum_{i=1}^N\xi_i \\i = 1,2, \cdots,N\tag{1.2}</script><p>其中$C &gt; 0$称为<strong>惩罚参数</strong>，C值大的时候对误分类的惩罚增大，C值小的时候对误分类的惩罚减小，$(1.2)$有两层含义：使得$\frac{1}{2} \Vert W \Vert^2$尽量小即是间隔尽可能大，同时使得误分类的数量尽量小，C是调和两者的系数，是一个超参数。<br>于是我们的软间隔SVM的问题可以描述为：</p><script type="math/tex; mode=display">\min_{W,b,\xi} \frac{1}{2} \Vert W \Vert^2+C \sum_{i=1}^N\xi_i \\s.t.　　y_i(W^T x_i+b) \geq 1-\xi_i \\\xi_i \geq 0 \\i = 1,2, \cdots, N\tag{1.3}</script><p>表述为标准形式：</p><script type="math/tex; mode=display">\min_{W,b,\xi} \frac{1}{2} \Vert W \Vert ^2+C \sum_{i=1}^N\xi_i \\s.t.　　1-\xi_i-y_i(W^T x_i+b) \leq 0 \\-\xi_i \leq 0 \\i = 1,2, \cdots, N\tag{1.4}</script><h1 id="软间隔SVM的拉格朗日函数表述和对偶问题"><a href="#软间隔SVM的拉格朗日函数表述和对偶问题" class="headerlink" title="软间隔SVM的拉格朗日函数表述和对偶问题"></a>软间隔SVM的拉格朗日函数表述和对偶问题</h1><p>我们采用<a href="http://blog.csdn.net/LoseInVain/article/details/78636285">《SVM的拉格朗日函数表示以及其对偶问题》</a>中介绍过的相似的方法，将$(1.4)$得到其对偶问题。过程如下：<br>将$(1.4)$转换为其拉格朗日函数形式：</p><script type="math/tex; mode=display">L(W, b, \xi, \alpha, \beta) = \frac{1}{2} \Vert W \Vert^2+C \sum_{i=1}^N\xi_i+\sum_{i=1}^N \alpha_i(1-\xi_i-y_i(W^T x_i+b))- \sum_{i=1}^N \beta_i \xi_i \\= \frac{1}{2} \Vert W \Vert^2+C \sum_{i=1}^N\xi_i+ \sum_{i=1}^N \alpha_i - \sum_{i=1}^N \alpha_i \xi_i - \sum_{i=1}^N \alpha_i y_i(W^T x_i+b) - \sum_{i=1}^N \beta_i \xi_i \\\tag{2.1}</script><p>原问题可以表述为（具体移步<a href="http://blog.csdn.net/loseinvain/article/details/78636341">《最优化问题的对偶问题》</a>）:</p><script type="math/tex; mode=display">\min_{W,b,\xi} \max_{\alpha, \beta} L(W, b, \xi, \alpha, \beta)\tag{2.2}</script><p>得到其对偶问题为：</p><script type="math/tex; mode=display">\max_{\alpha, \beta} \min_{W, b, \xi} L(W, b, \xi, \alpha, \beta)\tag{2.3}</script><p>我们先求对偶问题$\theta_D(\alpha, \beta) = \min_{W, b, \xi} L(W, b, \xi, \alpha, \beta)$，根据KKT条件(具体移步<a href="http://blog.csdn.net/loseinvain/article/details/78624888">《拉格朗日乘数法和KKT条件的直观解释》</a>)，我们有：</p><script type="math/tex; mode=display">\nabla_{W} L(W, b, \xi, \alpha, \beta) = W-\sum_{i=1}^N\alpha_i y_i x_i = 0\tag{2.4}</script><script type="math/tex; mode=display">\nabla_{b} L(W, b, \xi, \alpha, \beta) = \sum_{i=1}^N \alpha_i y_i = 0\tag{2.5}</script><script type="math/tex; mode=display">\nabla_{\xi_i} L(W, b, \xi, \alpha, \beta) = C-\alpha_i-\beta_i = 0\tag{2.6}</script><script type="math/tex; mode=display">\alpha_i \geq 0, \beta_i \geq 0\tag{2.7}</script><p>整理得到：</p><script type="math/tex; mode=display">W = \sum_{i=1}^N\alpha_i y_i x_i \\\sum_{i=1}^N \alpha_i y_i = 0 \\C = \alpha_i+\beta_i\tag{2.8}</script><p>将$(2.8)$代入$(2.1)$，有：</p><script type="math/tex; mode=display">\begin{aligned}L(W, b, \xi, \alpha, \beta) &= \frac{1}{2} \sum_{i=1}^N\alpha_i y_i x_i \sum_{j=1}^N\alpha_j y_j x_j \\&+(\alpha_i+\beta_i)\sum_{i=1}^N \xi_i+\sum_{i=1}^N \alpha_i - \sum_{i=1}^N \alpha_i \xi_i - \sum_{i=1}^N \beta_i \xi_i - \\&\sum_{i=1}^N \alpha_iy_i(\sum_{j=1}^N\alpha_j y_j x_j \cdot x_i +b) \\&= -\frac{1}{2} \sum_{i=1}^N\sum_{j=1}^N \alpha_i \alpha_j y_i y_j (x_i \cdot x_j)+\sum_{i=1}^N \alpha_i\end{aligned}\tag{2.9}</script><p>所以问题变为：</p><script type="math/tex; mode=display">\max_{\alpha, \beta} \theta_D(\alpha, \beta) = \max_{\alpha} -\frac{1}{2} \sum_{i=1}^N\sum_{j=1}^N \alpha_i \alpha_j y_i y_j (x_i \cdot x_j)+\sum_{i=1}^N \alpha_i\tag{2.10}</script><p>表述为最小化问题：</p><script type="math/tex; mode=display">\min_{\alpha} \frac{1}{2} \sum_{i=1}^N\sum_{j=1}^N \alpha_i \alpha_j y_i y_j (x_i \cdot x_j)-\sum_{i=1}^N \alpha_i \\s.t.　　　\sum_{i=1}^N \alpha_i y_i = 0 \\\alpha_i \geq 0 \\\beta_i \geq 0 \\C = \alpha_i+\beta_i\tag{2.11}</script><p>通过将$\beta_i = C-\alpha_i$，$(2.11)$可以化为：</p><script type="math/tex; mode=display">\min_{\alpha} \frac{1}{2} \sum_{i=1}^N\sum_{j=1}^N \alpha_i \alpha_j y_i y_j (x_i \cdot x_j)-\sum_{i=1}^N \alpha_i \\s.t.　　　\sum_{i=1}^N \alpha_i y_i = 0 \\0 \leq \alpha_i \leq C\tag{2.12}</script><p>对比文章<a href="http://blog.csdn.net/LoseInVain/article/details/78636285">《SVM的拉格朗日函数表示以及其对偶问题》</a>中的硬间隔SVM的最终的表达式：</p><script type="math/tex; mode=display">\min_{\alpha}\frac{1}{2}\sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_jy_iy_j(x_i \cdot x_j)- \sum_{i=1}^N\alpha_i</script><script type="math/tex; mode=display">s.t. \ \sum_{i=1}^N\alpha_iy_i=0</script><script type="math/tex; mode=display">\alpha_i \geq0,i=1,\cdots,N\tag{2.13}</script><p>不难发现软间隔SVM只是在对拉格朗日乘子$\alpha_i$的约束上加上了一个上界$C$。<br>我们以后都会利用$(2.12)$求解，接下来我们在<strong>SMO算法</strong>中，也将对式子$(2.12)$进行求解。</p><h1 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h1><ol><li><a href="http://blog.csdn.net/LoseInVain/article/details/78636285">《SVM的拉格朗日函数表示以及其对偶问题》 CSDN</a></li><li><a href="http://blog.csdn.net/LoseInVain/article/details/78636176">《SVM支持向量机的目的和起源》 CSDN</a></li><li><a href="http://blog.csdn.net/loseinvain/article/details/78636341">《最优化问题的对偶问题》 CSDN</a></li><li><a href="http://blog.csdn.net/loseinvain/article/details/78624888">《拉格朗日乘数法和KKT条件的直观解释》 CSDN</a></li><li><a href="https://book.douban.com/subject/10590856/" target="_blank" rel="noopener">《统计学习方法》 豆瓣</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;font size=&quot;6&quot;&gt;&lt;b&gt;前言&lt;/b&gt;&lt;/font&gt;&lt;br&gt;&lt;strong&gt;在以前的文章中，我们介绍了支持向量机的基本表达式，那是基于硬间隔线性支持向量机的，即是假设数据是完全线性可分的，在数据是近似线性可分的时候，我们不能继续使用硬间隔SVM了，而是需要采用软间
      
    
    </summary>
    
      <category term="Machine Learning" scheme="https://blog.csdn.net/LoseInVain/categories/Machine-Learning/"/>
    
    
      <category term="Machine Learning" scheme="https://blog.csdn.net/LoseInVain/tags/Machine-Learning/"/>
    
      <category term="SVM" scheme="https://blog.csdn.net/LoseInVain/tags/SVM/"/>
    
  </entry>
  
  <entry>
    <title>SVM沉思录4——最优化问题的对偶问题</title>
    <link href="https://blog.csdn.net/LoseInVain/2018/10/21/svm/svm4/"/>
    <id>https://blog.csdn.net/LoseInVain/2018/10/21/svm/svm4/</id>
    <published>2018-10-21T04:16:07.944Z</published>
    <updated>2018-10-21T01:50:27.403Z</updated>
    
    <content type="html"><![CDATA[<p><font size="6"><b>前言</b></font><br><strong>在SVM的推导中，在得到了原问题的拉格朗日函数表达之后，是一个最小最大问题，通常会将其转化为原问题的对偶问题即是最大最小问题进行求解，我们这里简单介绍下最优化问题的对偶问题。本人无专业的数学学习背景，只能在直观的角度上解释这个问题，如果有数学专业的朋友，还望不吝赐教。注意，本文应用多限于SVM，因此会比较狭隘。</strong><br><strong>如有谬误，请联系指正。转载请注明出处。</strong><br><em>联系方式：</em><br><strong>e-mail</strong>: <a href="FesianXu@163.com"><code>FesianXu@163.com</code></a><br><strong>QQ</strong>: <code>973926198</code><br><strong>github</strong>: <a href="https://github.com/FesianXu" target="_blank" rel="noopener"><code>https://github.com/FesianXu</code></a><br><strong>有关代码开源</strong>: <a href="https://github.com/FesianXu/AI_Blog/tree/master/SVM相关" target="_blank" rel="noopener">click</a></p><hr><h1 id="最优化问题"><a href="#最优化问题" class="headerlink" title="最优化问题"></a>最优化问题</h1><p>最优化问题研究的是当函数（目标函数）在给定了一系列的约束条件下的最大值或最小值的问题，一般来说，一个最优化问题具有以下形式：</p><script type="math/tex; mode=display">\min_{x \in R^n} f(x) \\s.t. 　　g_i(x) \leq 0 ,i=1,2,\cdots, N\\　　　   h_j(x) = 0, j=1,2,\cdots, M\tag{1.1}</script><p>最优化问题可以根据<strong>目标函数和约束条件的类型</strong>进行分类: </p><ol><li>如果目标函数和约束条件都为变量的线性函数, 称该最优化问题为<strong>线性规划</strong>;</li><li>如果目标函数为变量的二次函数, 约束条件为变量的仿射函数, 称该最优化问题为<strong>二次规划</strong>; </li><li>如果目标函数或者约束条件为变量的非线性函数, 称该最优化问题为<strong>非线性规划</strong>.</li></ol><hr><h1 id="对偶问题"><a href="#对偶问题" class="headerlink" title="对偶问题"></a>对偶问题</h1><p>最优化问题存在对偶问题，所谓对偶问题，源于这个思想：</p><blockquote><p>原始问题比较难以求解，通过构建其对偶问题，期望解决这个对偶问题得到其原问题的下界（在<strong>弱对偶</strong>情况下，对于最小化问题来说），或者得到原问题的解（<strong>强对偶</strong>情况下）。</p></blockquote><p>在SVM中，因为其属于<strong>凸优化问题</strong>，因此是强对偶问题，可以通过构建对偶问题解决得到原问题的解。<br>我们举一个线性规划中一个经典问题，描述如下：</p><blockquote><p>某工厂有两种原料A、B，而且能用其生产两种产品：</p><ol><li>生产第一种产品需要2个A和4个B，能够获利6；</li><li>生产第二种产品需要3个A和2个B，能够获利4；<br>此时共有100个A和120个B，问该工厂最多获利多少？</li></ol></blockquote><p>可以简单得到其问题的数学表达式为：</p><script type="math/tex; mode=display">\max_{x_1, x_2} 6x_1+4x_2 \\s.t.　　2x_1+3x_2 \leq 100 \\　　　4x_1+2x_2 \leq 120\tag{2.1}</script><p>当然，得到这个式子的根据就是最大化其卖出去的产品的利润。但是，如果只问收益的话，明显地，还可以考虑卖出原材料A和B的手段，前提就是卖出原材料的盈利会比生产商品盈利高，假设产品A和产品B的单价为$w_1$和$w_2$，从这个角度看，只要最小化购买原材料的价格，我们就可以得出另一个数学表达式：</p><script type="math/tex; mode=display">\min_{w_1, w_2} {100w_1+120w_2} \\s.t.　　2w_1+4w_2  \geq 6 \\　　　  3w_1+2w_2 \geq 4\tag{2.2}</script><p><strong>其实，我们可以发现这其实是极大极小问题和其对偶问题，极小极大问题</strong>。</p><hr><h1 id="一些定义"><a href="#一些定义" class="headerlink" title="一些定义"></a>一些定义</h1><h2 id="原始问题"><a href="#原始问题" class="headerlink" title="原始问题"></a>原始问题</h2><p>我们要讨论原问题和对偶问题，就需要一些定义，我们给出原始问题的非拉格朗日函数表达形式如式子$(1.1)$所示，引进其广义拉格朗日函数（详见文章<a href="http://blog.csdn.net/loseinvain/article/details/78624888">《拉格朗日乘数法和KKT条件的直观解释》</a>）：</p><script type="math/tex; mode=display">L(x, \alpha, \beta)_{x \in R^n} = f(x)+\sum_{i=1}^N \alpha_i g_i(x)+\sum_{j=1}^M \beta_j h_j(x)\tag{3.1}</script><p>其中$x=(x^{(1)}, x^{(2)}, \cdots, x^{(n)})^T \in R^n$，而$\alpha_i$和$\beta_j$是拉格朗日乘子，其中由<strong>KKT条件</strong>有$\alpha_i \geq 0$，考虑关于x的函数：</p><script type="math/tex; mode=display">\theta_P(x) = \max_{\alpha, \beta; \alpha_i \geq 0} L(x, \alpha, \beta)\tag{3.2}</script><p>这里下标$P$用以表示这个是原始问题。<br>联想到我们在文章<a href="http://blog.csdn.net/loseinvain/article/details/78636285">《SVM的拉格朗日函数表示以及其对偶问题》</a>中一些关于对偶问题的讨论，我们知道其实$(3.2)$中的$\theta_P(x)$其实就表示了$(1.1)$中的原问题的目标函数和其约束条件，这里再探讨一下：<br><strong>假设我们存在一个x，使得x违反原始问题的约束条件，从而有$g_i(x) &gt; 0$或者$h_j(x) \neq 0$，那么我们可以推论出：</strong></p><script type="math/tex; mode=display">\theta_P(x) = \max_{\alpha, \beta; \alpha_i \geq 0} [f(x)+\sum_{i=1}^N \alpha_i g_i(x)+ \sum_{j=1}^M \beta_j h_j(x)] = + \infty\tag{3.3}</script><p>为什么呢？<strong>因为若存在某个i使得$g_i(x) &gt; 0$， 那么就可以令$\alpha_i \rightarrow +\infty$使得$\theta_P(x$)取得无穷大这个“最大值”；同样的，若存在一个j使得$h_j(x) \neq 0$， 那么就总是可以使得$\beta_j$让$\beta_j h_j(x) \rightarrow +\infty$， 而其他各个$\alpha_i$和$\beta_j$均取为0（满足约束条件的拉格朗日乘子取为0）。这样，只有对于满足约束条件的i和j，才会有$\theta_P(x)=f(x)$成立。</strong>于是我们有这个分段表达式：</p><script type="math/tex; mode=display">\theta_P(x) =\begin{cases}f(x) & x满足原始问题约束 \\+ \infty & 其他\end{cases}\tag{3.4}</script><p>所以，如果是最小化问题，我们有极小极大问题$(3.5)$:</p><script type="math/tex; mode=display">\min_{x} \theta_P(x) = \min_{x} \max_{\alpha, \beta; \alpha_i \geq 0} L(x, \alpha, \beta)\tag{3.5}</script><p><strong>其与式子$(1.1)$是完全等价的，有着同样的解</strong>。这样一来，我们就把原始的最优化问题转换为了广义拉格朗日函数的极小极大问题，为了后续讨论方便，我们记：</p><script type="math/tex; mode=display">p^* = \min_{x} \theta_P(x)\tag{3.6}</script><p>其中$p^*$为问题的解。</p><h2 id="极小极大问题的对偶，-极大极小问题"><a href="#极小极大问题的对偶，-极大极小问题" class="headerlink" title="极小极大问题的对偶， 极大极小问题"></a>极小极大问题的对偶， 极大极小问题</h2><p>我们定义：</p><script type="math/tex; mode=display">\theta_{D} (\alpha, \beta) = \min_{x} L(x, \alpha, \beta)\tag{3.7}</script><p>在考虑极大化$(3.7)$有：</p><script type="math/tex; mode=display">\max_{\alpha, \beta; \alpha_i \geq 0} \theta_D(\alpha, \beta)=\max_{\alpha, \beta; \alpha_i \geq 0} \min_{x} L(x, \alpha, \beta)\tag{3.8}</script><p>式子$(3.8)$称为广义拉格朗日函数的极大极小问题，将其变成约束形式，为：</p><script type="math/tex; mode=display">\max_{\alpha, \beta} \theta_D(\alpha, \beta)=\max_{\alpha, \beta} \min_{x} L(x, \alpha, \beta) \\s.t. 　　\alpha_i \geq 0\tag{3.9}</script><p>式子$(3.9)$被称为原问题的对偶问题，定义其最优解为：</p><script type="math/tex; mode=display">d^* = \max_{\alpha, \beta} \theta_D(\alpha, \beta)\tag{3.10}</script><p>实际上，通过这种方法我们可以将式子$(2.1)$转化为式子$(2.2)$，也就是将原问题转化为对偶问题，有兴趣的朋友可以自行尝试。</p><hr><h1 id="原始问题和对偶问题的关系"><a href="#原始问题和对偶问题的关系" class="headerlink" title="原始问题和对偶问题的关系"></a>原始问题和对偶问题的关系</h1><p>正如前面所谈到的，原始问题的解和对偶问题的解存在一定的关系，对于任意的$\alpha, x, \beta$，我们有：</p><script type="math/tex; mode=display">\theta_D(\alpha, \beta)=\min_{x} L(x, \alpha, \beta) \leq L(x, \alpha, \beta) \leq \max_{\alpha, \beta; \alpha_i \geq 0} L(x, \alpha, \beta)=\theta_P(x)\tag{4.1}</script><p>等价于：</p><script type="math/tex; mode=display">\theta_D(\alpha, \beta) \leq \theta_P(x)\tag{4.2}</script><p>注意，式子$(4.2)$对于所有的$x, \alpha, \beta$都成立，因为原始问题和对偶问题均有最优解，所以有：</p><script type="math/tex; mode=display">\max_{\alpha, \beta; \alpha_i \geq 0} \theta_D(\alpha, \beta) \leq \min_{x} \theta_P(x)\tag{4.3}</script><p>容易得到：</p><script type="math/tex; mode=display">d^* = \max_{\alpha, \beta; \alpha_i \geq 0} \min_{x} L(x, \alpha, \beta) \leq \min_{x} \max_{\alpha, \beta; \alpha_i \geq 0} L(x, \alpha, \beta) = p^*\tag{4.4}</script><p>由此我们得到了在最小化问题中$d^* \leq p^*$的结论，这个称为<strong>弱对偶</strong>。<strong>弱对偶指出，解决最小化问题的对偶问题可以得到原问题的解的下界</strong>。</p><p>既然有弱对偶就会存在<strong>强对偶</strong>。强对偶指的是$d^*=p^*$的情况，在某些情况下，原始问题和对偶问题的解相同，这时可以用解决对偶问题来代替原始问题，下面以定理的方式给出强对偶成立的重要条件而不予以证明：</p><blockquote><p>考虑原始问题$(1.1)$和对偶问题$(3.9)$，假设$f(x)$和$g_i(x)$都是凸函数， $h_j(x)$是仿射函数，并且不等式约束$g_i(x)$是严格可行的，既存在$x$，对所有$i$有$g_i(x) &lt; 0 $，则存在$x^*,\alpha^*,\beta^*$，使得$x^*$是原始问题的解，$\alpha^*,\beta^*$是对偶问题的解（满足这个条件的充分必要条件就是$x^*, \alpha^*, \beta^*$满足KKT条件<sup><a href="#fn_1" id="reffn_1">1</a></sup>），并且：<br>$p^* = d^* = L(x^*, \alpha^*, \beta^*)$</p></blockquote><hr><h1 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h1><ol><li><a href="http://blog.csdn.net/qq_34531825/article/details/52872819">最优化问题学习笔记1-对偶理论 CSDN</a></li><li><a href="https://book.douban.com/subject/10590856/" target="_blank" rel="noopener">《统计学习方法》 豆瓣</a></li><li><a href="https://www.zhihu.com/question/27057384" target="_blank" rel="noopener">如何理解对偶问题？ feng liu的回答</a></li><li><a href="http://blog.csdn.net/loseinvain/article/details/78624888">《拉格朗日乘数法和KKT条件的直观解释》 CSDN</a></li><li><a href="http://blog.csdn.net/loseinvain/article/details/78636285">SVM的拉格朗日函数表示以及其对偶问题 CSDN</a></li></ol><blockquote id="fn_1"><sup>1</sup>. 见<a href="http://blog.csdn.net/loseinvain/article/details/78624888">《拉格朗日乘数法和KKT条件的直观解释》</a><a href="#reffn_1" title="Jump back to footnote [1] in the text."> &#8617;</a></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;font size=&quot;6&quot;&gt;&lt;b&gt;前言&lt;/b&gt;&lt;/font&gt;&lt;br&gt;&lt;strong&gt;在SVM的推导中，在得到了原问题的拉格朗日函数表达之后，是一个最小最大问题，通常会将其转化为原问题的对偶问题即是最大最小问题进行求解，我们这里简单介绍下最优化问题的对偶问题。本人无专业的数
      
    
    </summary>
    
      <category term="Machine Learning" scheme="https://blog.csdn.net/LoseInVain/categories/Machine-Learning/"/>
    
    
      <category term="Machine Learning" scheme="https://blog.csdn.net/LoseInVain/tags/Machine-Learning/"/>
    
      <category term="SVM" scheme="https://blog.csdn.net/LoseInVain/tags/SVM/"/>
    
  </entry>
  
  <entry>
    <title>SVM沉思录3——拉格朗日乘数法和KKT条件的直观解释</title>
    <link href="https://blog.csdn.net/LoseInVain/2018/10/21/svm/svm3/"/>
    <id>https://blog.csdn.net/LoseInVain/2018/10/21/svm/svm3/</id>
    <published>2018-10-21T04:16:07.943Z</published>
    <updated>2018-10-21T01:46:22.868Z</updated>
    
    <content type="html"><![CDATA[<p><font size="6"><b>前言</b></font><br><strong>在SVM的推导中，出现了核心的一个最优化问题，这里我们简单介绍下最优化问题，特别是带有约束的最优化问题，并且引入拉格朗日乘数法和广义拉格朗日乘数法，介绍并且直观解释了KKT条件，用于解决带约束的最优化问题。本人无专业的数学学习背景，只能在直观的角度上解释这个问题，如果有数学专业的朋友，还望不吝赐教。</strong><br><strong>如有误，请联系指正。转载请注明出处。</strong><br><em>联系方式：</em><br><strong>e-mail</strong>: <a href="FesianXu@163.com"><code>FesianXu@163.com</code></a><br><strong>QQ</strong>: <code>973926198</code><br><strong>github</strong>: <a href="https://github.com/FesianXu" target="_blank" rel="noopener"><code>https://github.com/FesianXu</code></a><br><strong>有关代码开源</strong>: <a href="https://github.com/FesianXu/AI_Blog/tree/master/SVM%E7%9B%B8%E5%85%B3" target="_blank" rel="noopener">click</a></p><hr><h1 id="最优化问题"><a href="#最优化问题" class="headerlink" title="最优化问题"></a>最优化问题</h1><p>我们在高中，包括在高数中都会经常遇到求解一个函数的最小值，最大值之类的问题，这类问题就是属于最优化问题。比如给出下列一个不带有约束的最优化问题：</p><script type="math/tex; mode=display">\min_{x} 3x^2+4x+5, x \in R\tag{1.1}</script><p>其中的$3x^2+4x+5$我们称为<strong>目标函数(objective  function)</strong>。这样的问题，直接利用<strong>罗尔定理（Rolle’s theorem）</strong>求出其鞍点，又因为其为凸函数而且可行域是整个$R$，求出的鞍点便是最值点，这个是对于无约束最优化问题的解题套路。<br>如果问题带有约束条件，那么就变得不一样了，如：</p><script type="math/tex; mode=display">\min_{x, y} 3xy^2 \\s.t.　　　4x+5y = 10\tag{1.2}</script><p>因为此时的约束条件是<strong>仿射函数（affine function）</strong><sup><a href="#fn_1" id="reffn_1">1</a></sup>，所以可以利用换元法将$x$表示为$y$的函数，从而将目标函数变为无约束的形式，然后利用罗尔定理便可以求出最值点了。<br>然而如果约束条件一般化为$g(x, y) = c$，那么$x$就不一定可以用其他变量表示出来了，这个时候就要利用<strong>拉格朗日乘数法(Lagrange multipliers )</strong>了。</p><hr><h1 id="拉格朗日乘数法-Lagrange-multipliers"><a href="#拉格朗日乘数法-Lagrange-multipliers" class="headerlink" title="拉格朗日乘数法(Lagrange multipliers)"></a>拉格朗日乘数法(Lagrange multipliers)</h1><p>我们先一般化一个二元最优化问题为$(2.1)$形式：</p><script type="math/tex; mode=display">\min_{x, y} f(x, y) \\s.t.　　g(x, y) = c\tag{2.1}</script><p>将目标函数$f(x, y)$和等式约束条件$g(x, y)=c$画出来就如下图所示：<br><img src="/imgs/svm/general_min.png" alt="general_min"></p><p>其中的$f(x, y)$虚线为等高线，而红线为$g(x, y)=c$这个约束函数曲线与$f(x,y)$的交点的连线在$x-y平面$的映射。其中，假设有$d_3 &gt; d_2 &gt; d_1$， $d_1$点为最小值点（最优值点）。<strong>从直观上可以发现，在$g(x,y)=c$与$f(x,y)$的非最优化交点$A$,$B$,$C$,$D$上，其$f(x,y)$和$g(x,y)$的法线方向并不是共线的，注意，这个相当关键，因为如果不是共线的，说明$g(x,y)=c$与$f(x,y)$的交点中，还存在可以取得更小值的点存在。对于A点来说，B点就是更为小的存在。因此，我们从直觉上推论出只有当$g(x,y)=c$与$f(x,y)$的法线共线时，才是最小值点的候选点（鞍点）。推论到多元变量的问题的时候，法线便用梯度表示</strong>。于是，我们有原问题取得最优值的必要条件：</p><script type="math/tex; mode=display">\nabla f(x,y) = \nabla \lambda (g(x, y)-c)\tag{2.2}</script><p>$(2.2)$其中的$\lambda$表示两个梯度共线。<br>可以简单的变形为</p><script type="math/tex; mode=display">\nabla L(x, y, \lambda) = \nabla f(x,y) - \nabla \lambda (g(x, y)-c) = 0\tag{2.3}</script><p>让我们去掉梯度算子，得出</p><script type="math/tex; mode=display">L(x, y, \lambda) = f(x, y) - \lambda(g(x, y) - c)\tag{2.4}</script><p>这个时候$\lambda$取个负号也是不影响的，所以式子$(2.4)$通常写作：</p><script type="math/tex; mode=display">L(x, y, \lambda) = f(x, y) + \lambda(g(x, y) - c)\tag{2.5}</script><p>看！我们得出了我们高数中经常见到的<strong>等式约束下的拉格朗日乘数函数的表示方法</strong>。</p><h2 id="多约束的拉格朗日乘数法"><a href="#多约束的拉格朗日乘数法" class="headerlink" title="多约束的拉格朗日乘数法"></a>多约束的拉格朗日乘数法</h2><p>以上，我们讨论的都是单约束的拉格朗日乘数法，当存在多个等式约束时（其实不等式约束也是一样的），我们进行一些推广。先一般化一个二元多约束最小化问题：</p><script type="math/tex; mode=display">\min_{x, y} f(x, y) \\s.t.　　g_i(x, y) = 0, i = 1,2, \cdots,N\tag{2.6}</script><p>对于每个目标函数和约束配对，我们有:</p><script type="math/tex; mode=display">L_1(x ,y ,\lambda_1) = f(x,y)+\lambda_1 g_1(x,y) \\\vdots \\L_N(x, y, \lambda_N) = f(x,y)+\lambda_N g_N(x,y)</script><p>将上式相加有：</p><script type="math/tex; mode=display">\sum_{i=1}^N L_i(x,y,\lambda_i)=N f(x, y)+\sum_{i=1}^N \lambda_ig_i(x,y)\tag{2.7}</script><p>定义多约束的拉格朗日函数为：</p><script type="math/tex; mode=display">L(x,y,\lambda) = f(x,y)+\frac{1}{N} \sum_{i=1}^N \lambda_ig_i(x,y)\tag{2.8}</script><p>因为$\lambda_i$是常数，表示共线的含义而已，所以乘上一个常数$\frac{1}{N}$也不会有任何影响，我们仍然用$\lambda_i$表示，因此式子$(2.8)$变成：</p><script type="math/tex; mode=display">L(x,y,\lambda) = f(x,y)+\sum_{i=1}^N \lambda_ig_i(x,y)\tag{2.9}</script><p><strong>这就是多约束拉格朗日乘数法的函数表达形式。</strong></p><h2 id="一个计算例子"><a href="#一个计算例子" class="headerlink" title="一个计算例子"></a>一个计算例子</h2><p>让我们举一个单约束的拉格朗日乘数法的计算例子，例子来源于引用3。<br>给出一个<strong>最大化任务</strong>：</p><script type="math/tex; mode=display">\max_{x,y} xy^2 \\s.t.　　g(x,y):x^2+y^2-3=0\tag{2.10}</script><p>图像如：<br><img src="/imgs/svm/lm_example.png" alt="lm_example"></p><p>只有一个约束，使用一个乘子$\lambda$，有拉格朗日函数：</p><script type="math/tex; mode=display">L(x,y,\lambda)=xy^2+\lambda(x^2+y^2-3)</script><p>按照条件求解候选点：</p><script type="math/tex; mode=display">\nabla_{x,y,\lambda} L(x,y,\lambda) = (\frac{\partial L}{\partial x}, \frac{\partial L}{\partial y}, \frac{\partial L}{\partial \lambda})=(2xy+2\lambda x, x^2+2 \lambda y, x^2+y^2-3)=0</script><p>有</p><script type="math/tex; mode=display">x(y+\lambda)=0 \tag{i}</script><script type="math/tex; mode=display">x^2+2 \lambda y = 0 \tag{ii}</script><script type="math/tex; mode=display">x^2+y^2=3 \tag{iii}</script><p>根据式子$(i)(ii)(iii)$， 解得有：</p><script type="math/tex; mode=display">(\pm \sqrt{2}, 1, -1); (\pm \sqrt{2}, -1, 1); (0, \pm \sqrt{3}, 0)</script><p>代入$f(x,y)$，得到：2， -2， 0，也就是我们需要求得的最大值，最小值。可以从图中看出，<strong>我们观察到其等高线与约束投影线的确是相切的。</strong></p><hr><h1 id="广义拉格朗日乘数法-Generalized-Lagrange-multipliers"><a href="#广义拉格朗日乘数法-Generalized-Lagrange-multipliers" class="headerlink" title="广义拉格朗日乘数法(Generalized Lagrange multipliers)"></a>广义拉格朗日乘数法(Generalized Lagrange multipliers)</h1><p>上面我们的拉格朗日乘数法解决了等式约束的最优化问题，但是在<strong>存在不等式约束的最优化问题</strong>（包括我们SVM中需要求解的最优化问题）上，普通的拉格朗日乘数法并不能解决，因此学者提出了<strong>广义拉格朗日乘数法（Generalized Lagrange multipliers）</strong>， 用于解决含有不等式约束的最优化问题。这一章，我们谈一谈广义拉格朗日乘数法。</p><p>首先，我们先一般化我们的问题，规定一个二元标准的带有不等式约束的最小化问题(当然可以推广到多元的问题)，如：</p><script type="math/tex; mode=display">\min_{x, y} f(x, y) \\s.t.　　g_i(x, y) \leq 0, i = 1,2,\cdots,N \\h_i(x, y) = 0, i = 1,2, \cdots, M\tag{3.1}</script><p>类似于拉格朗日乘数法，参照式子$(2.9)$，我们使用$\alpha_i$和$\beta_i$作为等式约束和不等式约束的拉格朗日乘子，得出下式：</p><script type="math/tex; mode=display">L(x, y, \alpha, \beta) = f(x, y)+\sum_{i=1}^N \alpha_ig_i(x ,y)+\sum_{i=1}^M \beta_i h_i(x, y)\tag{3.2}</script><p><strong>KKT条件（Karush–Kuhn–Tucker conditions）</strong>指出，当满足以下几个条件的时候，其解是问题最优解的候选解(摘自wikipedia)。</p><ol><li><strong>Stationarity</strong><ul><li>对于最小化问题就是：<br>$\nabla f(x,y)+\sum_{i=1}^N \alpha_i \nabla g_i(x ,y)+\sum_{i=1}^M \beta_i \nabla h_i(x, y) = 0 \tag{3.3}$</li><li>对于最大化问题就是：<br>$\nabla f(x,y)-(\sum_{i=1}^N \alpha_i \nabla g_i(x ,y)+\sum_{i=1}^M \beta_i \nabla h_i(x, y)) = 0 \tag{3.4}$</li></ul></li></ol><ol><li><p><strong>Primal feasibility</strong></p><ul><li>$g_i(x,y) \leq0, i = 1,2,\cdots,N \tag{3.5}$</li><li>$h_i(x, y) = 0, i = 1,2,\cdots,M \tag{3.6}$</li></ul></li><li><p><strong>Dual feasibility</strong></p><ul><li>$\alpha_i \geq 0, i = 1,2, \cdots, N \tag{3.7}$</li></ul></li><li><p><strong>Complementary slackness</strong></p><ul><li>$\alpha_i g_i(x, y) = 0, i = 1,2,\cdots,N \tag{3.8}$</li></ul></li></ol><p>其中的第一个条件和我们的拉格朗日乘数法的含义是相同的，就是梯度共线的意思；而第二个条件就是主要约束条件，自然是需要满足的；<strong>有趣的和值得注意的是第三个和第四个条件，接下来我们探讨下这两个条件，以及为什么不等式约束会多出这两个条件。</strong></p><hr><p>为了接下来的讨论方便，我们将N设为3，并且去掉等式约束，这样我们的最小化问题的广义拉格朗日函数就变成了：</p><script type="math/tex; mode=display">L(x, y, \alpha, \beta) = f(x, y)+\sum_{i=1}^3 \alpha_ig_i(x ,y)\tag{3.9}</script><p>绘制出来的示意图如下所示：<br><img src="/imgs/svm/general_lagrange_multipliers.png" alt="general_lagrange_multipliers"></p><p>其中$d_i &gt; d_j, when　i &gt; j$，而蓝线为最优化寻路过程。</p><p>让我们仔细观察式子$(3.7)$和$(3.8)$，我们不难发现，因为$\alpha_i \geq 0$而$g_i(x, y) \leq 0$，并且需要满足$\alpha_i g_i(x, y) = 0$，所以$\alpha_i$和$g_i(x,y)$之中必有一个为0，那为什么会这样呢？</p><hr><p>我们从上面的示意图入手理解并且记好公式$(3.3)$。让我们假设初始化一个点A， 这个点A明显不处于最优点，也不在可行域内，可知$g_2(x,y)&gt;0$违背了$(3.5)$，为了满足约束$(3.8)$，有$\alpha_2=0$，导致$\alpha_2 \nabla g_2(x,y)=0$，而对于$i=1,3$，因为满足约束条件而且$g_1(x,y) \neq 0, g_3(x,y) \neq 0$，所以$\alpha_1 = 0, \alpha_3 = 0$。这样我们的式子$(3.3)$就只剩下$\nabla f(x,y)$，<strong>因此对着$\nabla f(x,y)$进行优化，也就是沿着$f(x,y)$梯度方向下降即可，不需考虑其他的条件（因为还完全处于可行域之外）</strong>。因此，A点一直走啊走，从A到B，从B到C，从C到D，这个时候因为D点满足$g_2(x,y)=0$，因此$\alpha_2 &gt; 0$，所以$\alpha_2\nabla g_2(x,y) \neq 0$，因此$(3.3)$就变成了$\nabla f(x,y)+\alpha_2\nabla g_2(x,y)$所以在优化下一个点E的时候，就会考虑到需要满足约束$g_2(x,y) \leq 0$的条件，朝着向$g_2(x,y)$减小，而且$f(x,y)$减小的方向优化。因此下一个优化点就变成了E点，而不是G点。因此没有约束的情况下其优化路径可能是$A \rightarrow B \rightarrow C \rightarrow D \rightarrow G \rightarrow H$，而添加了约束之后，其路径变成了$A \rightarrow B \rightarrow C \rightarrow D \rightarrow E \rightarrow F$。</p><hr><p>这就是为什么KKT条件引入了条件3和条件4，就是为了在满足不等式约束的情况下对目标函数进行优化。让我们记住这个条件，因为这个条件中某些$\alpha_i=0$的特殊性质，将会在SVM中广泛使用，而且正是这个性质定义了<strong>支持向量(SV)</strong>。</p><hr><h1 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h1><ol><li><a href="https://www.zhihu.com/question/38586401" target="_blank" rel="noopener">拉格朗日乘子法如何理解？ 知乎</a></li><li><a href="https://book.douban.com/subject/10590856/" target="_blank" rel="noopener">《统计学习方法》 豆瓣</a></li><li><a href="http://mp.weixin.qq.com/s?__biz=MzI4MDYzNzg4Mw==&amp;mid=2247486230&amp;idx=1&amp;sn=41c3bcbf00b43535e912af1dab2704f5&amp;chksm=ebb433c2dcc3bad45fa784314d4ea6a464538ff19b937b58bd99f12b43ec3c3cf01a269f9124&amp;mpshare=1&amp;scene=23&amp;srcid=11211sArM7yh9Xf0pkvnWaJA#rd" target="_blank" rel="noopener">《【直观详解】拉格朗日乘法和KKT条件》  微信公众号</a></li><li><a href="http://blog.csdn.net/on2way/article/details/47729419">《解密SVM系列（一）：关于拉格朗日乘子法和KKT条件》 CSDN</a></li><li><a href="https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions" target="_blank" rel="noopener">Karush–Kuhn–Tucker conditions wikipedia</a></li></ol><blockquote id="fn_1"><sup>1</sup>. 最高次数为1的多项式，形如 $f(x) = AX+B$，其中$X$是$m \times k$的仿射矩阵，其与线性函数的区别就是，线性函数是$f(x) = AX$没有偏置项$B$。<a href="#reffn_1" title="Jump back to footnote [1] in the text."> &#8617;</a></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;font size=&quot;6&quot;&gt;&lt;b&gt;前言&lt;/b&gt;&lt;/font&gt;&lt;br&gt;&lt;strong&gt;在SVM的推导中，出现了核心的一个最优化问题，这里我们简单介绍下最优化问题，特别是带有约束的最优化问题，并且引入拉格朗日乘数法和广义拉格朗日乘数法，介绍并且直观解释了KKT条件，用于解决带
      
    
    </summary>
    
      <category term="Machine Learning" scheme="https://blog.csdn.net/LoseInVain/categories/Machine-Learning/"/>
    
    
      <category term="Machine Learning" scheme="https://blog.csdn.net/LoseInVain/tags/Machine-Learning/"/>
    
      <category term="SVM" scheme="https://blog.csdn.net/LoseInVain/tags/SVM/"/>
    
  </entry>
  
  <entry>
    <title>SVM沉思录2——SVM的拉格朗日函数表示以及其对偶问题</title>
    <link href="https://blog.csdn.net/LoseInVain/2018/10/21/svm/svm2/"/>
    <id>https://blog.csdn.net/LoseInVain/2018/10/21/svm/svm2/</id>
    <published>2018-10-21T04:16:07.941Z</published>
    <updated>2018-10-21T01:39:07.511Z</updated>
    
    <content type="html"><![CDATA[<p><font size="6"><b>前言</b></font><br><strong>支持向量机的对偶问题比原问题容易解决，在符合KKT条件的情况下，其对偶问题和原问题的解相同，这里我们结合李航博士的《统计学习方法》一书和林轩田老师的《机器学习技法》中的内容，介绍下SVM的对偶问题。本人无专业的数学学习背景，只能直观上理解一些问题，请数学专业的朋友不吝赐教。</strong><br><strong>如有谬误，请联系指正。转载请注明出处。</strong><br><em>联系方式：</em><br><strong>e-mail</strong>: <code>FesianXu@163.com</code><br><strong>QQ</strong>: <code>973926198</code><br><strong>github</strong>: <code>https://github.com/FesianXu</code><br><strong>有关代码开源</strong>: <a href="https://github.com/FesianXu/AI_Blog/tree/master/SVM%E7%9B%B8%E5%85%B3" target="_blank" rel="noopener">click</a></p><hr><h1 id="SVM的原问题的拉格朗日乘数表示"><a href="#SVM的原问题的拉格朗日乘数表示" class="headerlink" title="SVM的原问题的拉格朗日乘数表示"></a>SVM的原问题的拉格朗日乘数表示</h1><p>　　我们在上一篇博文《SVM笔记系列1，SVM起源与目的》中，谈到了SVM的原问题，这里摘抄如下：</p><script type="math/tex; mode=display">\min_{W,b} \frac{1}{2} \Vert W  \Vert^2 \\s.t. 1-y_i(W^Tx_i+b) \leq 0, \ i=1,\cdots,N\tag{1.1}</script><p>其满足形式:</p><script type="math/tex; mode=display">\min_{W,b} f(x) \\s.t. c_i(x) \leq0, i=1,\cdots,k \\h_j(x) = 0, j=1,\cdots,l\tag{1.2}</script><p>假设原问题为$\theta_P(x)$，并且其最优解为$p^*=\theta_P(x^*)$。<br>这是一个有约束的最优化问题，我们利用广义拉格朗日乘子法(具体移步<a href="http://blog.csdn.net/loseinvain/article/details/78624888">《拉格朗日乘数法和KKT条件的直观解释》</a>)，将其转换为无约束的形式：</p><script type="math/tex; mode=display">L(W,b,\alpha) = \frac{1}{2}  \Vert W  \Vert ^2 + \sum_{i=1}^N \alpha_i (1-y_i(W^Tx_i+b)), \ \alpha_i \geq 0\tag{1.3}</script><p>变形为：</p><script type="math/tex; mode=display">L(W,b,\alpha) = \frac{1}{2}||W||^2 + \sum_{i=1}^N {\alpha_i}-\sum_{i=1}^N{\alpha_iy_i(W^Tx_i+b)} , \ \alpha_i \geq 0\tag{1.4}</script><p>我们将会得到原问题的另一个表述为：</p><script type="math/tex; mode=display">\begin{aligned}f(x) &= \max_{\alpha} L(W, b, \alpha) \\&=\max_{\alpha} \frac{1}{2}  \Vert W  \Vert ^2 + \sum_{i=1}^N {\alpha_i}-\sum_{i=1}^N{\alpha_iy_i(W^Tx_i+b)}, \\&\alpha_i \geq 0\end{aligned}\tag{1.5}</script><script type="math/tex; mode=display">\begin{aligned}\theta_P(x) &= \min_{W,b}f(x) = \min_{W,b} \max_{\alpha} L(W, b, \alpha) \\&= \min_{W,b} \max_{\alpha} \frac{1}{2}||W||^2 + \sum_{i=1}^N {\alpha_i}-\sum_{i=1}^N{\alpha_iy_i(W^Tx_i+b)},, \ \alpha_i \geq 0\end{aligned}\tag{1.6}</script><p>这里我觉得有必要解释下为什么$f(x)$可以表述为$\max_{\alpha} L(W, b, \alpha)$这种形式。<br>假设我们有一个样本点$x_i$是不满足原问题的约束条件$1-y_i(W^Tx_i+b) \leq 0$的，也就是说$1-y_i(W^Tx_i+b) \gt 0$，那么在$\max_{\alpha}$这个环节就会使得$\alpha_i \rightarrow +\infty$从而使得$L(W,b,\alpha) \rightarrow +\infty$。如果$x_i$是满足约束条件的，那么为了求得最大值，因为$1-y_i(W^Tx_i+b) \leq 0$而且$\alpha_i \geq 0$，所以就会使得$\alpha_i = 0$。由此我们得知：</p><script type="math/tex; mode=display">\max_{\alpha}L(W,b,\alpha) = \begin{cases}  \frac{1}{2}  \Vert W  \Vert^2 & 1-y_i(W^Tx_i+b) \leq 0 满足约束条件\\+\infty & 1-y_i(W^Tx_i+b) \gt 0 不满足约束条件\end{cases}\tag{1.7}</script><p>因此在满足约束的情况下，</p><script type="math/tex; mode=display">\max_{\alpha}L(W,b,\alpha)=\frac{1}{2} \Vert W \Vert ^2</script><p>不满足约束条件的样本点则因为无法对正无穷求最小值而自然抛弃。<br>这个时候，我们试图去解$\max_{\alpha}L(W,b,\alpha)$中的$\max_{\alpha}$我们会发现因为$L(W,b,\alpha)=\frac{1}{2}||W||^2 + \sum_{i=1}^N {\alpha_i}-\sum_{i=1}^N{\alpha_iy_i(W^Tx_i+b)}$对于$\alpha$是线性的，非凸的<sup><a href="#fn_1" id="reffn_1">1</a></sup>，因此无法通过梯度的方法求得其最大值点，其最大值点应该处于可行域边界上，因此我们需要得到SVM的对偶问题进行求解。<br><strong>至此，我们得到了原问题的最小最大表述：</strong></p><script type="math/tex; mode=display">\begin{aligned}\theta_P(x) &= \min_{W,b} \max_{\alpha} L(W, b, \alpha) \\&=\min_{W,b} \max_{\alpha} \frac{1}{2}  \Vert W  \Vert^2 + \sum_{i=1}^N {\alpha_i}-\sum_{i=1}^N{\alpha_iy_i(W^Tx_i+b)} \\&\alpha_i \geq0,i=1,\cdots,N\end{aligned}\tag{1.8}</script><hr><h1 id="SVM的对偶问题"><a href="#SVM的对偶问题" class="headerlink" title="SVM的对偶问题"></a>SVM的对偶问题</h1><p>从上面的讨论中，我们得知了SVM的原问题的最小最大表达形式为：</p><script type="math/tex; mode=display">\begin{aligned}\theta_P(x) &= \min_{W,b} \max_{\alpha} L(W, b, \alpha) \\&=\min_{W,b} \max_{\alpha} \frac{1}{2}||W||^2 + \sum_{i=1}^N {\alpha_i}-\sum_{i=1}^N{\alpha_iy_i(W^Tx_i+b)} \\&\alpha_i \geq0,i=1,\cdots,N\end{aligned}\tag{2.1}</script><p>设SVM的对偶问题为$\theta_D(\alpha)$，其最优解为$d^*=\theta_D(\alpha^*)$，可知道其为：</p><script type="math/tex; mode=display">\begin{aligned}g(x) &= \min_{W,b} L(W,b,\alpha) \\&=\min_{W,b} \frac{1}{2}  \Vert W  \Vert^2 + \sum_{i=1}^N {\alpha_i}-\sum_{i=1}^N{\alpha_iy_i(W^Tx_i+b)}\end{aligned}\tag{2.2}</script><script type="math/tex; mode=display">\begin{aligned}\theta_D(\alpha) &= \max_{\alpha}g(x) = \max_{\alpha} \min_{W,b} L(W,b,\alpha)\\&=\max_{\alpha} \min_{W,b} \frac{1}{2} \Vert W  \Vert^2 + \sum_{i=1}^N {\alpha_i}-\sum_{i=1}^N{\alpha_iy_i(W^Tx_i+b)}\end{aligned}\tag{2.3}</script><p>此时，我们得到了对偶问题的最大最小表述，同样的，我们试图去求解$\theta_D(\alpha)$中的$\min_{W,b}$，我们会发现由于$L(W,b,\alpha)=\frac{1}{2} \Vert W \Vert^2 + \sum_{i=1}^N {\alpha_i}-\sum_{i=1}^N{\alpha_iy_i(W^Tx_i+b)}$对于$W$来说是凸函数，因此可以通过梯度的方法求得其最小值点（即是其极小值点）。</p><p>求解$\min_{W,b} L(W,b,\alpha)$，因为$L(W,b,\alpha)$是凸函数，我们对采用求梯度的方法求解其最小值（也是KKT条件中的，$\nabla_WL(W,b,\alpha)=0$和$\nabla_b L(W,b,\alpha)=0$）：</p><script type="math/tex; mode=display">\frac{\partial{L}}{\partial{W}}=W-\sum_{i=1}^N\alpha_iy_ix_i=0, i=1,\cdots,N\tag{2.4}</script><script type="math/tex; mode=display">\frac{\partial{L}}{\partial{b}}=\sum_{i=1}^N\alpha_iy_i=0,i=1,\cdots,N\tag{2.5}</script><p>得出：</p><script type="math/tex; mode=display">W=\sum_{i=1}^N\alpha_iy_ix_i,　\sum_{i=1}^N\alpha_iy_i=0,　\alpha_i \geq0,i=1,\cdots,N\tag{2.6}</script><p>将其代入$g(x)$，注意到$\sum_{i=1}^N\alpha_iy_i=0$,得：</p><script type="math/tex; mode=display">\begin{aligned}g(x) &= \frac{1}{2} \sum_{i=1}^N \alpha_iy_ix_i \sum_{j=1}^N a_jy_jx_j+\sum_{i=1}^N\alpha_i-\sum_{i=1}^N\alpha_iy_i(\sum_{j=1}^N \alpha_jy_jx_j \cdot x_i+b) \\&= -\frac{1}{2}\sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_jy_iy_j(x_i \cdot x_j)+ \sum_{i=1}^N\alpha_i\end{aligned}</script><p>整理为:</p><script type="math/tex; mode=display">\max_{\alpha}g(x) = \max_{\alpha}-\frac{1}{2}\sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_jy_iy_j(x_i \cdot x_j)+ \sum_{i=1}^N\alpha_i \\s.t. \ \sum_{i=1}^N\alpha_iy_i=0 \\\alpha_i \geq0,i=1,\cdots,N\tag{2.7}</script><p>等价为求最小问题:</p><script type="math/tex; mode=display">\min_{\alpha}g(x) = \min_{\alpha}\frac{1}{2}\sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_jy_iy_j(x_i \cdot x_j)- \sum_{i=1}^N\alpha_i \\s.t. \ \sum_{i=1}^N\alpha_iy_i=0 \\\alpha_i \geq0,i=1,\cdots,N\tag{2.8}</script><p>根据<strong>Karush–Kuhn–Tucker(KKT)条件</strong><sup><a href="#fn_2" id="reffn_2">2</a></sup>,我们有：</p><script type="math/tex; mode=display">\nabla_WL(W^*,b^*,\alpha^*)=W^*-\sum_{i=1}^N\alpha_i^*y_ix_i=0 \Longrightarrow W^* = \sum_{i=1}^N\alpha_i^*y_ix_i\tag{2.9}</script><script type="math/tex; mode=display">\nabla_bL(W^*,b^*,\alpha^*) = -\sum_{i=1}^N \alpha^*_i y_i=0\tag{2.10}</script><script type="math/tex; mode=display">\alpha^*_i(1-y_i(W^*x_i+b^*))=0\tag{2.11}</script><script type="math/tex; mode=display">1-y_i(W^*x_i+b^*) \leq 0\tag{2.12}</script><script type="math/tex; mode=display">\alpha^*_i \geq 0\tag{2.13}</script><p>前两个式子我们已经在求极值的时候利用了，得知:</p><script type="math/tex; mode=display">W^* = \sum_{i=1}^N\alpha_i^*y_ix_i\tag{2.14}</script><p>并且其中至少有一个$\alpha_j^* \gt 0$，对此$j$有，$y_j(W^*x_j+b^*)-1=0$<br>代入刚才的$W^*$，我们有</p><script type="math/tex; mode=display">b^*=y_j-\sum_{i=1}^N\alpha^*_iy_i(x_i \cdot x_j)\tag{2.15}</script><p>所以决策超平面为：</p><script type="math/tex; mode=display">\sum_{i=1}^N \alpha^*_iy_i(x_i \cdot x)+b^*=0\tag{2.16}</script><p>分类超平面为：</p><script type="math/tex; mode=display">\theta(x)=sign(\sum_{i=1}^N \alpha^*_iy_i(x_i \cdot x)+b^*)\tag{2.17}</script><p>其中，我们可以观察到超平面只是依赖于$\alpha_i^*&gt;0$的样本点$x_i$，而其他样本点对其没有影响，所以这些样本是对决策超平面起着决定性作用的，因此我们将$\alpha_i^*&gt;0$对应的样本点集合$x_i$称为<strong>支持向量</strong>。同时，我们可以这样理解当$\alpha^*_i &gt;0$时，我们有$1-y_i(W^*x_i+b)=0$，这个恰恰是表明了<strong>支持向量</strong>的函数间隔都是1，恰好和我们之前的设定一致。<br><img src="/imgs/svm/svm_margin_2.png" alt="svm_margin"></p><p>至此，我们得到了<strong>硬间隔线性支持向量机</strong>的数学表述形式，所谓硬间隔线性支持向量机，就是满足我之前的假设</p><blockquote><p>两类样本是线性可分的，总是存在一个超平面$W^Tx+b$可以将其完美分割开。</p></blockquote><p>但是，在现实生活中的数据往往是或本身就是非线性可分但是近似线性可分的，或是线性可分但是具有噪声的，以上两种情况都会导致在现实应用中，<strong>硬间隔线性支持向量机</strong>变得不再实用，因此我们将会在后续讨论用以解决近似线性可分的<strong>软间隔线性支持向量机</strong>和<strong>基于kernel的支持向量机</strong>，后者可以解决非线性可分的问题。下图表示了<strong>硬间隔线性支持向量机</strong>和<strong>软间隔支持向量机</strong>之间的区别。<br><img src="/imgs/svm/hard_margin_svm.png" alt="hard_margin"></p><p><img src="/imgs/svm/soft_margin_svm.png" alt="soft_margin"><br>在下一篇中，我们紧接着现在的内容，介绍<strong>序列最小最优化算法（Sequential Minimal Optimization,SMO）</strong>，用于求解$\theta_D(x)$，得到$\alpha^*_i$以便于得到超平面的$W^*$和$b$。我们将在其他文章中介绍<strong>软间隔线性支持向量机</strong>，<strong>广义拉格朗日乘数法</strong>，<strong>KKT条件</strong>和<strong>基于kernel的支持向量机</strong>。</p><p><strong>这里我们要记住我们需要最优化的目的式子，我们以后将会反复提到这个式子。</strong></p><script type="math/tex; mode=display">\min_{\alpha}\frac{1}{2}\sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_jy_iy_j(x_i \cdot x_j)- \sum_{i=1}^N\alpha_i</script><script type="math/tex; mode=display">s.t. \ \sum_{i=1}^N\alpha_iy_i=0</script><script type="math/tex; mode=display">\alpha_i \geq0,i=1,\cdots,N</script><blockquote id="fn_1"><sup>1</sup>. 易证明。参考wikipedia的凸函数定义。<a href="#reffn_1" title="Jump back to footnote [1] in the text."> &#8617;</a></blockquote><blockquote id="fn_2"><sup>2</sup>. 事实上，如果$\theta_D(x)$的$L(W,b,\alpha)$满足KKT条件，那么在SVM这个问题中，$W^*$和$b^*$和$\alpha^*_i$同时是原问题和对偶问题的解的充分必要条件是满足KKT条件，具体见《统计学习方法》附录和<a href="http://blog.csdn.net/loseinvain/article/details/78624888">《拉格朗日乘数法和KKT条件的直观解释》</a>。<a href="#reffn_2" title="Jump back to footnote [2] in the text."> &#8617;</a></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;font size=&quot;6&quot;&gt;&lt;b&gt;前言&lt;/b&gt;&lt;/font&gt;&lt;br&gt;&lt;strong&gt;支持向量机的对偶问题比原问题容易解决，在符合KKT条件的情况下，其对偶问题和原问题的解相同，这里我们结合李航博士的《统计学习方法》一书和林轩田老师的《机器学习技法》中的内容，介绍下SVM的
      
    
    </summary>
    
      <category term="Machine Learning" scheme="https://blog.csdn.net/LoseInVain/categories/Machine-Learning/"/>
    
    
      <category term="Machine Learning" scheme="https://blog.csdn.net/LoseInVain/tags/Machine-Learning/"/>
    
      <category term="SVM" scheme="https://blog.csdn.net/LoseInVain/tags/SVM/"/>
    
  </entry>
  
  <entry>
    <title>SVM沉思录1——什么是支持向量机SVM</title>
    <link href="https://blog.csdn.net/LoseInVain/2018/10/21/svm/svm1/"/>
    <id>https://blog.csdn.net/LoseInVain/2018/10/21/svm/svm1/</id>
    <published>2018-10-21T04:16:07.940Z</published>
    <updated>2018-10-21T01:03:00.246Z</updated>
    
    <content type="html"><![CDATA[<p><font size="6"><b>前言</b></font><br><strong>支持向量机是常用的，泛化性能佳的，而且可以应用核技巧的机器学习算法，在深度学习流行前是最被广泛使用的机器学习算法之一，就算是深度学习流行的现在，支持向量机也由于其高性能，较低的计算复杂度而被人们广泛应用。这里结合李航博士的《统计学习方法》一书的推导和林轩田老师在《机器学习技法》中的讲解，谈谈自己的认识。</strong><br><strong>如有谬误，请联系指正。转载请注明出处。</strong><br><em>联系方式：</em><br><strong>e-mail</strong>: <code>FesianXu@163.com</code><br><strong>QQ</strong>: <code>973926198</code><br><strong>github</strong>: <code>https://github.com/FesianXu</code><br><strong>有关代码开源</strong>: <a href="https://github.com/FesianXu/AI_Blog/tree/master/SVM%E7%9B%B8%E5%85%B3" target="_blank" rel="noopener">click</a></p><hr><h1 id="SVM的起源"><a href="#SVM的起源" class="headerlink" title="SVM的起源"></a>SVM的起源</h1><p>　　<strong>支持向量机(Support Vector Machine, SVM)</strong>是一种被广泛使用的机器学习算法，自从被Vapnik等人提出来之后便被广泛使用和发展。传统的支持向量机一般是<strong>二类分类器</strong>，其基本出发点很简单，就是<strong>找到一个策略，能够让线性分类器的分类超平面能够最大程度的把两类的样本最好地分割开</strong>，这里我们讨论下什么叫做<strong>最好地分割开</strong>，和<strong>实现这个对整个分类器的意义。</strong></p><h2 id="最好地分割数据"><a href="#最好地分割数据" class="headerlink" title="最好地分割数据"></a>最好地分割数据</h2><p>　　在进行接下来的讨论之前，为了简化我们的讨论从而直面问题所在，我们进行以下假设：</p><blockquote><p>1） 我们现在的两类数据是线性可分的， 也就是总是存在一个超平面$W^TX+b$可以将数据完美的分开。<br>2） 我们的数据维度是二维的，也就是特征维只有两个，类标签用+1， -1表示，这样方便我们绘制图像。</p></blockquote><p>　　我在前篇博文<a href="http://blog.csdn.net/LoseInVain/article/details/78430585">《机器学习系列之 感知器模型》</a>中已经介绍到了感知器这一简单的线性分类器。感知器很原始，只能对线性可分的数据进行准确分割，而且由于其激活函数选用的是阶跃函数，因此不能通过梯度的方法进行参数更新，而是只能采用<strong>错误驱动的策略</strong>进行参数更新，这样我们的超平面其实是<strong>不确定的</strong>，因为其取决于具体随机到的是哪个样本点进行更新，这是一个不稳定的结果。而且，由于采用了这种参数更新策略，感知器的超平面即使是能够将线性数据完美地分割开，也经常会出现超平面非常接近某一个类的样本，而偏离另一个类的样本的这种情况，特别是在真实情况下的数据是叠加了噪声的情况下。<br>　　如下图所示，其中绿线是感知器的运行结果，因为其算法的不稳定性，所以每次的结果都可能不同，选中的这一次我们可以看出来虽然绿线<strong>将两类数据完美地分割开了，但是和蓝色样本很接近，如果新来的测试样本叠加一个噪声，这个超平面就很容易将它分类错误，而最佳分类面粉色线则对噪声有着更好地容忍。</strong><br><img src="/imgs/svm/best_divide.png" alt="best_divide"></p><h2 id="样本噪声"><a href="#样本噪声" class="headerlink" title="样本噪声"></a>样本噪声</h2><p>　　刚才我们谈到了样本集上叠加的<strong>噪声</strong>，噪声广泛存在于真实数据集中，无法避免，因此我们的分类超平面要能够对噪声进行一定的容忍。一般我们假设噪声为高斯噪声，如下图所示：<br><img src="/imgs/svm/noise.png" alt="noise"><br>　　其中红点为实际的采样到的样本位置$P_{sample}$，而蓝点是可能的样本的实际位置$P_{actual}$，因为噪声$N$的叠加才使得其偏离到了红点位置，其中蓝点的位置满足高斯分布。</p><script type="math/tex; mode=display">P_{sample} = P_{actual}+N, N \sim N(\mu, \sigma^2)\tag{1.1}</script><h2 id="最佳分类超平面"><a href="#最佳分类超平面" class="headerlink" title="最佳分类超平面"></a>最佳分类超平面</h2><p>　　也就是说我们根据$P_{sample}$点训练出来的感知器的分类器超平面很可能会出现可以完美地划分$P_{sample}$点，但是却不能正确地划分对新来的测试样本的现象。因为新来的样本很可能位于蓝色的样本点的位置，也就是表现出了严重的过拟合现象， 而我们的支持向量机的机制可以很好地减免这种现象，具有更好的泛化能力。我们用几张图来表述下导致这种过拟合的原因：<br><img src="/imgs/svm/overfit.png" alt="overfit"><br>Figure 1, 感知器分类超平面能将线性可分的样本完美分割，但是由于样本叠加了高斯噪声$N$，所以当测试样本的数据出现在超平面“穿过”的“绿圈”之内时，就可能会出现错分的情况，这就是<strong>过拟合</strong>的一种表现。<br><img src="/imgs/svm/svm_divide.png" alt="svm_divide"><br>Figure 2,假设我们的样本集都是独立同分布采样的，那么其叠加的高斯噪声$N$应该都是相同分布$N \sim N(\mu, \sigma^2)$的，因此这个绿圈的大小应该都是相同的，因此最佳的分类超平面应该是可以和距离它最接近的若干个样本的边界相切的。我们把最接近超平面的若干个样本点称为<strong>支持向量</strong>，支持向量和超平面的距离越远，相当于我们可以容忍的噪声的高斯分布的方差越小，泛化性能越好。<strong>注意，这里的高斯分布的方差是我们假设的，不一定是实际数据集叠加的高斯噪声分布的方差，但是假设的越大，总是能带来更好的泛化能力。</strong></p><hr><h1 id="SVM提出"><a href="#SVM提出" class="headerlink" title="SVM提出"></a>SVM提出</h1><p>　　我们在上面谈到了最佳分类超平面应能够使得支持向量距离超平面的距离最大，这个就是<strong>支持向量机</strong>的基本机制的最优化的目标，我们需要解决这个问题就必须要先数学形式化我们这个目的，只有这样才能进行最优化和求解。</p><h2 id="数学形式化表述"><a href="#数学形式化表述" class="headerlink" title="数学形式化表述"></a>数学形式化表述</h2><p>　　我们这里对SVM问题进行数学上的形式化表述，以便于求解，这里主要讨论SVM的原问题，实际上，SVM通常转化为对偶问题进行求解，我们将在下一篇文章里讨论SVM的对偶问题。</p><h2 id="函数间隔和几何间隔"><a href="#函数间隔和几何间隔" class="headerlink" title="函数间隔和几何间隔"></a>函数间隔和几何间隔</h2><p>　　我们刚才的表述中，我们知道了SVM的关键就是：<strong>使得支持向量距离分类超平面的距离之和最小</strong>，这里涉及到了“距离”这个概念，因此我们就必须要定义这个“距离”。这个距离可以定义为<strong>函数间隔(functional margin)</strong>和<strong>几何间隔(geometric margin)</strong>。我们分别来观察下这两个间隔。</p><h3 id="函数间隔"><a href="#函数间隔" class="headerlink" title="函数间隔"></a>函数间隔</h3><p>　　我们表征一个样本点$x_i$到达一个超平面$\theta(x)=W^Tx+b$的距离，直接可以表述为:</p><script type="math/tex; mode=display">\gamma_i = y_i\theta(x_i) = y_i(W^Tx_i+b), \ x_i \in R^n\tag{2.1}</script><p>　　其中$y_i$为正确的标签，为$+1$或$-1$，乘上$y_i$的目的是当$x_i$分类正确的时候$\gamma_i$为正，而当分类错误的时候，$\gamma_i$为负，负数的最大值不超过0，所以也就不存在最大间隔了。整个式子也很好理解，当$x_i$使得$\theta(x)=0$时，该样本点就处于超平面上，当$x_i$使得$\theta(x)$大于0时，该样本点处于超平面之外，该值越大离超平面就越远。<br><img src="/imgs/svm/margin.png" alt="margin"></p><h3 id="几何间隔"><a href="#几何间隔" class="headerlink" title="几何间隔"></a>几何间隔</h3><p>　　函数间隔可以在一定程度上表征距离，但是存在一个问题，就是在$W$和$b$同时增大一个相同的倍数$\alpha$时，变成$\theta(x)=\alpha W^Tx+\alpha b$时，因为当$\alpha \neq 0$时，其零点还是相同的，所以表示的还是相同的超平面。但是我们从函数间隔的定义中可以看出，如果两者都放大$\alpha$倍，那么其函数间隔也被放大了$\alpha$倍，这个就不符合我们的需求了，我们希望的是<strong>只要是相同的一个样本点和一个固定的超平面，那么它们之间的距离应该是一定的，这个也是符合我们直观的</strong>。因此我们将函数间隔标准化，定义了几何间隔：</p><script type="math/tex; mode=display">\hat{\gamma_i} = \frac{y_i(W^Tx_i+b)}{||W||_{L2}}\tag{2.2}</script><p>　　$||W||_{L2}$是L2范式，由于这个标准化因子的作用，使得$\hat{\gamma_i}$的值不会随着放大因子$\alpha$的变化而变化了。很容易看出：</p><script type="math/tex; mode=display">\gamma_i = \hat{\gamma_i}||W||_{L2}\tag{2.3}</script><h2 id="最大化最小距离"><a href="#最大化最小距离" class="headerlink" title="最大化最小距离"></a>最大化最小距离</h2><p>　　定义了几何间隔和函数间隔之后，我们就需要最大化最小距离了，这个听起来挺绕口的，其实意思很简单，就是求得一组$W$和$b$的情况下的最小样本距离，然后在不同的$W$和$b$的情况下最大化这个最小样本距离，最后得出的结果就是能够使得支持向量到超平面的距离最大的超平面了。我们观察下公式可能会更直观一些：</p><script type="math/tex; mode=display">\gamma = \min_{N=1,\cdots,N} \gamma_i, \ i=1,\cdots,N\tag{2.4}</script><script type="math/tex; mode=display">\hat{\gamma} = \min_{N=1,\cdots,N} \hat{\gamma_i}, \ i=1,\cdots,N\tag{2.5}</script><p>这个就是最小几何间隔距离，我们现在最大化它，有：</p><script type="math/tex; mode=display">\max_{W,b} \hat{\gamma}\tag{2.6}</script><p>将两者写在一起，可以表述为：</p><script type="math/tex; mode=display">\max_{W,b} \hat{\gamma} \\s.t. \ \frac{y_i(W^Tx_i+b)}{ \Vert W \Vert} \geq \hat{\gamma}, \ i=1,\cdots,N\tag{2.7}</script><p>容易看出其中的$\frac{y_i(W^Tx_i+b)}{ \Vert W  \Vert} \geq \hat{\gamma},  i=1,\cdots,N$</p><p>其实是和约束条件$\hat{\gamma} = \min_{N=1,\cdots,N} \hat{\gamma_i},  i=1,\cdots,N$等价的。我们做一些恒等变换有:</p><script type="math/tex; mode=display">\max_{W,b} \frac{\gamma}{ \Vert W  \Vert} \\s.t. \ y_i(W^Tx_i+b) \geq \gamma, \ i=1,\cdots,N\tag{2.8}</script><p>　　这里我们要想一下：$\gamma$的具体取值会不会影响到最优化后的$W$和$b$的取值呢？答案是不会的，因为我们只要令<strong>所有支持向量，也就是距离超平面最近的若干个样本点到超平面的距离为单位量，比如为1即可，这个可以通过等比例调整$W$和$b$容易地做到，其他样本也会随着进行相应的缩放。这样对整个超平面的最优化点是没有任何影响的。</strong>所以我们现在将$\gamma$设为常数1。现在有：</p><script type="math/tex; mode=display">\max_{W,b} \frac{1}{ \Vert W \Vert} \\s.t. \ y_i(W^Tx_i+b) \geq 1, \ i=1,\cdots,N\tag{2.9}</script><p>此时最大化问题转化为最小化问题：</p><script type="math/tex; mode=display">\min_{W,b} \frac{1}{2} \Vert W \Vert^2 \\s.t. 1-y_i(W^Tx_i+b) \leq 0, \ i=1,\cdots,N\tag{2.10}</script><p>至此，我们得到了SVM的标准原问题表达。注意到这个式子里的$1-y_i(W^Tx_i+b) \leq 0,$，当存在$x_i$使得$1-y_i(W^Tx_i+b) = 0$时，这个$x_i$就被称之为<strong>支持向量</strong>。如下图的虚线上的红色样本和蓝色样本所示，虽然样本有很多个，但是有效的，决定超平面的样本，也就是<strong>支持向量</strong>一共就只有五个，其到决策面的距离被标准化为了1。<br><img src="/imgs/svm/svm_margin_2.png" alt="svm_margin"></p><p><strong>我们接下来将会讨论SVM原问题拉格朗日函数形式以及其对偶问题，以便于更好地解决这个最优化问题。</strong></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;font size=&quot;6&quot;&gt;&lt;b&gt;前言&lt;/b&gt;&lt;/font&gt;&lt;br&gt;&lt;strong&gt;支持向量机是常用的，泛化性能佳的，而且可以应用核技巧的机器学习算法，在深度学习流行前是最被广泛使用的机器学习算法之一，就算是深度学习流行的现在，支持向量机也由于其高性能，较低的计算复杂度而
      
    
    </summary>
    
      <category term="Machine Learning" scheme="https://blog.csdn.net/LoseInVain/categories/Machine-Learning/"/>
    
    
      <category term="Machine Learning" scheme="https://blog.csdn.net/LoseInVain/tags/Machine-Learning/"/>
    
      <category term="SVM" scheme="https://blog.csdn.net/LoseInVain/tags/SVM/"/>
    
  </entry>
  
  <entry>
    <title>深度学习Debug沉思录</title>
    <link href="https://blog.csdn.net/LoseInVain/2018/10/21/dl_debug_thinking/dl_debug/"/>
    <id>https://blog.csdn.net/LoseInVain/2018/10/21/dl_debug_thinking/dl_debug/</id>
    <published>2018-10-21T04:02:57.352Z</published>
    <updated>2018-10-20T18:21:23.808Z</updated>
    
    <content type="html"><![CDATA[<p><font size="6"><b>前言</b></font><br>接触深度学习也有一两年了，一直没有将一些实战经验整理一下形成文字。本文打算用来纪录一些在深度学习实践中的调试过程，纪录一些经验之谈。因为目前深度学习业界的理论基础尚且薄弱，很多工程实践中的问题没法用理论解释得很好，这里的只是实践中的一些经验之谈，以供参考以及排错。本文将持续更新。需要强调的是，本文的很多单纯只是经验，在尽可能列出参考文献的同时却并无严格理论验证，希望大家见谅。<strong>欢迎大家集思广益，共同维护这个经验集，为整个社区贡献微弱力量。</strong></p><p>如有问题请指出，转载请标注出处，联系方式：</p><p><strong>e-mail</strong>: FesianXu@163.com<br><strong>QQ</strong>: 973926198<br><strong>github</strong>: <a href="https://github.com/FesianXu" target="_blank" rel="noopener">https://github.com/FesianXu</a> </p><hr><h1 id="在分类问题中，损失函数及其快速得下降为0-0000"><a href="#在分类问题中，损失函数及其快速得下降为0-0000" class="headerlink" title="在分类问题中，损失函数及其快速得下降为0.0000"></a>在分类问题中，损失函数及其快速得下降为0.0000</h1><p>在分类问题中，我们一般采用的是交叉熵[1]作为损失函数，如式(1.1)所示</p><script type="math/tex; mode=display">\begin{aligned}\mathcal{L}_{cls} &= - \sum_{i=1}^n y_i \log{\hat{y}_i} = - \mathbf{y}^T \log{\mathbf{\hat{y}}} \\\mathbf{y} &\in \mathbb{R}^n, \mathbf{\hat{y}} \in \mathbb{R}^n\end{aligned}\tag{1.1}</script><p>其中$\hat{y}$和$\mathbf{\hat{y}}$是预测结果，以概率分布的形式表达，如$[0.2,0.3,0.3,0.2]$等，一般是通过softmax层实现，$y$和$\mathbf{ {y} }$是样本真实标签，在单分类问题中，采用的是独热编码[2]，只有一个分量是为1的，如$[0.0,1.0,0.0,0.0]$。（公式第二行是向量化表达）</p><p><strong>我们发现，交叉熵损失的下确界是0，但是永远都不可能达到0</strong>，因为要达到0，那么所有的预测向量分布就必须完全和真实标签一致，退化为独热编码。但是实际上在神经网络中，经过了softmax层之后，是不可能使得除了目标分量的其他所有分量为0的（这个这里只是抛出了结论，讨论需要比较长的篇幅。），因此永远不可能达到0的，正是因为如此，交叉熵损失可以一直优化，这也是其比MSE损失优的一个点之一。</p><p>既然注意到了不可能为0，我们就可以分析，这肯定是自己程序问题，我们将经过softmax之前的logit打印出，如：</p><script type="math/tex; mode=display">[1023,-201,1021,124]</script><p>发现了没有，这些值都很大，而softmax函数为:</p><script type="math/tex; mode=display">P(x_i) = \dfrac{\exp(x_i)}{\sum_{i=1}^n \exp{(x_i)}}\tag{1.2}</script><p>我们会发现，过大或者过小的指数项，比如1023，会涉及到计算$e^{1023}$，这个数值在TensorFlow或者大部分框架中是溢出的，显示为inf，因此就会把该分量拉成1，而其他变成了0。这种操作是会导致严重的过拟合的。因此，一般来说，logit值不能太大，否则将会出现数值计算问题。</p><p><strong>那么如何解决？</strong><br>出现这种问题的情况很多时候是因为<strong>参数初始化</strong>导致的数值计算问题，比如都采用了方差过小的高斯分布进行初始化，那么就会把网络的输出的范围拉的特别大，导致以上的问题。因此在参数初始化中，确保每一层的初始化都是在一定范围内的，可以考虑采用Xavier初始化，Kaiming初始化等。（这个初始化的影响我们将会以后讨论，这是一个新的话题。）</p><hr><h1 id="在正则化的过程中对神经网络的偏置也进行了正则"><a href="#在正则化的过程中对神经网络的偏置也进行了正则" class="headerlink" title="在正则化的过程中对神经网络的偏置也进行了正则"></a>在正则化的过程中对神经网络的偏置也进行了正则</h1><p>一般来说，我们常用的是二范数正则，也即是岭回归，如式子(2.1)</p><script type="math/tex; mode=display">\mathcal{L} = \gamma_{\mathbf{w}}(y,\hat{y})+\dfrac{1}{2}\mathbf{w}^T\mathbf{w}\tag{2.1}</script><p>一般来说，我们只会对神经网络的<strong>权值</strong>进行正则操作，使得权值具有一定的稀疏性[3]，减少模型的容量以减少过拟合的风险。同时，我们注意到神经网络中每一层的权值的作用是<strong>调节每一层超平面的方向</strong>（因为$\mathbf{w}$就是其法向量），因此只要比例一致，不会影响超平面的形状的。但是，我们必须注意到，每一层中的偏置是<strong>调节每一层超平面的平移长度的</strong>，如果你对偏置进行了正则，那么我们的$b$可能就会变得很小，或者很稀疏，这样就导致你的每一层的超平面只能局限于很小的一个范围内，使得模型的容量大大减少，一般会导致欠拟合[7]的现象。</p><p><strong>因此，一般我们不会对偏置进行正则的，注意了。</strong></p><hr><h1 id="学习率太大导致不收敛"><a href="#学习率太大导致不收敛" class="headerlink" title="学习率太大导致不收敛"></a>学习率太大导致不收敛</h1><p>不收敛是个范围很大的问题，有很多可能性，其中有一种是和网络结构无关的原因，就是学习率设置的太大了，如下图所示，太大的学习率将会导致严重的抖动，使得无法收敛，<strong>甚至在某些情况下可能使得损失变得越来越大直到无穷</strong>。这个时候请调整你的学习率，尝试是否可以收敛。当然，这里的“太大”目前没有理论可以衡量，不过我喜欢从$10^{-3} \sim 10^{-4}$的Adam优化器[4]开始进行尝试优化。<br><img src="/imgs/thinking_in_dl/large_lr.png" alt="large_lr"></p><p>下图展示了过大过小的学习率对模型性能的影响曲线图：<br><img src="/imgs/thinking_in_dl/what.png" alt="what"></p><hr><h1 id="别在softmax层前面的输入施加了激活函数"><a href="#别在softmax层前面的输入施加了激活函数" class="headerlink" title="别在softmax层前面的输入施加了激活函数"></a>别在softmax层前面的输入施加了激活函数</h1><p>softmax函数如式(4.1)所示：</p><script type="math/tex; mode=display">S(x_i)=\dfrac{\exp{(x_i)}}{\sum_{j=1}^n \exp(x_j)}\tag{4.1}</script><p>假设我们的网络提取出来的最后的特征向量是$\tilde{\mathbf{y}} = f_{\theta}(\mathbf{x}), \tilde{\mathbf{y}}\in\mathbb{R}^m$，如果我们最后的分类的类别有$n$类，那么我们会用一个全连接层将其映射到对应维度的空间里面，如式(4.2)。</p><script type="math/tex; mode=display">\mathbf{y}=g_{\mathbf{w}}(\tilde{\mathbf{y}}) \\\mathbf{y} \in \mathbb{R}^n\tag{4.2}</script><p>那么，这个全连接层虽然说可以看成是分类器，但是我们最好把它看成是上一层的“近线性可分特征”的一个维度转换（有点绕，意思是我们这里只是一个维度的转换，而不涉及到kernel），不管怎么说，这个时候，我们的输出是不能有激活函数的，如下式是<strong>不可以的</strong>：</p><script type="math/tex; mode=display">\mathbf{y}=\sigma(g_{\mathbf{w}}(\tilde{\mathbf{y}}) ) \\\sigma(\cdot) 为激活函数\\\mathbf{y} \in \mathbb{R}^n\tag{4.3}</script><p>这时候的输出，具有和分类类别相同的维度，在很多框架中被称之为<strong>logits</strong>值，这个值一般是在实数范围内的，一般不会太大，参考笔记第一点的情况。</p><hr><h1 id="检查原数据输入的值范围"><a href="#检查原数据输入的值范围" class="headerlink" title="检查原数据输入的值范围"></a>检查原数据输入的值范围</h1><p>原始数据输入可能千奇百怪，每个特征维的值范围可能有着数量级上的差别，这个时候如果我们不对数据进行预处理，将会大大增大设计网络的负担。一般来说我们希望输入的数据是中心对齐的，也即是<strong>0均值</strong>的[5]，可以加速网络收敛的速度。同时，我们希望不同维度上的数值范围是一致的，可以采用一些归一化[6]的手段进行处理（这个时候假设每个维度重要性是一样的，比如我们图片的三个通道等）。</p><hr><h1 id="别忘了对你的训练数据进行打乱"><a href="#别忘了对你的训练数据进行打乱" class="headerlink" title="别忘了对你的训练数据进行打乱"></a>别忘了对你的训练数据进行打乱</h1><p>经常，你的训练过程非常完美，能够很好地拟合训练数据，但是在测试过程中确实一塌糊涂，是的，你的模型这个时候过拟合[7]了。这个时候你会检查模型的有效性，不过在进行这一步之前，不妨先检查下你的<strong>数据加载器</strong>(Data Loader)是否是正常设计的。</p><p>一般来说，我们的训练数据在训练过程中，每一个epoch[8]中，都是需要进行<strong>打乱</strong>(shuffle)的，很多框架的数据加载器参数列表中都会有这项选项，比如pytorch的DataLoader类[9]。为什么需要打乱呢？那是因为如果不打乱我们的训练数据，我们的模型就有可能学习到训练数据的个体与个体之间特定的排列顺序，而这种排列顺序，在很多情况下是无用的，会导致过拟合的糟糕现象。因此，我们在训练过程中，在每一个epoch训练中都对训练集进行打乱，以确保模型不能“记忆”样本之间的特定排序。这其实也是<strong>正则</strong>的一种手段。</p><p>在训练中，大概如：</p><script type="math/tex; mode=display">epoch\_1 \rightarrow shuffle \rightarrow epoch\_2 \rightarrow shuffle \cdots</script><hr><h1 id="一个batch中，label不要全部相同"><a href="#一个batch中，label不要全部相同" class="headerlink" title="一个batch中，label不要全部相同"></a>一个batch中，label不要全部相同</h1><p>这个情况有点类似与笔记的第六点，我们需要尽量给训练过程中人为引入不确定性，这是很多正则手段，包括dropout，stochastic depth等的思路，这样能够有效地减少过拟合的风险。因此，一个batch中，尽量确保你的样本是来自于各个类的（针对分类问题而言），这样你的模型会减少执着与某个类别的概率，减少过拟合风险，同时也会加快收敛速度。</p><hr><h1 id="少用vanilla-SGD优化器"><a href="#少用vanilla-SGD优化器" class="headerlink" title="少用vanilla SGD优化器"></a>少用vanilla SGD优化器</h1><p>在高维度情况下的优化，其优化平面会出现很多鞍点（既是梯度为0，但却不是极点），通常，鞍点会比局部极值更容易出现（直观感受就是，因为高维度情况下，一个点周围有很多维度，如果是极值点，那么就需要其他所有维度都是朝向同一个方向“弯曲”的，而这个要比鞍点的各个方向“弯曲”的情况可能要小），因此这个时候我们更担心陷于鞍点，而不是局部极小值点（当然局部极小值点也是一个大麻烦，不过鞍点更麻烦）。如果采用普通的SGD优化器，那么就会陷于任何一个梯度为0的点，也就是说，极有可能会陷于鞍点。如果要使用SGD方法，建议使用带有momentum的SGD方法，可以有效避免陷入鞍点的风险。<br><img src="/imgs/thinking_in_dl/optimizer.gif" alt="optimizer"></p><p>下图是某个函数的三维曲线图和等高线图，我们可以看到有若干个局部最优点和鞍点，这些点对于vanilla SGD来说是不容易处理的。</p><h2 id=""><a href="#" class="headerlink" title=""></a><img src="/imgs/thinking_in_dl/curve.png" alt="curve"></h2><h1 id="检查各层梯度，对梯度爆炸进行截断"><a href="#检查各层梯度，对梯度爆炸进行截断" class="headerlink" title="检查各层梯度，对梯度爆炸进行截断"></a>检查各层梯度，对梯度爆炸进行截断</h1><p>有些时候，你会发现在训练过程中，你的损失突然变得特别大，或者特别小，这个时候不妨检查下每一层的梯度（用tensorboard的distribution可以很好地检查），很可能是发生了<strong>梯度爆炸</strong>(gradient explosion)的情况，特别是在存在LSTM等时序的网络中，很容易出现这种情况。因此，这个时候我们会用梯度截断进行处理，操作很简单粗暴，就是设置一个阈值，把超过这个阈值的梯度全部拉到这个阈值，如下图所示：<br><img src="/imgs/thinking_in_dl/clip_gradient.png" alt="clip_gradient"></p><p>在tensorflow中也提供了相应的API供梯度截断使用[10]，如：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tf.clip_by_value(</span><br><span class="line">    t,</span><br><span class="line">    clip_value_min, <span class="comment"># 指定截断最小值</span></span><br><span class="line">    clip_value_max, <span class="comment"># 指定截断最大值</span></span><br><span class="line">    name=<span class="keyword">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure></p><p>具体使用见[11]，在应用梯度之前，对梯度截断进行处理。</p><hr><h1 id="检查你的样本label"><a href="#检查你的样本label" class="headerlink" title="检查你的样本label"></a>检查你的样本label</h1><p>有些时候，你的训练过程可以很好地收敛，当使用MSE损失[12]的时候甚至可能达到0.0000的情况。但是，当你把模型拿到测试集中评估的时候，却发现性能极差，仿佛没有训练一样。这是过拟合吗？显然是的，但是这可能并不是你的模型的问题，请检查你的数据加载中训练集的样本标签是否正确对应。</p><p>这个问题很白痴，但是却真的很容易在数据加载过程中因为种种原因把label信息和对应样本给混掉。根据文献[13]中的实验，用MSE损失的情况下，就算是你的label完全随机的，和样本一点关系都没有，也可以通过基于SGD的优化算法达到0.0000损失的。<strong>因此，请务必确保你的样本label是正确的。</strong></p><hr><h1 id="分类问题中的分类置信度问题"><a href="#分类问题中的分类置信度问题" class="headerlink" title="分类问题中的分类置信度问题"></a>分类问题中的分类置信度问题</h1><p>在分类问题中我们一般都是采用的是交叉熵损失，如式子(1.1)所示，在一些实验中，如果我们绘制出训练损失和分类准确度的曲线图，我们可能会有下图这种情况[14]：<br><img src="/imgs/thinking_in_dl/sbkyl.png" alt="sbkyl"></p><p>其中上图为分类损失，紫色为训练损失，蓝色为测试损失，下图为分类准确度，绿色为训练准确度，蓝色为测试准确度。<strong>我们不难发现一个比较有意思的现象，就是当测试损失开始到最低点，开始向上反弹的时候，其测试准确度却还是上升的，而不是下降。</strong> 这是为什么呢？为什么分类准确度不会顺着分类损失的增大而减少呢？</p><p>这个涉及到了分类过程中对某个类的“置信程度”的多少，比如：</p><script type="math/tex; mode=display">[0.9,0.01,0.02,0.07]</script><p>模型是对第一类相当确信的，但是在第二种情况：</p><script type="math/tex; mode=display">[0.5,0.4,0.05,0.05]</script><p>这对第一类的置信程度就很低了，虽然按照贝叶斯决策，还是会选择第一类作为决策结果。因此这就是导致以上现象的原因，<strong>在那个拐点后面，这个模型对于分类的置信程度其实已经变得很差了，虽然对于准确度而言，其还能分类正确。</strong> 但是这其实正是过拟合的一种表现，模型已经对自己的分类结果不确信了。</p><hr><h1 id="少在太小的批次中使用BatchNorm层"><a href="#少在太小的批次中使用BatchNorm层" class="headerlink" title="少在太小的批次中使用BatchNorm层"></a>少在太小的批次中使用BatchNorm层</h1><p>Batch Normalization[15]，中文译作<strong>批规范化</strong>，在深度学习中是一种加快收敛速度，提高性能的一个利器，其本质和我们对输入的原数据进行0均值单位方差规范化差不多，是以batch为单位，对中间层的输出进行规范化，可以缓和<strong>内部协方差偏移</strong>(Internal Covariate Shift)的现象。其基本公式很简单，如下：</p><script type="math/tex; mode=display">\begin{aligned}\tilde{x} &= \dfrac{x_i-\mu}{\sigma_i} \\x_i^{\rm{norm}} &= \gamma_i \cdot \tilde{x} + \beta_i \end{aligned}\tag{12.1}</script><p>不过这里并不打算对BN进行详细讲解，只是想告诉大家，因为BN操作在训练过程中是对每个batch进行处理的，从每个batch中求得均值和方差才能进行操作。如果你的batch特别小（比如是受限于硬件条件或者网络要求小batch），那么BN层的batch均值和方差可能就会不能很好符合整个训练集的统计特征，导致差的性能。实际上，实验[16]说明了这个关系，当batch小于16时，性能大幅度下降。<br><img src="/imgs/thinking_in_dl/bn_batch.png" alt="bn_batch"><br><strong>因此，少在太小的batch中使用BN层，如果实在要使用，在发生性能问题时优先检查BN层。</strong></p><hr><h1 id="数值计算问题，出现Nan"><a href="#数值计算问题，出现Nan" class="headerlink" title="数值计算问题，出现Nan"></a>数值计算问题，出现Nan</h1><p>Nan(Not An Number)是一个在数值计算中容易出现的问题，在深度学习中因为涉及到很多损失函数，有些损失函数的定义域并不是整个实数，比如常用的对数，因此一不小心就会出现Nan。在深度学习中，如果某一层出现了Nan，那么是具有传递性的，后面的层也会出现Nan，因此可以通过<strong>二分法</strong>对此进行排错。</p><p>一般来说，在深度学习中出现Nan是由于除0异常或者是因为损失函数中的（比如交叉熵，KL散度）对数操作中，输入小于或者等于0了，一般等于0的情况比较多，因此通常会：</p><script type="math/tex; mode=display">\log(x+\epsilon) \approx \log(x)</script><p>这里的$\epsilon$是个很小的值，一般取$10^{-6}$即可，可以防止因为对数操作中输入0导致的Nan异常。</p><p><strong>需要注意的是，有些时候因为参数初始化或者学习率太大也会导致数值计算溢出，这也是会出现Nan的，一般这样会出现在较前面的层里面。</strong></p><hr><h1 id="BN层放置的位置问题"><a href="#BN层放置的位置问题" class="headerlink" title="BN层放置的位置问题"></a>BN层放置的位置问题</h1><p>BN层有两种常见的放置位置，如下图所示：<br><strong>第一个是放在激活函数之前：</strong><br><img src="/imgs/thinking_in_dl/bn1.png" alt="bn1"><br><strong>第二个是放在激活函数之后：</strong><br><img src="/imgs/thinking_in_dl/bn2.png" alt="bn2"></p><p>在原始BN的论文[15]中，Batch Norm(BN)层是位于激活层之前的，因为是对原始的，未经过激活的logit数据进行数据分布的重整。然而，不少实验证实似乎BN层放在<strong>激活层之后效果会更好</strong>，这个原因目前不明。</p><hr><h1 id="dropout层应用在卷积层中可能导致更差的性能"><a href="#dropout层应用在卷积层中可能导致更差的性能" class="headerlink" title="dropout层应用在卷积层中可能导致更差的性能"></a>dropout层应用在卷积层中可能导致更差的性能</h1><p>dropout[19]是hinton大神与2012年提出的一种神经网络正则手段，其可以简单解释为<strong>在训练过程中，按一定概率让神经网络中的某些神经元输出为0</strong>，其原因可以有几个解释，一个是作为一种集成模型进行解释，另一个可以看成是在特征提取学习过程中给数据加入噪声，可以看成是一种数据增强的正则手段。</p><p>在原始论文中，dropout被应用于全连接层中，而没有应用在卷积层中，Hinton的解释是因为卷积层参数并不多，过拟合风险较小不适合采用dropout这种大杀器的正则手段。有人也认为因为卷积网络是局部感知的，用dropout正则对于其在后层中对于全局信息的获取可能具有负作用[20]。</p><p>不过在一些工作中，也有人将dropout层应用在卷积层中的[17-18]，其层次安排为:$CONV-&gt;RELU-&gt;DROP$，不过其丢弃率$p$都是选择的较小数如$0.1$，$0.2$等，个人觉得这里的作用大概是对<strong>中间数据进行加入噪声，以便于数据增强的正则手段。</strong></p><p><strong>个人建议是可以尝试在卷积层中使用少量的dropout，用较小的丢弃率，但是最后别忘了扔掉这些dropout再进行一些探索，也许可以具有更好的效果。</strong></p><hr><h1 id="较小的batch-size可以提供较好的泛化"><a href="#较小的batch-size可以提供较好的泛化" class="headerlink" title="较小的batch size可以提供较好的泛化"></a>较小的batch size可以提供较好的泛化</h1><p>现代的深度学习优化器基本上都是基于SGD算法进行修改而成的，在每一次训练中都是以一个batch size为单位进行训练的，在这个过程中相当于在统计这个batch中样本的一些统计特性，因此batch size是会影响模型的超曲线形状的。</p><p>一般来说较大的batch size比如128，256会和整个训练集的统计性质更相近，从而使得具有较少的多样性，而较小的batch size 比如16，32，因为batch size较小，不同batch之间的差异性较大，这种差异性可以看成是正则手段，有机会提高模型的泛化性能。（不过有些文章似乎不同意这个观点，认为较大batch size有较好性能，<strong>个人建议是大batch size和小batch size都可以跑跑，有可能能提升性能。</strong>）</p><hr><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p>[1]. Janocha K, Czarnecki W M. On loss functions for deep neural networks in classification[J]. arXiv preprint arXiv:1702.05659, 2017.(Overview about loss function used in DNN)<br>[2]. <a href="https://blog.csdn.net/LoseInVain/article/details/78819390">tf.one_hot()进行独热编码</a><br>[3]. <a href="https://blog.csdn.net/LoseInVain/article/details/82824987">曲线拟合问题与L2正则</a><br>[4]. Kinga D, Adam J B. A method for stochastic optimization[C]//International Conference on Learning Representations (ICLR). 2015, 5.<br>[5]. <a href="https://blog.csdn.net/LoseInVain/article/details/78109098">&lt;深度学习系列&gt;深度学习中激活函数的选择</a><br>[6]. <a href="https://blog.csdn.net/leiting_imecas/article/details/54986045">机器学习之特征归一化（normalization）</a><br>[7]. <a href="https://blog.csdn.net/LoseInVain/article/details/78108990">机器学习模型的容量，过拟合与欠拟合</a><br>[8]. <a href="https://blog.csdn.net/LoseInVain/article/details/79348965">在机器学习中epoch, iteration, batch_size的区别</a><br>[9]. <a href="https://pytorch.org/docs/stable/_modules/torch/utils/data/dataloader.html" target="_blank" rel="noopener">Pytorch Dataloader doc</a><br>[10]. <a href="https://www.tensorflow.org/api_docs/python/tf/clip_by_value" target="_blank" rel="noopener">tf.clip_by_value</a><br>[11]. <a href="https://blog.csdn.net/mmc2015/article/details/79419365">梯度截断的tensorflow实现</a><br>[12]. <a href="https://blog.csdn.net/reallocing1/article/details/56292877">均方误差(MSE)和均方根误差(RMSE)和平均绝对误差(MAE)</a><br>[13]. Du S S, Zhai X, Poczos B, et al. Gradient Descent Provably Optimizes Over-parameterized Neural Networks[J]. arXiv preprint arXiv:1810.02054, 2018.<br>[14]. <a href="https://stackoverflow.com/questions/51896843/the-inconsistency-between-the-loss-curve-and-metric-curve" target="_blank" rel="noopener">The inconsistency between the loss curve and metric curve?</a><br>[15]. Ioffe S, Szegedy C. Batch normalization: accelerating deep network training by reducing internal covariate shift[C]// International Conference on International Conference on Machine Learning. JMLR.org, 2015:448-456.<br>[16]. Wu Y, He K. Group normalization[J]. arXiv preprint arXiv:1803.08494, 2018.<br>[17]. Park S, Kwak N. Analysis on the dropout effect in convolutional neural networks[C]//Asian Conference on Computer Vision. Springer, Cham, 2016: 189-204.<br>[18]. <a href="https://stats.stackexchange.com/questions/240305/where-should-i-place-dropout-layers-in-a-neural-network" target="_blank" rel="noopener">Where should I place dropout layers in a neural network?</a><br>[19]. Hinton G E, Srivastava N, Krizhevsky A, et al. Improving neural networks by preventing co-adaptation of feature detectors[J]. arXiv preprint arXiv:1207.0580, 2012.<br>[20]. <a href="https://www.reddit.com/r/MachineLearning/comments/42nnpe/why_do_i_never_see_dropout_applied_in/" target="_blank" rel="noopener">Why do I never see dropout applied in convolutional layers?</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;font size=&quot;6&quot;&gt;&lt;b&gt;前言&lt;/b&gt;&lt;/font&gt;&lt;br&gt;接触深度学习也有一两年了，一直没有将一些实战经验整理一下形成文字。本文打算用来纪录一些在深度学习实践中的调试过程，纪录一些经验之谈。因为目前深度学习业界的理论基础尚且薄弱，很多工程实践中的问题没法用理论解
      
    
    </summary>
    
      <category term="Deep Learning" scheme="https://blog.csdn.net/LoseInVain/categories/Deep-Learning/"/>
    
    
      <category term="Deep Learning" scheme="https://blog.csdn.net/LoseInVain/tags/Deep-Learning/"/>
    
      <category term="Debug Practice" scheme="https://blog.csdn.net/LoseInVain/tags/Debug-Practice/"/>
    
  </entry>
  
</feed>
